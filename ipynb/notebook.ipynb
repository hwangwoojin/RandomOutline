{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RandomOutline.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6658a136d38d4dd18bda0a609d60ece3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_61ec38033a814dd0b8c564865b346937",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b12d3afd6c204ecfba964ee5f44e7c55",
              "IPY_MODEL_8df77a3e59144846914f79295e40cc99"
            ]
          }
        },
        "61ec38033a814dd0b8c564865b346937": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b12d3afd6c204ecfba964ee5f44e7c55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7ee7238148de4e29ae7ab341fce8018e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 169001437,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 169001437,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_07b267c3a88544fc980bc78b5a4ec336"
          }
        },
        "8df77a3e59144846914f79295e40cc99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6ae6f326bc0045ad9a24c406eefb3d9a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 169001984/? [00:17&lt;00:00, 9844932.96it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a3731c5b373c4adf9f4cafa362f8a806"
          }
        },
        "7ee7238148de4e29ae7ab341fce8018e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "07b267c3a88544fc980bc78b5a4ec336": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6ae6f326bc0045ad9a24c406eefb3d9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a3731c5b373c4adf9f4cafa362f8a806": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSFGYaIDG6f0"
      },
      "source": [
        "Cutout Data Augmentation.\n",
        "\n",
        "This code is implmented by following the official code (https://github.com/uoguelph-mlrg/Cutout)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCVSE5-UboYl"
      },
      "source": [
        "##**Import all neceassary packages**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YBMwPsubsbX"
      },
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "from tqdm.notebook import tqdm as tqdm\n",
        "\n",
        "# new\n",
        "import random\n",
        "import cv2\n",
        "from PIL import Image, ImageChops\n",
        "import torchvision.transforms.functional as TF"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L88afYXKMSdL"
      },
      "source": [
        "##**Model - Define ResNet Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMFSLTnkMQdq"
      },
      "source": [
        "'''ResNet18/34/50/101/152 in Pytorch.'''\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(in_planes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = conv3x3(3,64)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18(num_classes=10):\n",
        "    return ResNet(BasicBlock, [2,2,2,2], num_classes)\n",
        "\n",
        "def ResNet34(num_classes=10):\n",
        "    return ResNet(BasicBlock, [3,4,6,3], num_classes)\n",
        "\n",
        "def ResNet50(num_classes=10):\n",
        "    return ResNet(Bottleneck, [3,4,6,3], num_classes)\n",
        "\n",
        "def ResNet101(num_classes=10):\n",
        "    return ResNet(Bottleneck, [3,4,23,3], num_classes)\n",
        "\n",
        "def ResNet152(num_classes=10):\n",
        "    return ResNet(Bottleneck, [3,8,36,3], num_classes)\n",
        "\n",
        "def test_resnet():\n",
        "    net = ResNet50()\n",
        "    y = net(Variable(torch.randn(1,3,32,32)))\n",
        "    print(y.size())\n",
        "\n",
        "# test_resnet()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjM3cl279Lvg"
      },
      "source": [
        "##**Utils**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIvuSgE49Kvu"
      },
      "source": [
        "class AverageMeter(object):\n",
        "    r\"\"\"Computes and stores the average and current value\n",
        "    \"\"\"\n",
        "    def __init__(self, name, fmt=':f'):\n",
        "        self.name = name\n",
        "        self.fmt = fmt\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __str__(self):\n",
        "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
        "        return fmtstr.format(**self.__dict__)\n",
        "\n",
        "\n",
        "class ProgressMeter(object):\n",
        "    def __init__(self, num_batches, *meters, prefix=\"\"):\n",
        "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
        "        self.meters = meters\n",
        "        self.prefix = prefix\n",
        "\n",
        "    def print(self, batch):\n",
        "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
        "        entries += [str(meter) for meter in self.meters]\n",
        "        print('\\t'.join(entries))\n",
        "\n",
        "    def _get_batch_fmtstr(self, num_batches):\n",
        "        num_digits = len(str(num_batches // 1))\n",
        "        fmt = '{:' + str(num_digits) + 'd}'\n",
        "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
        "\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    r\"\"\"Computes the accuracy over the $k$ top predictions for the specified values of k\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        # _, pred = output.topk(maxk, 1, True, True)\n",
        "        # pred = pred.t()\n",
        "        # correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        # faster topk (ref: https://github.com/pytorch/pytorch/issues/22812)\n",
        "        _, idx = output.sort(descending=True)\n",
        "        pred = idx[:,:maxk]\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size))\n",
        "        return res"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6y3zhSfMbdC"
      },
      "source": [
        "##**Cutout: Main Code for Applying Cutout data augmentation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMQI2K4AMopg"
      },
      "source": [
        "class RandomOutline(object):\n",
        "    def __init__(self, p):\n",
        "        self.p = p\n",
        "        self.dim = 3\n",
        "\n",
        "    def __call__(self, img):\n",
        "        h = img.size()[1]\n",
        "        w = img.size()[2]\n",
        "        \n",
        "        h_size = int(h * self.p) # max h outline size\n",
        "        w_size = int(w * self.p) # max w outline size\n",
        "\n",
        "        for j in range(h):\n",
        "            top = random.randint(0, h_size)\n",
        "            bottom = random.randint(0, h_size)\n",
        "            img[:, :top, j] = torch.from_numpy(np.random.rand(3,top))\n",
        "            img[:, h-bottom:, j] = torch.from_numpy(np.random.rand(3,bottom))\n",
        "            \n",
        "        for j in range(w):\n",
        "            left = random.randint(0, w_size)\n",
        "            right = random.randint(0, w_size)\n",
        "            img[:, j, :left] = torch.from_numpy(np.random.rand(3,left))\n",
        "            img[:, j, w-right:] = torch.from_numpy(np.random.rand(3,right))\n",
        "        \n",
        "        return img"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9s8oXpzdMvol"
      },
      "source": [
        "##**Parameter Settings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pjeqawi9cNK6"
      },
      "source": [
        "dataset = 'cifar100' # cifar10 or cifar100\n",
        "model = 'resnet34' # resnet18, resnet50, resnet101\n",
        "batch_size = 128  # Input batch size for training (default: 128)\n",
        "epochs = 150 # Number of epochs to train (default: 200)\n",
        "learning_rate = 0.1 # Learning rate\n",
        "data_augmentation = True # Traditional data augmentation such as augmantation by flipping and cropping?\n",
        "cutout = True # Apply Cutout?\n",
        "n_holes = 1 # Number of holes to cut out from image\n",
        "length = 16 # Length of the holes\n",
        "seed = 0 # Random seed (default: 0)\n",
        "print_freq = 30\n",
        "cuda = torch.cuda.is_available()\n",
        "cudnn.benchmark = True  # Should make training should go faster for large models\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "if cuda:\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "test_id = dataset + '_' + model"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXL_PBj6cVoe"
      },
      "source": [
        "##**Load and preprocess data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117,
          "referenced_widgets": [
            "6658a136d38d4dd18bda0a609d60ece3",
            "61ec38033a814dd0b8c564865b346937",
            "b12d3afd6c204ecfba964ee5f44e7c55",
            "8df77a3e59144846914f79295e40cc99",
            "7ee7238148de4e29ae7ab341fce8018e",
            "07b267c3a88544fc980bc78b5a4ec336",
            "6ae6f326bc0045ad9a24c406eefb3d9a",
            "a3731c5b373c4adf9f4cafa362f8a806"
          ]
        },
        "id": "dvQjH3T9caYs",
        "outputId": "3de4bcae-00e6-4391-8b62-f68fb10e5898"
      },
      "source": [
        "# Image Preprocessing\n",
        "normalize = transforms.Normalize(mean=[x / 255.0 for x in [125.3, 123.0, 113.9]],\n",
        "                                     std=[x / 255.0 for x in [63.0, 62.1, 66.7]])\n",
        "\n",
        "\"\"\" Cutout\n",
        "train_transform = transforms.Compose([])\n",
        "if data_augmentation:\n",
        "    train_transform.transforms.append(transforms.RandomCrop(32, padding=4))\n",
        "    train_transform.transforms.append(transforms.RandomHorizontalFlip())\n",
        "train_transform.transforms.append(transforms.ToTensor())\n",
        "train_transform.transforms.append(normalize)\n",
        "\n",
        "if cutout:\n",
        "    train_transform.transforms.append(Cutout(n_holes=n_holes, length=length))\n",
        "\"\"\"\n",
        "\n",
        "# Random Outline\n",
        "train_transform = transforms.Compose([])\n",
        "train_transform.transforms.append(transforms.RandomCrop(32, padding=4))\n",
        "train_transform.transforms.append(transforms.RandomHorizontalFlip())\n",
        "train_transform.transforms.append(transforms.ToTensor())\n",
        "train_transform.transforms.append(normalize)\n",
        "train_transform.transforms.append(RandomOutline(0.05))\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    normalize])\n",
        "\n",
        "if dataset == 'cifar10':\n",
        "    num_classes = 10\n",
        "    train_dataset = datasets.CIFAR10(root='data/',\n",
        "                                     train=True,\n",
        "                                     transform=train_transform,\n",
        "                                     download=True)\n",
        "\n",
        "    test_dataset = datasets.CIFAR10(root='data/',\n",
        "                                    train=False,\n",
        "                                    transform=test_transform,\n",
        "                                    download=True)\n",
        "elif dataset == 'cifar100':\n",
        "    num_classes = 100\n",
        "    train_dataset = datasets.CIFAR100(root='data/',\n",
        "                                      train=True,\n",
        "                                      transform=train_transform,\n",
        "                                      download=True)\n",
        "\n",
        "    test_dataset = datasets.CIFAR100(root='data/',\n",
        "                                     train=False,\n",
        "                                     transform=test_transform,\n",
        "                                     download=True)\n",
        "\n",
        "\n",
        "# Data Loader (Input Pipeline)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True,\n",
        "                                           pin_memory=True,\n",
        "                                           num_workers=2)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False,\n",
        "                                          pin_memory=True,\n",
        "                                          num_workers=2)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to data/cifar-100-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6658a136d38d4dd18bda0a609d60ece3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=169001437.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting data/cifar-100-python.tar.gz to data/\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2yT03dyuuUJS",
        "outputId": "de8a2c52-b73f-4b92-9a0a-9c6fe06cc887"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def custom_imshow(img):\n",
        "    w, h = img.size()[1], img.size()[2]\n",
        "    img = img.numpy()\n",
        "    top = int(random.uniform(0, 1) * (h // 4))\n",
        "    bottom = int(h - random.uniform(0, 1) * (h // 4))\n",
        "    left = int(random.uniform(0, 1) * (w // 4))\n",
        "    right = int(w - random.uniform(0, 1) * (w // 4))\n",
        "    print(w, h, top, bottom, left, right)\n",
        "    plt.imshow(np.transpose(img, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "def process():\n",
        "    count = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        if count > 6: break\n",
        "        custom_imshow(inputs[0])\n",
        "        count += 1\n",
        "\n",
        "process()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "32 32 4 28 2 28\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "32 32 2 26 4 29\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAUn0lEQVR4nO3de3RV1Z0H8O8PCKAEhABCDG9BhVFEjBkfWN8P0BFxWR+1lpmxxvE1sKqdos4UOmMXVgcd1qqPYmHAFhErWKmvgTIqvgZ5CCEaeYiIREjEgCCiBPjNH+dkNdDz2/fm3nPPDezvZy0XN/t39zk/T/LLzT377r1FVUFEh78W+U6AiJLBYifyBIudyBMsdiJPsNiJPMFiJ/JEq2w6i8ilACYDaAngt6r6gOv5RZ1aaUlxm8hYZdU3Zr8BXbpEtu/e9ZXZZ9PuejuPXj3MWN3GTWYsE0W9jjZj37XuZMZ2rVud0flO7Rt9zNoOBWafrrujvycAsHzNZxnlYTlFTjRj72tlrOfKheOHdDdjq1dsMWNDeg00Yys2VjU5jzYnRv9c1VfvwL663RIVk0zH2UWkJYA1AC4CsAnAEgDXq+qHVp+TBrXTuU//TWTsuFOWmOdaUH5LZPvKxS+afe5eWW3GfvjYw2bs97f9xIxl4obHx5ixT0quMmPvXHFORufb+7urI9ufuPgYs8/Nq/qZsTYXjs0oD8vXBevMWGF9/1jPlQtv1N1jxs4pmmjGtj+63Ix1vH1ok/MYsDb652rjqFn4dlVNZLFn82d8GYB1qrpeVfcAeAbAyCyOR0Q5lE2xlwBo/DfeprCNiJqhnN+gE5FyEVkqIkvrtu/N9emIyJBNsVcD6Nno6x5h2wFUdYqqlqpqaVHHrO4HElEWsin2JQAGiEhfEWkN4DoA8+JJi4jilvHdeAAQkREA/gvB0Ns0Vf2l6/ntOnfQE0ecFhl77/f/m3EeRL65//stI9sf+/M+VNdp5N34rP6uVtWXAbyczTGIKBn8BB2RJ1jsRJ5gsRN5gsVO5AkWO5Enshp6a/LJRLi6JVGOqUYPvfGVncgTLHYiT7DYiTzBYifyBIudyBOJzjk9ocPJeOrMhZGxslej15kjor+2a1L0olDDHnnd7MNXdiJPsNiJPMFiJ/IEi53IEyx2Ik+w2Ik8wYkwRIcZToQh8hyLncgTLHYiT7DYiTzBYifyBIudyBNZzXoTkQ0AdgLYB2Cvqpa6nt9hcA8MeyV6E/mXS36aTSpElEIcU1zPU9WtMRyHiHKIf8YTeSLbYlcA80VkmYiUx5EQEeVGtn/GD1PVahE5GsACEflIVRc1fkL4S6AcANqWdMzydESUqaxe2VW1Ovy3FsDzAMoinjNFVUtVtbR158JsTkdEWci42EWknYi0b3gM4GIAlXElRkTxyubP+G4AnheRhuM8raqvujrUr/8Sn183M4tTElGmMi52VV0P4OQYcyGiHOLQG5EnWOxEnmCxE3mCxU7kCRY7kSe44CTRYYYLThJ5jsVO5AkWO5EnWOxEnmCxE3mCxU7kCRY7kSdY7ESeYLETeYLFTuQJFjuRJ1jsRJ6IY0cYb3Qz2nc7+uzIRSJEGeArO5EnWOxEnmCxE3mCxU7kCRY7kSdY7ESeSDn0JiLTAFwOoFZVTwzbigDMBtAHwAYA16jqttyl2Tx0MdrvGW73+eErdqy341yfOWL7HTHyw9PVn0e23zf8UrNPOq/s0wEcfIRxABaq6gAAC8OviagZS1ns4X7rdQc1jwQwI3w8A8CVMedFRDHL9D17N1XdHD7eAvvDZUTUTGT9cVlVVdd68CJSDqA82/MQUXYyfWWvEZFiAAj/rbWeqKpTVLVUVUszPBcRxSDTYp8HYHT4eDSAF+JJh4hyJeX2TyIyC8C5CEaeagCMB/BHAM8C6AXgUwRDbwffxIs6VkbbP1k3BGoyOVjCrnXEzutvx7ZusmOffNv0PM7rbscG9LVj7661Y2O3Nj0PiseukoGR7cNqP8HyPbsjt39K+Z5dVa83QheknxoR5Rs/QUfkCRY7kSdY7ESeYLETeYLFTuSJQ2LBybiH2C5zxF6K+VyzXbF1mR2zxBGzvqEFW+w+w8+0Y2VX2LH+i+zYRGO239t2F2qCJ5c/Ftm+9eJbzD58ZSfyBIudyBMsdiJPsNiJPMFiJ/IEi53IE4kOvfUpOAnju/0pMvYPm/rEeq6an7QzY0dPWmLGRAbFmkcuVGfQZ5oj1mquHbtxsx1z/fT89Kro9rcd56L0HTv5pMj2NjVHmH34yk7kCRY7kSdY7ESeYLETeYLFTuSJRO/G75CvMb/FW7Ed78tr7FjRpPvs4P9Mjy2HQ8UeR+xdR+x0xz5UxY61684vPyOyXee84zjbPjPy0oSjzNjlv9jlOObhadimmZHthfX2UpB8ZSfyBIudyBMsdiJPsNiJPMFiJ/IEi53IEymH3kRkGoDLAdSq6olh2wQANwP4Inzavar6cqpjdT9iD+4ZvDEyNiu62WmWY1LF7TjNDrZd0/STJWzXQ9eZsSNP62V3bG9cyJ72xCB0bGvHCjrbMQx2xIY6YpaWZuSyCV+bse/O/r4Za3Phcxnk0fx1empMk/uk88o+HcClEe2PqOqQ8L+UhU5E+ZWy2FV1EYCUmzYSUfOWzXv2O0SkQkSmiUin2DIiopzItNgfB3AsgCEANgOYZD1RRMpFZKmILN22x7+PNRI1FxkVu6rWqOo+Vd0P4EkAZY7nTlHVUlUt7dTacZOIiHIqo2IXkeJGX44CUBlPOkSUK6Kq7ieIzAJwLoAuCHZiGh9+PQSAAtgA4BZVda1WBgA4plcLvemugsjY/WNd87KaTscPt4NX2UNXcvJvYs1jXHd7WGvimOiZYQCAH11ux44Z5TijNatsvaOPa0Mpe7aZe+S2jdHu2syr1hE7zhFzDLPWz4hsnnTWvWaXu+0lCpuNjQOjfwYu++Q1VOzeJlGxlOPsqnp9RPPUpqVGRPnGT9AReYLFTuQJFjuRJ1jsRJ5gsRN5IuXQW6wnE0nsZIvPtmNlc0Y7ehbaoZ39otv7/Z3dp365HXvK/OAhcNNsOwbHSo+mbxyxCkdstSP2uSNmDfQ4Nq9a/54da+8YOOrquP6whje/dfRxWPi+GZIL/yWzY2agbG23yPbKUV9i16r6yKE3vrITeYLFTuQJFjuRJ1jsRJ5gsRN5gsVO5IlEh97aH9lZhw64JDK2qGJWrOd6daAdu+TV2+1ge8csr04/NgL2YoiL+tqLMg5xrNfY4YXkvi/uVcdcO8F96IhtN9qjh4wCrpl5rrUQHAtmYpDR7poD5hqKtBcCdf2/ibhmD8ZLVTn0RuQzFjuRJ1jsRJ5gsRN5gsVO5ImUy1LFae/erqir+6fI2Mlo+t34lY7YxCo7dknX3nbwCOvuLWBO4vjXm8wewzfYR9t2puvOdJKKHDHX2m/HOGLWnfqOGZ7LxfVjbN2pd53LMUlmjWPy0nFPmCHXqJdI5M3z2PGVncgTLHYiT7DYiTzBYifyBIudyBMsdiJPpLP9U08ATyH4lL8CmKKqk0WkCMBsAH0QbAF1japuS3Es82SOwTBzsMa1iphrL6pXHOvTnfmAY0umHtEd5xz/oNnlGkeS9T+wJ3e0mOmagHKSI5aJjY6Ya7KLaw26RdHNO13Hc0xoaT/U0c81jGbtOWoPsc4YYH9f/m+dfabH1bW11dFmpL9E/4R/jK8cx7NlMxFmL4C7VHUQgNMB3C4igwCMA7BQVQcAWBh+TUTNVMpiV9XNqro8fLwTQBWCnQBHAmjYNW8GgCtzlSQRZa9J79lFpA+AUwAsBtCt0c6tW+CeqExEeZb2x2VFpBDAHABjVXVH44/4qapa78dFpBxAebaJElF20nplF5ECBIU+U1Xnhs01IlIcxothbK6tqlNUtVRVS+NImIgyk7LYJXgJnwqgSlUfbhSaB6Bha5XRAF6IPz0iiks6Q2/DALwJYBWA/WHzvQjetz8LoBeATxEMvbkWM4t9+6dzHLEejphrxbKPHLEtRvvHjj4ui7vYsbLF9kw69DvPcdQbjPYdjj6/ccRa2iF91Y7NW2C0O0611xH7WS87Nih6JiUAQKPfqQ5pYW/V5JpN2dkR26qvOaLnmpEexqw3x0ZZWF12eWT7VZVvonLX9sgDpnzPrqpvAbDm4F2Qqj8RNQ/8BB2RJ1jsRJ5gsRN5gsVO5AkWO5EnEt3+Ke6ht0y1dsQucsT6Gu07HX1mOGKu37Q3OmLTbrMHD1s8agx5OQccXdsdOaYI4j079Ma/R7fPXWN2+dxKHUArxzqVb66yY1fbO3PFTqsd24od82sz1MUYevsy0zy4/ROR31jsRJ5gsRN5gsVO5AkWO5EnWOxEnvBy6C1uf+uIuRbFdCxtiRMcsecdsdfrjOGfToPNPpPKbjFjWx0z0V55344NMdov6m73mWhNKwTwgR1qNiY4YuPVnln448HR13+qY0hx0sLozQwfufVqfLa6kkNvRD5jsRN5gsVO5AkWO5EnWOxEnkh7Kek4HAXBOS2jTzlvX32SqcRqcYb9XJNuxlh31QEsKXrU7jj7ucjm9VXTzS53L3EkkiFrHbdZjjvue+JPI3bHO2KuJfSA5WbkbGNYxnU3/q4LBjrPFoWv7ESeYLETeYLFTuQJFjuRJ1jsRJ5gsRN5IuXQm4j0BPAUgi2ZFcAUVZ0sIhMA3Azgi/Cp96rqy65jHXVCe4yYflZkbN7przQh7cPDfzpiD83/0Iw9+CPHt61saGRz2+3vpJlVbh0Kw2sucx1L8t32ph2re8CeCPOta7aU4fU/RNdL+c/+2eyTzjj7XgB3qepyEWkPYJmINCwN+Iiqun5miaiZSGevt80ANoePd4pIFYCSXCdGRPFq0nt2EekD4BT85UNjd4hIhYhME5FOMedGRDFKu9hFpBDAHABjVXUHgMcBHItgnYLNACYZ/cpFZKmILP16+6H+jo3o0JVWsYtIAYJCn6mqcwFAVWtUdZ+q7gfwJICyqL6qOkVVS1W1tLCja3sGIsqllMUuIgJgKoAqVX24UXtxo6eNAlAZf3pEFJd07safhWA3olUisiJsuxfA9SIyBMFw3AYA9kJmoX27e2N7hTUE0SuNVA4vvR2xmVNfM2M3zIx8xxToGn3vtHuVa4unrxwxaqyw0I6tc/T7b8dExbZl7YzILrPPGcWTI9sLC2rMPuncjX8LQNQCds4xdSJqXvgJOiJPsNiJPMFiJ/IEi53IEyx2Ik9w+6ccO9YR+9gRO9URW/q6vRglBh0V3b7THnq784IXzdiSDfapMl1o81A2va0de94xe+0jxzHvuSL6e/b38zIbElVVbv9E5DMWO5EnWOxEnmCxE3mCxU7kCRY7kSc49HYIGumIXW9sAXbtsjFmn/WTo2dQAUCrr+1z/cnevgx3HKbrh17riM3O8JjRS7ACb2d4PA69EXmOxU7kCRY7kSdY7ESeYLETeYLFTuSJdBacpGbmBUdsS1V0+7WffGr2WbXWPl7b7Xase7Eds7YMqra7HBIyHV5zyWSIrfyl6EHAuWPmm334yk7kCRY7kSdY7ESeYLETeYLFTuSJlBNhRKQtgEUA2iC4e/+cqo4Xkb4AngHQGcAyADeqqnObVk6EicfxjtitJ0W3n3aN3ecf/82O3WkcDwD6n2LHHjJuCi/cYveheGQzEeY7AOer6skItme+VEROB/ArAI+oan8A2wDcFFeyRBS/lMWugYaJjgXhfwrgfADPhe0zAFyZkwyJKBbp7s/eMtzBtRbAAgSrIG9X1b3hUzbB/hwFETUDaRW7qu5T1SEAegAoA3BCuicQkXIRWSoiSzPMkYhi0KS78aq6HcBrAM4A0FFEGj5u2wPGJyFVdYqqlqpqaVaZElFWUha7iHQVkY7h4yMAXASgCkHRXx0+bTTcH9kmojxLZyJMMYAZItISwS+HZ1X1RRH5EMAzInI/gPcBTE11oMIjW+LUgYWRsTeWZbbVTdz+eOcbZkx2R2+h1Lu4zOzz7v4jzditv3SMazmsdsQ2fRHdXvDufrNPyYIfmLGOP3/GjM24YpsZ+6DVf0QHpj1s9mku5newp7tcvMO1Cl281s2wf3b6j/6mycdLWeyqWgHgr0ZUVXU9gvfvRHQI4CfoiDzBYifyBIudyBMsdiJPsNiJPJH09k9fAGhYDK0LgK2JndzGPA7EPA50qOXRW1W7RgUSLfYDTiyytDl8qo55MA9f8uCf8USeYLETeSKfxT4lj+dujHkciHkc6LDJI2/v2YkoWfwznsgTeSl2EblURFaLyDoRGZePHMI8NojIKhFZkeTiGiIyTURqRaSyUVuRiCwQkbXhv53ylMcEEakOr8kKERmRQB49ReQ1EflQRD4QkTFhe6LXxJFHotdERNqKyHsisjLM4xdhe18RWRzWzWwRad2kA6tqov8BaIlgWat+AFoDWAlgUNJ5hLlsANAlD+f9HoChACobtT0IYFz4eByAX+UpjwkA7k74ehQDGBo+bg9gDYBBSV8TRx6JXhMAAqAwfFwAYDGA0wE8C+C6sP0JALc25bj5eGUvA7BOVddrsPT0MwBG5iGPvFHVRQDqDmoeiWDhTiChBTyNPBKnqptVdXn4eCeCxVFKkPA1ceSRKA3EvshrPoq9BMBnjb7O52KVCmC+iCwTkfI85dCgm6puDh9vAdAtj7ncISIV4Z/5OX870ZiI9EGwfsJi5PGaHJQHkPA1ycUir77foBumqkMBDAdwu4h8L98JAcFvdgS/iPLhcQDHItgjYDOASUmdWEQKAcwBMFZVdzSOJXlNIvJI/JpoFou8WvJR7NUAejb62lysMtdUtTr8txbA88jvyjs1IlIMAOG/tflIQlVrwh+0/QCeRELXREQKEBTYTFWdGzYnfk2i8sjXNQnP3eRFXi35KPYlAAaEdxZbA7gOwLykkxCRdiLSvuExgIsBVLp75dQ8BAt3AnlcwLOhuEKjkMA1ERFBsIZhlao2XqQu0Wti5ZH0NcnZIq9J3WE86G7jCAR3Oj8GcF+ecuiHYCRgJYAPkswDwCwEfw7WI3jvdROCPfMWAlgL4M8AivKUx+8ArAJQgaDYihPIYxiCP9ErAKwI/xuR9DVx5JHoNQEwGMEirhUIfrH8vNHP7HsA1gH4A4A2TTkuP0FH5Anfb9AReYPFTuQJFjuRJ1jsRJ5gsRN5gsVO5AkWO5EnWOxEnvh/AJxErgPIbHIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "32 32 6 26 4 24\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "32 32 1 26 6 31\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "32 32 0 27 0 25\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "32 32 5 24 6 24\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "32 32 6 29 5 27\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gITLQIAr9lAZ"
      },
      "source": [
        "##**Main Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0-lYvAp9oHA",
        "outputId": "558d5c56-3112-4edf-97b0-ddfa35914b78"
      },
      "source": [
        "def train(train_loader, epoch, model, optimizer, criterion):\n",
        "    batch_time = AverageMeter('Time', ':6.3f')\n",
        "    losses = AverageMeter('Loss', ':.4e')\n",
        "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
        "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
        "    progress = ProgressMeter(len(train_loader), batch_time, losses,\n",
        "                             top1, top5, prefix=\"Epoch: [{}]\".format(epoch))\n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "\n",
        "    end = time.time()\n",
        "    for i, (input, target) in enumerate(train_loader):\n",
        "        # measure data loading time\n",
        "        input = input.cuda()\n",
        "        target = target.cuda()\n",
        "\n",
        "        # compute output\n",
        "        output = model(input)\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        # measure accuracy and record loss, accuracy \n",
        "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "        losses.update(loss.item(), input.size(0))\n",
        "        top1.update(acc1[0].item(), input.size(0))\n",
        "        top5.update(acc5[0].item(), input.size(0))\n",
        "\n",
        "        # compute gradient and do SGD step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        if i % print_freq == 0:\n",
        "            progress.print(i)\n",
        "\n",
        "    print('==> Train Accuracy: Acc@1 {top1.avg:.3f} || Acc@5 {top5.avg:.3f}'.format(top1=top1, top5=top5))\n",
        "    return top1.avg\n",
        "\n",
        "def test(test_loader,epoch, model):\n",
        "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
        "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
        "    model.eval()\n",
        "    for i,(input,target) in enumerate(test_loader):\n",
        "        input = input.cuda()\n",
        "        target = target.cuda()\n",
        "\n",
        "        output = model(input)\n",
        "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "        top1.update(acc1[0].item(), input.size(0))\n",
        "        top5.update(acc5[0].item(), input.size(0))\n",
        "    print('==> Test Accuracy:  Acc@1 {top1.avg:.3f} || Acc@5 {top5.avg:.3f}'.format(top1=top1, top5=top5))\n",
        "    return top1.avg\n",
        "\n",
        "model = ResNet34(num_classes=num_classes).cuda()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate,momentum=0.9, nesterov=True, weight_decay=5e-4)\n",
        "\n",
        "scheduler = MultiStepLR(optimizer, milestones=[60, 90, 120], gamma=0.2)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss().cuda()\n",
        "###########################################################\n",
        "best_acc = 0\n",
        "for epoch in range(epochs):\n",
        "    print(\"\\n----- epoch: {}, lr: {} -----\".format(\n",
        "        epoch, optimizer.param_groups[0][\"lr\"]))\n",
        "\n",
        "    # train for one epoch\n",
        "    start_time = time.time()\n",
        "    train(train_loader, epoch, model, optimizer, criterion)\n",
        "    test_acc = test(test_loader,epoch,model)\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print('==> {:.2f} seconds to train this epoch\\n'.format(elapsed_time))\n",
        "    # learning rate scheduling\n",
        "    scheduler.step()\n",
        "    \n",
        "    # Save model for best accuracy\n",
        "    if best_acc < test_acc:\n",
        "        best_acc = test_acc\n",
        "        torch.save(model.state_dict(), 'model_best.pt')\n",
        "\n",
        "torch.save(model.state_dict(),'model_latest.pt')\n",
        "print(f\"Best Top-1 Accuracy: {best_acc}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "----- epoch: 0, lr: 0.1 -----\n",
            "Epoch: [0][  0/391]\tTime  1.570 ( 1.570)\tLoss 4.7682e+00 (4.7682e+00)\tAcc@1   0.00 (  0.00)\tAcc@5   7.03 (  7.03)\n",
            "Epoch: [0][ 30/391]\tTime  0.155 ( 0.250)\tLoss 4.5968e+00 (5.3534e+00)\tAcc@1   0.00 (  1.29)\tAcc@5   7.03 (  5.97)\n",
            "Epoch: [0][ 60/391]\tTime  0.158 ( 0.238)\tLoss 4.5382e+00 (4.9877e+00)\tAcc@1   1.56 (  1.72)\tAcc@5   8.59 (  7.39)\n",
            "Epoch: [0][ 90/391]\tTime  0.177 ( 0.232)\tLoss 4.4491e+00 (4.8058e+00)\tAcc@1   5.47 (  2.11)\tAcc@5  14.06 (  9.48)\n",
            "Epoch: [0][120/391]\tTime  0.155 ( 0.229)\tLoss 4.0450e+00 (4.6762e+00)\tAcc@1   7.03 (  2.63)\tAcc@5  24.22 ( 11.14)\n",
            "Epoch: [0][150/391]\tTime  0.155 ( 0.229)\tLoss 4.2458e+00 (4.5801e+00)\tAcc@1   4.69 (  2.90)\tAcc@5  16.41 ( 12.57)\n",
            "Epoch: [0][180/391]\tTime  0.270 ( 0.229)\tLoss 4.1742e+00 (4.5125e+00)\tAcc@1   2.34 (  3.15)\tAcc@5  17.97 ( 13.75)\n",
            "Epoch: [0][210/391]\tTime  0.292 ( 0.228)\tLoss 4.0599e+00 (4.4527e+00)\tAcc@1   7.81 (  3.56)\tAcc@5  28.91 ( 14.92)\n",
            "Epoch: [0][240/391]\tTime  0.169 ( 0.226)\tLoss 4.0394e+00 (4.4066e+00)\tAcc@1   4.69 (  3.86)\tAcc@5  20.31 ( 15.80)\n",
            "Epoch: [0][270/391]\tTime  0.173 ( 0.226)\tLoss 4.1372e+00 (4.3640e+00)\tAcc@1   6.25 (  4.12)\tAcc@5  21.88 ( 16.83)\n",
            "Epoch: [0][300/391]\tTime  0.167 ( 0.227)\tLoss 3.9789e+00 (4.3281e+00)\tAcc@1   8.59 (  4.43)\tAcc@5  29.69 ( 17.73)\n",
            "Epoch: [0][330/391]\tTime  0.169 ( 0.227)\tLoss 4.2578e+00 (4.2965e+00)\tAcc@1   6.25 (  4.68)\tAcc@5  18.75 ( 18.47)\n",
            "Epoch: [0][360/391]\tTime  0.223 ( 0.226)\tLoss 3.8089e+00 (4.2680e+00)\tAcc@1  14.06 (  4.95)\tAcc@5  32.81 ( 19.19)\n",
            "Epoch: [0][390/391]\tTime  0.716 ( 0.227)\tLoss 3.4789e+00 (4.2382e+00)\tAcc@1  17.50 (  5.31)\tAcc@5  41.25 ( 19.94)\n",
            "==> Train Accuracy: Acc@1 5.308 || Acc@5 19.936\n",
            "==> Test Accuracy:  Acc@1 9.200 || Acc@5 30.050\n",
            "==> 93.05 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 1, lr: 0.1 -----\n",
            "Epoch: [1][  0/391]\tTime  0.573 ( 0.573)\tLoss 3.9367e+00 (3.9367e+00)\tAcc@1   7.03 (  7.03)\tAcc@5  28.12 ( 28.12)\n",
            "Epoch: [1][ 30/391]\tTime  0.232 ( 0.245)\tLoss 3.9842e+00 (3.8614e+00)\tAcc@1  10.16 (  9.43)\tAcc@5  28.12 ( 30.49)\n",
            "Epoch: [1][ 60/391]\tTime  0.242 ( 0.239)\tLoss 3.8722e+00 (3.8448e+00)\tAcc@1   9.38 (  9.36)\tAcc@5  33.59 ( 31.11)\n",
            "Epoch: [1][ 90/391]\tTime  0.276 ( 0.238)\tLoss 3.6888e+00 (3.8323e+00)\tAcc@1  10.16 (  9.62)\tAcc@5  30.47 ( 31.48)\n",
            "Epoch: [1][120/391]\tTime  0.314 ( 0.233)\tLoss 3.6121e+00 (3.8163e+00)\tAcc@1  15.62 (  9.89)\tAcc@5  43.75 ( 32.00)\n",
            "Epoch: [1][150/391]\tTime  0.241 ( 0.230)\tLoss 3.6949e+00 (3.7981e+00)\tAcc@1  10.94 ( 10.23)\tAcc@5  42.97 ( 32.57)\n",
            "Epoch: [1][180/391]\tTime  0.363 ( 0.230)\tLoss 3.6792e+00 (3.7834e+00)\tAcc@1  10.94 ( 10.49)\tAcc@5  34.38 ( 32.96)\n",
            "Epoch: [1][210/391]\tTime  0.349 ( 0.230)\tLoss 3.6879e+00 (3.7661e+00)\tAcc@1  10.94 ( 10.83)\tAcc@5  34.38 ( 33.58)\n",
            "Epoch: [1][240/391]\tTime  0.300 ( 0.230)\tLoss 3.4757e+00 (3.7426e+00)\tAcc@1  17.97 ( 11.29)\tAcc@5  44.53 ( 34.34)\n",
            "Epoch: [1][270/391]\tTime  0.161 ( 0.230)\tLoss 3.3781e+00 (3.7241e+00)\tAcc@1  19.53 ( 11.67)\tAcc@5  46.09 ( 34.80)\n",
            "Epoch: [1][300/391]\tTime  0.164 ( 0.230)\tLoss 3.4054e+00 (3.7048e+00)\tAcc@1  23.44 ( 11.99)\tAcc@5  47.66 ( 35.48)\n",
            "Epoch: [1][330/391]\tTime  0.170 ( 0.230)\tLoss 3.5913e+00 (3.6874e+00)\tAcc@1  11.72 ( 12.26)\tAcc@5  32.81 ( 35.82)\n",
            "Epoch: [1][360/391]\tTime  0.171 ( 0.229)\tLoss 3.5635e+00 (3.6711e+00)\tAcc@1  20.31 ( 12.59)\tAcc@5  42.97 ( 36.37)\n",
            "Epoch: [1][390/391]\tTime  0.155 ( 0.228)\tLoss 3.1892e+00 (3.6519e+00)\tAcc@1  17.50 ( 12.91)\tAcc@5  48.75 ( 36.91)\n",
            "==> Train Accuracy: Acc@1 12.906 || Acc@5 36.908\n",
            "==> Test Accuracy:  Acc@1 16.600 || Acc@5 45.120\n",
            "==> 93.45 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 2, lr: 0.1 -----\n",
            "Epoch: [2][  0/391]\tTime  0.564 ( 0.564)\tLoss 3.4196e+00 (3.4196e+00)\tAcc@1  14.84 ( 14.84)\tAcc@5  47.66 ( 47.66)\n",
            "Epoch: [2][ 30/391]\tTime  0.277 ( 0.237)\tLoss 3.1548e+00 (3.3648e+00)\tAcc@1  23.44 ( 17.77)\tAcc@5  53.12 ( 46.37)\n",
            "Epoch: [2][ 60/391]\tTime  0.187 ( 0.228)\tLoss 3.5007e+00 (3.3664e+00)\tAcc@1  21.09 ( 17.55)\tAcc@5  39.84 ( 46.20)\n",
            "Epoch: [2][ 90/391]\tTime  0.169 ( 0.226)\tLoss 3.3566e+00 (3.3595e+00)\tAcc@1  16.41 ( 17.80)\tAcc@5  44.53 ( 46.21)\n",
            "Epoch: [2][120/391]\tTime  0.327 ( 0.227)\tLoss 3.5508e+00 (3.3529e+00)\tAcc@1  13.28 ( 18.21)\tAcc@5  42.19 ( 46.36)\n",
            "Epoch: [2][150/391]\tTime  0.220 ( 0.227)\tLoss 3.2588e+00 (3.3473e+00)\tAcc@1  15.62 ( 18.28)\tAcc@5  49.22 ( 46.37)\n",
            "Epoch: [2][180/391]\tTime  0.179 ( 0.227)\tLoss 3.2440e+00 (3.3340e+00)\tAcc@1  21.09 ( 18.60)\tAcc@5  46.88 ( 46.78)\n",
            "Epoch: [2][210/391]\tTime  0.187 ( 0.227)\tLoss 3.0817e+00 (3.3202e+00)\tAcc@1  22.66 ( 18.81)\tAcc@5  52.34 ( 47.07)\n",
            "Epoch: [2][240/391]\tTime  0.313 ( 0.229)\tLoss 3.3326e+00 (3.3070e+00)\tAcc@1  15.62 ( 19.14)\tAcc@5  53.12 ( 47.30)\n",
            "Epoch: [2][270/391]\tTime  0.261 ( 0.230)\tLoss 3.2864e+00 (3.2935e+00)\tAcc@1  20.31 ( 19.25)\tAcc@5  44.53 ( 47.66)\n",
            "Epoch: [2][300/391]\tTime  0.260 ( 0.231)\tLoss 2.7979e+00 (3.2779e+00)\tAcc@1  25.00 ( 19.48)\tAcc@5  60.94 ( 48.01)\n",
            "Epoch: [2][330/391]\tTime  0.168 ( 0.231)\tLoss 3.1730e+00 (3.2666e+00)\tAcc@1  21.88 ( 19.71)\tAcc@5  52.34 ( 48.36)\n",
            "Epoch: [2][360/391]\tTime  0.173 ( 0.230)\tLoss 2.9920e+00 (3.2513e+00)\tAcc@1  33.59 ( 20.03)\tAcc@5  54.69 ( 48.79)\n",
            "Epoch: [2][390/391]\tTime  0.150 ( 0.230)\tLoss 3.3334e+00 (3.2388e+00)\tAcc@1  18.75 ( 20.31)\tAcc@5  46.25 ( 49.15)\n",
            "==> Train Accuracy: Acc@1 20.308 || Acc@5 49.150\n",
            "==> Test Accuracy:  Acc@1 24.100 || Acc@5 54.120\n",
            "==> 94.17 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 3, lr: 0.1 -----\n",
            "Epoch: [3][  0/391]\tTime  0.552 ( 0.552)\tLoss 3.1521e+00 (3.1521e+00)\tAcc@1  25.78 ( 25.78)\tAcc@5  56.25 ( 56.25)\n",
            "Epoch: [3][ 30/391]\tTime  0.282 ( 0.240)\tLoss 3.1366e+00 (3.0426e+00)\tAcc@1  21.09 ( 23.84)\tAcc@5  51.56 ( 54.33)\n",
            "Epoch: [3][ 60/391]\tTime  0.239 ( 0.230)\tLoss 3.0358e+00 (3.0207e+00)\tAcc@1  21.88 ( 24.32)\tAcc@5  53.91 ( 55.15)\n",
            "Epoch: [3][ 90/391]\tTime  0.262 ( 0.231)\tLoss 2.9787e+00 (3.0000e+00)\tAcc@1  27.34 ( 25.12)\tAcc@5  54.69 ( 55.50)\n",
            "Epoch: [3][120/391]\tTime  0.264 ( 0.231)\tLoss 2.7651e+00 (2.9934e+00)\tAcc@1  30.47 ( 25.20)\tAcc@5  62.50 ( 55.61)\n",
            "Epoch: [3][150/391]\tTime  0.271 ( 0.231)\tLoss 2.9241e+00 (2.9861e+00)\tAcc@1  25.78 ( 25.24)\tAcc@5  59.38 ( 55.77)\n",
            "Epoch: [3][180/391]\tTime  0.264 ( 0.230)\tLoss 3.1048e+00 (2.9906e+00)\tAcc@1  24.22 ( 25.06)\tAcc@5  49.22 ( 55.50)\n",
            "Epoch: [3][210/391]\tTime  0.245 ( 0.231)\tLoss 2.9131e+00 (2.9778e+00)\tAcc@1  24.22 ( 25.35)\tAcc@5  59.38 ( 55.95)\n",
            "Epoch: [3][240/391]\tTime  0.154 ( 0.230)\tLoss 3.0154e+00 (2.9691e+00)\tAcc@1  21.09 ( 25.44)\tAcc@5  58.59 ( 56.16)\n",
            "Epoch: [3][270/391]\tTime  0.177 ( 0.230)\tLoss 2.7388e+00 (2.9564e+00)\tAcc@1  31.25 ( 25.65)\tAcc@5  63.28 ( 56.58)\n",
            "Epoch: [3][300/391]\tTime  0.173 ( 0.230)\tLoss 2.6765e+00 (2.9470e+00)\tAcc@1  37.50 ( 25.79)\tAcc@5  67.19 ( 56.78)\n",
            "Epoch: [3][330/391]\tTime  0.158 ( 0.230)\tLoss 2.7992e+00 (2.9377e+00)\tAcc@1  26.56 ( 25.90)\tAcc@5  64.06 ( 57.06)\n",
            "Epoch: [3][360/391]\tTime  0.260 ( 0.229)\tLoss 2.6843e+00 (2.9288e+00)\tAcc@1  35.16 ( 26.11)\tAcc@5  64.06 ( 57.21)\n",
            "Epoch: [3][390/391]\tTime  0.154 ( 0.229)\tLoss 2.8776e+00 (2.9186e+00)\tAcc@1  26.25 ( 26.30)\tAcc@5  60.00 ( 57.45)\n",
            "==> Train Accuracy: Acc@1 26.298 || Acc@5 57.446\n",
            "==> Test Accuracy:  Acc@1 29.500 || Acc@5 60.430\n",
            "==> 93.83 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 4, lr: 0.1 -----\n",
            "Epoch: [4][  0/391]\tTime  0.547 ( 0.547)\tLoss 2.6008e+00 (2.6008e+00)\tAcc@1  30.47 ( 30.47)\tAcc@5  60.16 ( 60.16)\n",
            "Epoch: [4][ 30/391]\tTime  0.299 ( 0.236)\tLoss 2.6564e+00 (2.7145e+00)\tAcc@1  31.25 ( 29.91)\tAcc@5  68.75 ( 62.85)\n",
            "Epoch: [4][ 60/391]\tTime  0.183 ( 0.235)\tLoss 2.6768e+00 (2.6960e+00)\tAcc@1  32.03 ( 30.25)\tAcc@5  62.50 ( 63.09)\n",
            "Epoch: [4][ 90/391]\tTime  0.203 ( 0.232)\tLoss 2.7972e+00 (2.6960e+00)\tAcc@1  28.12 ( 30.68)\tAcc@5  67.19 ( 62.98)\n",
            "Epoch: [4][120/391]\tTime  0.356 ( 0.231)\tLoss 2.5772e+00 (2.6730e+00)\tAcc@1  32.81 ( 30.90)\tAcc@5  67.97 ( 63.56)\n",
            "Epoch: [4][150/391]\tTime  0.179 ( 0.229)\tLoss 2.4225e+00 (2.6628e+00)\tAcc@1  37.50 ( 31.15)\tAcc@5  67.19 ( 63.59)\n",
            "Epoch: [4][180/391]\tTime  0.174 ( 0.230)\tLoss 2.6171e+00 (2.6596e+00)\tAcc@1  35.94 ( 31.23)\tAcc@5  64.06 ( 63.62)\n",
            "Epoch: [4][210/391]\tTime  0.159 ( 0.231)\tLoss 2.7223e+00 (2.6609e+00)\tAcc@1  30.47 ( 31.16)\tAcc@5  60.94 ( 63.48)\n",
            "Epoch: [4][240/391]\tTime  0.176 ( 0.232)\tLoss 2.5898e+00 (2.6495e+00)\tAcc@1  28.12 ( 31.32)\tAcc@5  69.53 ( 63.76)\n",
            "Epoch: [4][270/391]\tTime  0.159 ( 0.232)\tLoss 2.6072e+00 (2.6495e+00)\tAcc@1  30.47 ( 31.33)\tAcc@5  59.38 ( 63.81)\n",
            "Epoch: [4][300/391]\tTime  0.171 ( 0.231)\tLoss 2.5420e+00 (2.6381e+00)\tAcc@1  34.38 ( 31.69)\tAcc@5  64.84 ( 63.98)\n",
            "Epoch: [4][330/391]\tTime  0.184 ( 0.232)\tLoss 2.4761e+00 (2.6297e+00)\tAcc@1  35.94 ( 31.93)\tAcc@5  65.62 ( 64.17)\n",
            "Epoch: [4][360/391]\tTime  0.297 ( 0.232)\tLoss 2.5106e+00 (2.6226e+00)\tAcc@1  39.06 ( 32.11)\tAcc@5  65.62 ( 64.36)\n",
            "Epoch: [4][390/391]\tTime  0.149 ( 0.231)\tLoss 2.6855e+00 (2.6135e+00)\tAcc@1  33.75 ( 32.40)\tAcc@5  67.50 ( 64.63)\n",
            "==> Train Accuracy: Acc@1 32.398 || Acc@5 64.634\n",
            "==> Test Accuracy:  Acc@1 34.060 || Acc@5 65.780\n",
            "==> 94.52 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 5, lr: 0.1 -----\n",
            "Epoch: [5][  0/391]\tTime  0.570 ( 0.570)\tLoss 2.2165e+00 (2.2165e+00)\tAcc@1  44.53 ( 44.53)\tAcc@5  76.56 ( 76.56)\n",
            "Epoch: [5][ 30/391]\tTime  0.329 ( 0.244)\tLoss 2.3690e+00 (2.3819e+00)\tAcc@1  36.72 ( 36.90)\tAcc@5  72.66 ( 70.59)\n",
            "Epoch: [5][ 60/391]\tTime  0.195 ( 0.237)\tLoss 2.4061e+00 (2.4013e+00)\tAcc@1  32.81 ( 36.16)\tAcc@5  74.22 ( 69.93)\n",
            "Epoch: [5][ 90/391]\tTime  0.315 ( 0.236)\tLoss 2.3405e+00 (2.3986e+00)\tAcc@1  40.62 ( 36.44)\tAcc@5  71.88 ( 69.89)\n",
            "Epoch: [5][120/391]\tTime  0.349 ( 0.235)\tLoss 2.3502e+00 (2.3972e+00)\tAcc@1  37.50 ( 36.40)\tAcc@5  75.00 ( 70.05)\n",
            "Epoch: [5][150/391]\tTime  0.288 ( 0.235)\tLoss 2.5350e+00 (2.3880e+00)\tAcc@1  27.34 ( 36.78)\tAcc@5  65.62 ( 70.19)\n",
            "Epoch: [5][180/391]\tTime  0.162 ( 0.233)\tLoss 2.0182e+00 (2.3823e+00)\tAcc@1  46.09 ( 36.93)\tAcc@5  78.91 ( 70.23)\n",
            "Epoch: [5][210/391]\tTime  0.166 ( 0.233)\tLoss 2.1916e+00 (2.3732e+00)\tAcc@1  39.06 ( 37.16)\tAcc@5  76.56 ( 70.35)\n",
            "Epoch: [5][240/391]\tTime  0.183 ( 0.233)\tLoss 2.1370e+00 (2.3701e+00)\tAcc@1  43.75 ( 37.25)\tAcc@5  76.56 ( 70.34)\n",
            "Epoch: [5][270/391]\tTime  0.326 ( 0.233)\tLoss 2.2137e+00 (2.3639e+00)\tAcc@1  40.62 ( 37.52)\tAcc@5  75.78 ( 70.55)\n",
            "Epoch: [5][300/391]\tTime  0.250 ( 0.232)\tLoss 2.0374e+00 (2.3562e+00)\tAcc@1  39.84 ( 37.68)\tAcc@5  78.91 ( 70.72)\n",
            "Epoch: [5][330/391]\tTime  0.170 ( 0.231)\tLoss 2.2682e+00 (2.3541e+00)\tAcc@1  42.97 ( 37.73)\tAcc@5  71.09 ( 70.77)\n",
            "Epoch: [5][360/391]\tTime  0.172 ( 0.230)\tLoss 2.4890e+00 (2.3491e+00)\tAcc@1  34.38 ( 37.76)\tAcc@5  67.19 ( 70.95)\n",
            "Epoch: [5][390/391]\tTime  0.154 ( 0.229)\tLoss 1.8817e+00 (2.3389e+00)\tAcc@1  47.50 ( 38.01)\tAcc@5  85.00 ( 71.18)\n",
            "==> Train Accuracy: Acc@1 38.008 || Acc@5 71.178\n",
            "==> Test Accuracy:  Acc@1 39.800 || Acc@5 72.150\n",
            "==> 93.76 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 6, lr: 0.1 -----\n",
            "Epoch: [6][  0/391]\tTime  0.590 ( 0.590)\tLoss 2.4672e+00 (2.4672e+00)\tAcc@1  32.03 ( 32.03)\tAcc@5  69.53 ( 69.53)\n",
            "Epoch: [6][ 30/391]\tTime  0.319 ( 0.245)\tLoss 2.0948e+00 (2.1425e+00)\tAcc@1  40.62 ( 41.31)\tAcc@5  78.91 ( 75.38)\n",
            "Epoch: [6][ 60/391]\tTime  0.265 ( 0.237)\tLoss 1.9132e+00 (2.1359e+00)\tAcc@1  50.00 ( 42.15)\tAcc@5  78.12 ( 75.08)\n",
            "Epoch: [6][ 90/391]\tTime  0.254 ( 0.233)\tLoss 2.3578e+00 (2.1234e+00)\tAcc@1  33.59 ( 42.54)\tAcc@5  67.19 ( 74.96)\n",
            "Epoch: [6][120/391]\tTime  0.256 ( 0.229)\tLoss 2.2671e+00 (2.1302e+00)\tAcc@1  41.41 ( 42.47)\tAcc@5  73.44 ( 74.87)\n",
            "Epoch: [6][150/391]\tTime  0.322 ( 0.228)\tLoss 2.2852e+00 (2.1318e+00)\tAcc@1  37.50 ( 42.50)\tAcc@5  72.66 ( 74.79)\n",
            "Epoch: [6][180/391]\tTime  0.243 ( 0.228)\tLoss 2.3468e+00 (2.1253e+00)\tAcc@1  39.06 ( 42.82)\tAcc@5  67.97 ( 74.87)\n",
            "Epoch: [6][210/391]\tTime  0.178 ( 0.227)\tLoss 2.1421e+00 (2.1315e+00)\tAcc@1  42.97 ( 42.72)\tAcc@5  77.34 ( 74.85)\n",
            "Epoch: [6][240/391]\tTime  0.262 ( 0.227)\tLoss 2.1542e+00 (2.1273e+00)\tAcc@1  44.53 ( 42.87)\tAcc@5  77.34 ( 74.94)\n",
            "Epoch: [6][270/391]\tTime  0.182 ( 0.227)\tLoss 2.1505e+00 (2.1263e+00)\tAcc@1  44.53 ( 42.88)\tAcc@5  67.97 ( 74.99)\n",
            "Epoch: [6][300/391]\tTime  0.248 ( 0.227)\tLoss 1.8878e+00 (2.1228e+00)\tAcc@1  47.66 ( 42.95)\tAcc@5  85.16 ( 75.18)\n",
            "Epoch: [6][330/391]\tTime  0.318 ( 0.227)\tLoss 1.7238e+00 (2.1174e+00)\tAcc@1  53.12 ( 43.06)\tAcc@5  83.59 ( 75.30)\n",
            "Epoch: [6][360/391]\tTime  0.238 ( 0.227)\tLoss 1.9688e+00 (2.1129e+00)\tAcc@1  43.75 ( 43.15)\tAcc@5  80.47 ( 75.39)\n",
            "Epoch: [6][390/391]\tTime  0.151 ( 0.226)\tLoss 1.9066e+00 (2.1081e+00)\tAcc@1  48.75 ( 43.27)\tAcc@5  78.75 ( 75.49)\n",
            "==> Train Accuracy: Acc@1 43.274 || Acc@5 75.488\n",
            "==> Test Accuracy:  Acc@1 42.390 || Acc@5 73.770\n",
            "==> 92.58 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 7, lr: 0.1 -----\n",
            "Epoch: [7][  0/391]\tTime  0.582 ( 0.582)\tLoss 1.7984e+00 (1.7984e+00)\tAcc@1  53.91 ( 53.91)\tAcc@5  78.12 ( 78.12)\n",
            "Epoch: [7][ 30/391]\tTime  0.258 ( 0.242)\tLoss 1.8869e+00 (1.9990e+00)\tAcc@1  45.31 ( 45.74)\tAcc@5  80.47 ( 78.20)\n",
            "Epoch: [7][ 60/391]\tTime  0.353 ( 0.234)\tLoss 1.7714e+00 (1.9614e+00)\tAcc@1  52.34 ( 46.99)\tAcc@5  83.59 ( 78.64)\n",
            "Epoch: [7][ 90/391]\tTime  0.288 ( 0.231)\tLoss 2.0443e+00 (1.9612e+00)\tAcc@1  46.09 ( 47.10)\tAcc@5  75.00 ( 78.62)\n",
            "Epoch: [7][120/391]\tTime  0.224 ( 0.231)\tLoss 2.0071e+00 (1.9600e+00)\tAcc@1  46.88 ( 47.01)\tAcc@5  72.66 ( 78.62)\n",
            "Epoch: [7][150/391]\tTime  0.294 ( 0.231)\tLoss 1.8729e+00 (1.9575e+00)\tAcc@1  48.44 ( 47.06)\tAcc@5  79.69 ( 78.71)\n",
            "Epoch: [7][180/391]\tTime  0.248 ( 0.230)\tLoss 2.0961e+00 (1.9654e+00)\tAcc@1  42.97 ( 46.87)\tAcc@5  75.00 ( 78.41)\n",
            "Epoch: [7][210/391]\tTime  0.203 ( 0.228)\tLoss 1.9286e+00 (1.9559e+00)\tAcc@1  47.66 ( 47.11)\tAcc@5  77.34 ( 78.54)\n",
            "Epoch: [7][240/391]\tTime  0.270 ( 0.228)\tLoss 1.7262e+00 (1.9454e+00)\tAcc@1  50.00 ( 47.30)\tAcc@5  85.16 ( 78.71)\n",
            "Epoch: [7][270/391]\tTime  0.163 ( 0.227)\tLoss 1.5049e+00 (1.9357e+00)\tAcc@1  56.25 ( 47.50)\tAcc@5  85.94 ( 78.83)\n",
            "Epoch: [7][300/391]\tTime  0.192 ( 0.228)\tLoss 1.8193e+00 (1.9364e+00)\tAcc@1  53.12 ( 47.43)\tAcc@5  83.59 ( 78.80)\n",
            "Epoch: [7][330/391]\tTime  0.175 ( 0.229)\tLoss 1.6237e+00 (1.9376e+00)\tAcc@1  53.12 ( 47.34)\tAcc@5  86.72 ( 78.82)\n",
            "Epoch: [7][360/391]\tTime  0.163 ( 0.228)\tLoss 2.0627e+00 (1.9340e+00)\tAcc@1  46.09 ( 47.43)\tAcc@5  78.91 ( 78.91)\n",
            "Epoch: [7][390/391]\tTime  0.142 ( 0.228)\tLoss 2.0216e+00 (1.9328e+00)\tAcc@1  36.25 ( 47.42)\tAcc@5  78.75 ( 78.89)\n",
            "==> Train Accuracy: Acc@1 47.418 || Acc@5 78.894\n",
            "==> Test Accuracy:  Acc@1 43.940 || Acc@5 75.930\n",
            "==> 93.33 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 8, lr: 0.1 -----\n",
            "Epoch: [8][  0/391]\tTime  0.604 ( 0.604)\tLoss 1.8684e+00 (1.8684e+00)\tAcc@1  42.19 ( 42.19)\tAcc@5  81.25 ( 81.25)\n",
            "Epoch: [8][ 30/391]\tTime  0.296 ( 0.246)\tLoss 1.6999e+00 (1.7452e+00)\tAcc@1  54.69 ( 50.76)\tAcc@5  82.03 ( 83.14)\n",
            "Epoch: [8][ 60/391]\tTime  0.307 ( 0.241)\tLoss 1.5551e+00 (1.7531e+00)\tAcc@1  59.38 ( 51.78)\tAcc@5  83.59 ( 82.81)\n",
            "Epoch: [8][ 90/391]\tTime  0.275 ( 0.242)\tLoss 1.7163e+00 (1.7753e+00)\tAcc@1  53.12 ( 50.76)\tAcc@5  85.16 ( 82.26)\n",
            "Epoch: [8][120/391]\tTime  0.279 ( 0.241)\tLoss 1.7686e+00 (1.7810e+00)\tAcc@1  45.31 ( 50.80)\tAcc@5  78.12 ( 81.99)\n",
            "Epoch: [8][150/391]\tTime  0.275 ( 0.240)\tLoss 2.0553e+00 (1.7885e+00)\tAcc@1  43.75 ( 50.79)\tAcc@5  78.12 ( 81.84)\n",
            "Epoch: [8][180/391]\tTime  0.331 ( 0.239)\tLoss 1.9418e+00 (1.7983e+00)\tAcc@1  49.22 ( 50.46)\tAcc@5  78.12 ( 81.67)\n",
            "Epoch: [8][210/391]\tTime  0.314 ( 0.239)\tLoss 1.7838e+00 (1.8027e+00)\tAcc@1  52.34 ( 50.33)\tAcc@5  79.69 ( 81.59)\n",
            "Epoch: [8][240/391]\tTime  0.325 ( 0.237)\tLoss 1.7010e+00 (1.7961e+00)\tAcc@1  52.34 ( 50.43)\tAcc@5  82.81 ( 81.66)\n",
            "Epoch: [8][270/391]\tTime  0.177 ( 0.236)\tLoss 1.9533e+00 (1.8009e+00)\tAcc@1  51.56 ( 50.27)\tAcc@5  78.91 ( 81.65)\n",
            "Epoch: [8][300/391]\tTime  0.270 ( 0.235)\tLoss 1.7300e+00 (1.7973e+00)\tAcc@1  57.81 ( 50.35)\tAcc@5  83.59 ( 81.70)\n",
            "Epoch: [8][330/391]\tTime  0.200 ( 0.234)\tLoss 2.0650e+00 (1.7979e+00)\tAcc@1  46.88 ( 50.35)\tAcc@5  74.22 ( 81.67)\n",
            "Epoch: [8][360/391]\tTime  0.177 ( 0.233)\tLoss 2.0251e+00 (1.7991e+00)\tAcc@1  45.31 ( 50.38)\tAcc@5  82.03 ( 81.65)\n",
            "Epoch: [8][390/391]\tTime  0.151 ( 0.233)\tLoss 1.6102e+00 (1.7972e+00)\tAcc@1  57.50 ( 50.37)\tAcc@5  87.50 ( 81.67)\n",
            "==> Train Accuracy: Acc@1 50.370 || Acc@5 81.670\n",
            "==> Test Accuracy:  Acc@1 46.490 || Acc@5 78.130\n",
            "==> 95.18 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 9, lr: 0.1 -----\n",
            "Epoch: [9][  0/391]\tTime  0.596 ( 0.596)\tLoss 1.5920e+00 (1.5920e+00)\tAcc@1  53.91 ( 53.91)\tAcc@5  78.91 ( 78.91)\n",
            "Epoch: [9][ 30/391]\tTime  0.274 ( 0.243)\tLoss 1.4017e+00 (1.5989e+00)\tAcc@1  61.72 ( 55.49)\tAcc@5  84.38 ( 84.45)\n",
            "Epoch: [9][ 60/391]\tTime  0.171 ( 0.232)\tLoss 1.8320e+00 (1.6363e+00)\tAcc@1  48.44 ( 54.21)\tAcc@5  84.38 ( 84.34)\n",
            "Epoch: [9][ 90/391]\tTime  0.294 ( 0.231)\tLoss 1.7886e+00 (1.6462e+00)\tAcc@1  50.78 ( 54.14)\tAcc@5  85.16 ( 84.44)\n",
            "Epoch: [9][120/391]\tTime  0.303 ( 0.231)\tLoss 1.4945e+00 (1.6729e+00)\tAcc@1  58.59 ( 53.54)\tAcc@5  85.16 ( 83.88)\n",
            "Epoch: [9][150/391]\tTime  0.214 ( 0.230)\tLoss 1.9083e+00 (1.6713e+00)\tAcc@1  50.00 ( 53.49)\tAcc@5  83.59 ( 83.84)\n",
            "Epoch: [9][180/391]\tTime  0.341 ( 0.230)\tLoss 2.0355e+00 (1.6793e+00)\tAcc@1  45.31 ( 53.40)\tAcc@5  79.69 ( 83.72)\n",
            "Epoch: [9][210/391]\tTime  0.307 ( 0.230)\tLoss 1.9989e+00 (1.6835e+00)\tAcc@1  42.19 ( 53.31)\tAcc@5  75.78 ( 83.65)\n",
            "Epoch: [9][240/391]\tTime  0.249 ( 0.230)\tLoss 1.8149e+00 (1.6868e+00)\tAcc@1  50.00 ( 53.27)\tAcc@5  81.25 ( 83.53)\n",
            "Epoch: [9][270/391]\tTime  0.279 ( 0.229)\tLoss 1.7587e+00 (1.6883e+00)\tAcc@1  50.78 ( 53.29)\tAcc@5  82.03 ( 83.50)\n",
            "Epoch: [9][300/391]\tTime  0.287 ( 0.228)\tLoss 1.7375e+00 (1.6845e+00)\tAcc@1  57.81 ( 53.37)\tAcc@5  77.34 ( 83.54)\n",
            "Epoch: [9][330/391]\tTime  0.301 ( 0.228)\tLoss 1.6766e+00 (1.6815e+00)\tAcc@1  56.25 ( 53.39)\tAcc@5  82.03 ( 83.58)\n",
            "Epoch: [9][360/391]\tTime  0.261 ( 0.228)\tLoss 1.7489e+00 (1.6856e+00)\tAcc@1  48.44 ( 53.31)\tAcc@5  80.47 ( 83.51)\n",
            "Epoch: [9][390/391]\tTime  0.151 ( 0.228)\tLoss 1.8762e+00 (1.6901e+00)\tAcc@1  48.75 ( 53.23)\tAcc@5  83.75 ( 83.41)\n",
            "==> Train Accuracy: Acc@1 53.226 || Acc@5 83.414\n",
            "==> Test Accuracy:  Acc@1 47.060 || Acc@5 77.810\n",
            "==> 93.21 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 10, lr: 0.1 -----\n",
            "Epoch: [10][  0/391]\tTime  0.642 ( 0.642)\tLoss 1.6858e+00 (1.6858e+00)\tAcc@1  57.81 ( 57.81)\tAcc@5  77.34 ( 77.34)\n",
            "Epoch: [10][ 30/391]\tTime  0.165 ( 0.234)\tLoss 1.4013e+00 (1.5031e+00)\tAcc@1  58.59 ( 57.33)\tAcc@5  88.28 ( 86.29)\n",
            "Epoch: [10][ 60/391]\tTime  0.281 ( 0.228)\tLoss 1.6916e+00 (1.5444e+00)\tAcc@1  53.91 ( 56.38)\tAcc@5  83.59 ( 85.71)\n",
            "Epoch: [10][ 90/391]\tTime  0.181 ( 0.228)\tLoss 1.4650e+00 (1.5636e+00)\tAcc@1  55.47 ( 55.87)\tAcc@5  90.62 ( 85.47)\n",
            "Epoch: [10][120/391]\tTime  0.171 ( 0.230)\tLoss 1.6525e+00 (1.5895e+00)\tAcc@1  56.25 ( 55.46)\tAcc@5  79.69 ( 84.92)\n",
            "Epoch: [10][150/391]\tTime  0.171 ( 0.232)\tLoss 1.8573e+00 (1.5925e+00)\tAcc@1  48.44 ( 55.53)\tAcc@5  77.34 ( 84.84)\n",
            "Epoch: [10][180/391]\tTime  0.270 ( 0.231)\tLoss 1.5370e+00 (1.5881e+00)\tAcc@1  53.91 ( 55.63)\tAcc@5  85.94 ( 84.98)\n",
            "Epoch: [10][210/391]\tTime  0.277 ( 0.231)\tLoss 1.8625e+00 (1.5990e+00)\tAcc@1  51.56 ( 55.34)\tAcc@5  79.69 ( 84.80)\n",
            "Epoch: [10][240/391]\tTime  0.222 ( 0.231)\tLoss 1.6125e+00 (1.6024e+00)\tAcc@1  57.03 ( 55.32)\tAcc@5  83.59 ( 84.74)\n",
            "Epoch: [10][270/391]\tTime  0.182 ( 0.231)\tLoss 1.5114e+00 (1.6024e+00)\tAcc@1  55.47 ( 55.30)\tAcc@5  84.38 ( 84.68)\n",
            "Epoch: [10][300/391]\tTime  0.278 ( 0.231)\tLoss 1.5508e+00 (1.6031e+00)\tAcc@1  58.59 ( 55.33)\tAcc@5  86.72 ( 84.63)\n",
            "Epoch: [10][330/391]\tTime  0.287 ( 0.230)\tLoss 1.6940e+00 (1.6035e+00)\tAcc@1  50.00 ( 55.20)\tAcc@5  85.16 ( 84.66)\n",
            "Epoch: [10][360/391]\tTime  0.172 ( 0.229)\tLoss 1.4882e+00 (1.6046e+00)\tAcc@1  59.38 ( 55.22)\tAcc@5  85.94 ( 84.63)\n",
            "Epoch: [10][390/391]\tTime  0.151 ( 0.228)\tLoss 1.3452e+00 (1.6056e+00)\tAcc@1  62.50 ( 55.17)\tAcc@5  92.50 ( 84.61)\n",
            "==> Train Accuracy: Acc@1 55.168 || Acc@5 84.606\n",
            "==> Test Accuracy:  Acc@1 46.890 || Acc@5 76.860\n",
            "==> 93.52 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 11, lr: 0.1 -----\n",
            "Epoch: [11][  0/391]\tTime  0.572 ( 0.572)\tLoss 1.4974e+00 (1.4974e+00)\tAcc@1  56.25 ( 56.25)\tAcc@5  84.38 ( 84.38)\n",
            "Epoch: [11][ 30/391]\tTime  0.201 ( 0.223)\tLoss 1.3311e+00 (1.4899e+00)\tAcc@1  63.28 ( 58.14)\tAcc@5  87.50 ( 86.16)\n",
            "Epoch: [11][ 60/391]\tTime  0.257 ( 0.220)\tLoss 1.4719e+00 (1.4740e+00)\tAcc@1  62.50 ( 58.06)\tAcc@5  87.50 ( 86.54)\n",
            "Epoch: [11][ 90/391]\tTime  0.257 ( 0.220)\tLoss 1.5021e+00 (1.4888e+00)\tAcc@1  59.38 ( 57.87)\tAcc@5  82.81 ( 86.31)\n",
            "Epoch: [11][120/391]\tTime  0.174 ( 0.220)\tLoss 1.7219e+00 (1.5031e+00)\tAcc@1  57.03 ( 57.39)\tAcc@5  82.81 ( 86.25)\n",
            "Epoch: [11][150/391]\tTime  0.173 ( 0.222)\tLoss 1.5425e+00 (1.5152e+00)\tAcc@1  53.12 ( 57.02)\tAcc@5  85.94 ( 86.12)\n",
            "Epoch: [11][180/391]\tTime  0.180 ( 0.221)\tLoss 1.8391e+00 (1.5277e+00)\tAcc@1  50.00 ( 56.74)\tAcc@5  78.91 ( 85.99)\n",
            "Epoch: [11][210/391]\tTime  0.223 ( 0.222)\tLoss 1.7543e+00 (1.5273e+00)\tAcc@1  53.12 ( 56.81)\tAcc@5  85.16 ( 85.97)\n",
            "Epoch: [11][240/391]\tTime  0.245 ( 0.223)\tLoss 1.8101e+00 (1.5335e+00)\tAcc@1  50.78 ( 56.76)\tAcc@5  81.25 ( 85.79)\n",
            "Epoch: [11][270/391]\tTime  0.236 ( 0.222)\tLoss 1.6764e+00 (1.5328e+00)\tAcc@1  51.56 ( 56.86)\tAcc@5  79.69 ( 85.76)\n",
            "Epoch: [11][300/391]\tTime  0.258 ( 0.223)\tLoss 1.5545e+00 (1.5325e+00)\tAcc@1  60.16 ( 56.90)\tAcc@5  82.03 ( 85.76)\n",
            "Epoch: [11][330/391]\tTime  0.313 ( 0.223)\tLoss 1.7936e+00 (1.5371e+00)\tAcc@1  50.78 ( 56.84)\tAcc@5  85.94 ( 85.78)\n",
            "Epoch: [11][360/391]\tTime  0.288 ( 0.223)\tLoss 1.5735e+00 (1.5400e+00)\tAcc@1  57.81 ( 56.81)\tAcc@5  83.59 ( 85.74)\n",
            "Epoch: [11][390/391]\tTime  0.152 ( 0.222)\tLoss 1.7012e+00 (1.5424e+00)\tAcc@1  52.50 ( 56.68)\tAcc@5  86.25 ( 85.76)\n",
            "==> Train Accuracy: Acc@1 56.682 || Acc@5 85.756\n",
            "==> Test Accuracy:  Acc@1 45.540 || Acc@5 77.210\n",
            "==> 91.15 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 12, lr: 0.1 -----\n",
            "Epoch: [12][  0/391]\tTime  0.569 ( 0.569)\tLoss 1.3943e+00 (1.3943e+00)\tAcc@1  57.03 ( 57.03)\tAcc@5  86.72 ( 86.72)\n",
            "Epoch: [12][ 30/391]\tTime  0.177 ( 0.234)\tLoss 1.3888e+00 (1.4543e+00)\tAcc@1  61.72 ( 59.05)\tAcc@5  83.59 ( 87.32)\n",
            "Epoch: [12][ 60/391]\tTime  0.205 ( 0.228)\tLoss 1.5825e+00 (1.4511e+00)\tAcc@1  52.34 ( 59.08)\tAcc@5  85.94 ( 87.27)\n",
            "Epoch: [12][ 90/391]\tTime  0.303 ( 0.230)\tLoss 1.3531e+00 (1.4367e+00)\tAcc@1  62.50 ( 59.47)\tAcc@5  89.84 ( 87.34)\n",
            "Epoch: [12][120/391]\tTime  0.179 ( 0.227)\tLoss 1.4024e+00 (1.4516e+00)\tAcc@1  64.84 ( 59.18)\tAcc@5  85.94 ( 87.07)\n",
            "Epoch: [12][150/391]\tTime  0.157 ( 0.227)\tLoss 1.4598e+00 (1.4571e+00)\tAcc@1  61.72 ( 59.03)\tAcc@5  86.72 ( 87.02)\n",
            "Epoch: [12][180/391]\tTime  0.164 ( 0.226)\tLoss 1.3882e+00 (1.4588e+00)\tAcc@1  58.59 ( 58.90)\tAcc@5  87.50 ( 87.04)\n",
            "Epoch: [12][210/391]\tTime  0.262 ( 0.225)\tLoss 1.4602e+00 (1.4676e+00)\tAcc@1  60.16 ( 58.77)\tAcc@5  86.72 ( 86.95)\n",
            "Epoch: [12][240/391]\tTime  0.230 ( 0.225)\tLoss 1.3615e+00 (1.4701e+00)\tAcc@1  64.84 ( 58.77)\tAcc@5  87.50 ( 86.95)\n",
            "Epoch: [12][270/391]\tTime  0.236 ( 0.225)\tLoss 1.4839e+00 (1.4696e+00)\tAcc@1  60.16 ( 58.76)\tAcc@5  86.72 ( 86.95)\n",
            "Epoch: [12][300/391]\tTime  0.292 ( 0.225)\tLoss 1.4515e+00 (1.4748e+00)\tAcc@1  64.06 ( 58.55)\tAcc@5  85.94 ( 86.90)\n",
            "Epoch: [12][330/391]\tTime  0.259 ( 0.226)\tLoss 1.4860e+00 (1.4762e+00)\tAcc@1  56.25 ( 58.43)\tAcc@5  88.28 ( 86.92)\n",
            "Epoch: [12][360/391]\tTime  0.219 ( 0.226)\tLoss 1.3569e+00 (1.4796e+00)\tAcc@1  60.16 ( 58.35)\tAcc@5  89.06 ( 86.85)\n",
            "Epoch: [12][390/391]\tTime  0.145 ( 0.225)\tLoss 1.4953e+00 (1.4839e+00)\tAcc@1  56.25 ( 58.20)\tAcc@5  87.50 ( 86.79)\n",
            "==> Train Accuracy: Acc@1 58.198 || Acc@5 86.790\n",
            "==> Test Accuracy:  Acc@1 53.000 || Acc@5 82.380\n",
            "==> 92.15 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 13, lr: 0.1 -----\n",
            "Epoch: [13][  0/391]\tTime  0.559 ( 0.559)\tLoss 1.3653e+00 (1.3653e+00)\tAcc@1  56.25 ( 56.25)\tAcc@5  89.06 ( 89.06)\n",
            "Epoch: [13][ 30/391]\tTime  0.261 ( 0.228)\tLoss 1.5189e+00 (1.4197e+00)\tAcc@1  53.91 ( 59.45)\tAcc@5  90.62 ( 87.78)\n",
            "Epoch: [13][ 60/391]\tTime  0.173 ( 0.227)\tLoss 1.3370e+00 (1.3908e+00)\tAcc@1  58.59 ( 60.46)\tAcc@5  92.19 ( 88.46)\n",
            "Epoch: [13][ 90/391]\tTime  0.162 ( 0.225)\tLoss 1.6080e+00 (1.3910e+00)\tAcc@1  55.47 ( 60.18)\tAcc@5  83.59 ( 88.49)\n",
            "Epoch: [13][120/391]\tTime  0.342 ( 0.224)\tLoss 1.2735e+00 (1.4013e+00)\tAcc@1  59.38 ( 59.89)\tAcc@5  89.84 ( 88.20)\n",
            "Epoch: [13][150/391]\tTime  0.287 ( 0.224)\tLoss 1.5518e+00 (1.4016e+00)\tAcc@1  54.69 ( 59.81)\tAcc@5  84.38 ( 88.18)\n",
            "Epoch: [13][180/391]\tTime  0.317 ( 0.225)\tLoss 1.5366e+00 (1.4130e+00)\tAcc@1  54.69 ( 59.61)\tAcc@5  89.84 ( 87.92)\n",
            "Epoch: [13][210/391]\tTime  0.195 ( 0.224)\tLoss 1.1306e+00 (1.4203e+00)\tAcc@1  65.62 ( 59.49)\tAcc@5  92.97 ( 87.79)\n",
            "Epoch: [13][240/391]\tTime  0.168 ( 0.224)\tLoss 1.6159e+00 (1.4244e+00)\tAcc@1  59.38 ( 59.33)\tAcc@5  81.25 ( 87.67)\n",
            "Epoch: [13][270/391]\tTime  0.227 ( 0.224)\tLoss 1.4908e+00 (1.4288e+00)\tAcc@1  58.59 ( 59.18)\tAcc@5  85.94 ( 87.61)\n",
            "Epoch: [13][300/391]\tTime  0.165 ( 0.224)\tLoss 1.5186e+00 (1.4330e+00)\tAcc@1  59.38 ( 59.06)\tAcc@5  83.59 ( 87.58)\n",
            "Epoch: [13][330/391]\tTime  0.291 ( 0.224)\tLoss 1.3684e+00 (1.4332e+00)\tAcc@1  62.50 ( 59.14)\tAcc@5  89.06 ( 87.54)\n",
            "Epoch: [13][360/391]\tTime  0.173 ( 0.223)\tLoss 1.4146e+00 (1.4341e+00)\tAcc@1  57.81 ( 59.14)\tAcc@5  88.28 ( 87.52)\n",
            "Epoch: [13][390/391]\tTime  0.151 ( 0.223)\tLoss 1.5719e+00 (1.4357e+00)\tAcc@1  53.75 ( 59.14)\tAcc@5  86.25 ( 87.48)\n",
            "==> Train Accuracy: Acc@1 59.138 || Acc@5 87.480\n",
            "==> Test Accuracy:  Acc@1 51.260 || Acc@5 81.550\n",
            "==> 91.53 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 14, lr: 0.1 -----\n",
            "Epoch: [14][  0/391]\tTime  0.585 ( 0.585)\tLoss 1.4739e+00 (1.4739e+00)\tAcc@1  56.25 ( 56.25)\tAcc@5  88.28 ( 88.28)\n",
            "Epoch: [14][ 30/391]\tTime  0.170 ( 0.231)\tLoss 1.2105e+00 (1.3345e+00)\tAcc@1  62.50 ( 61.59)\tAcc@5  92.97 ( 89.34)\n",
            "Epoch: [14][ 60/391]\tTime  0.168 ( 0.228)\tLoss 1.5446e+00 (1.3463e+00)\tAcc@1  59.38 ( 61.40)\tAcc@5  86.72 ( 89.31)\n",
            "Epoch: [14][ 90/391]\tTime  0.215 ( 0.225)\tLoss 1.1686e+00 (1.3407e+00)\tAcc@1  65.62 ( 61.78)\tAcc@5  92.19 ( 89.17)\n",
            "Epoch: [14][120/391]\tTime  0.303 ( 0.226)\tLoss 1.2697e+00 (1.3528e+00)\tAcc@1  63.28 ( 61.44)\tAcc@5  89.84 ( 88.91)\n",
            "Epoch: [14][150/391]\tTime  0.229 ( 0.225)\tLoss 1.2923e+00 (1.3581e+00)\tAcc@1  62.50 ( 61.23)\tAcc@5  95.31 ( 88.81)\n",
            "Epoch: [14][180/391]\tTime  0.261 ( 0.226)\tLoss 1.4415e+00 (1.3698e+00)\tAcc@1  58.59 ( 60.95)\tAcc@5  85.94 ( 88.59)\n",
            "Epoch: [14][210/391]\tTime  0.311 ( 0.224)\tLoss 1.2369e+00 (1.3801e+00)\tAcc@1  67.19 ( 60.80)\tAcc@5  88.28 ( 88.40)\n",
            "Epoch: [14][240/391]\tTime  0.261 ( 0.224)\tLoss 1.4174e+00 (1.3814e+00)\tAcc@1  57.03 ( 60.76)\tAcc@5  86.72 ( 88.33)\n",
            "Epoch: [14][270/391]\tTime  0.183 ( 0.224)\tLoss 1.4058e+00 (1.3880e+00)\tAcc@1  59.38 ( 60.55)\tAcc@5  89.06 ( 88.18)\n",
            "Epoch: [14][300/391]\tTime  0.167 ( 0.224)\tLoss 1.4658e+00 (1.3943e+00)\tAcc@1  57.81 ( 60.42)\tAcc@5  85.16 ( 88.01)\n",
            "Epoch: [14][330/391]\tTime  0.256 ( 0.223)\tLoss 1.4854e+00 (1.3929e+00)\tAcc@1  59.38 ( 60.40)\tAcc@5  85.94 ( 88.01)\n",
            "Epoch: [14][360/391]\tTime  0.301 ( 0.224)\tLoss 1.2425e+00 (1.3932e+00)\tAcc@1  67.97 ( 60.38)\tAcc@5  91.41 ( 88.01)\n",
            "Epoch: [14][390/391]\tTime  0.151 ( 0.223)\tLoss 1.6684e+00 (1.3946e+00)\tAcc@1  60.00 ( 60.33)\tAcc@5  80.00 ( 87.98)\n",
            "==> Train Accuracy: Acc@1 60.334 || Acc@5 87.982\n",
            "==> Test Accuracy:  Acc@1 52.750 || Acc@5 81.870\n",
            "==> 91.49 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 15, lr: 0.1 -----\n",
            "Epoch: [15][  0/391]\tTime  0.553 ( 0.553)\tLoss 1.4576e+00 (1.4576e+00)\tAcc@1  57.81 ( 57.81)\tAcc@5  87.50 ( 87.50)\n",
            "Epoch: [15][ 30/391]\tTime  0.166 ( 0.224)\tLoss 1.2454e+00 (1.3146e+00)\tAcc@1  61.72 ( 62.00)\tAcc@5  89.06 ( 89.69)\n",
            "Epoch: [15][ 60/391]\tTime  0.178 ( 0.220)\tLoss 1.1671e+00 (1.3167e+00)\tAcc@1  67.97 ( 61.92)\tAcc@5  92.97 ( 89.61)\n",
            "Epoch: [15][ 90/391]\tTime  0.162 ( 0.219)\tLoss 1.2692e+00 (1.3190e+00)\tAcc@1  60.16 ( 62.06)\tAcc@5  91.41 ( 89.40)\n",
            "Epoch: [15][120/391]\tTime  0.264 ( 0.221)\tLoss 1.3722e+00 (1.3244e+00)\tAcc@1  60.16 ( 61.94)\tAcc@5  89.06 ( 89.28)\n",
            "Epoch: [15][150/391]\tTime  0.235 ( 0.220)\tLoss 1.1330e+00 (1.3271e+00)\tAcc@1  64.84 ( 61.83)\tAcc@5  89.06 ( 89.24)\n",
            "Epoch: [15][180/391]\tTime  0.162 ( 0.220)\tLoss 1.4735e+00 (1.3281e+00)\tAcc@1  53.12 ( 61.85)\tAcc@5  84.38 ( 89.22)\n",
            "Epoch: [15][210/391]\tTime  0.177 ( 0.221)\tLoss 1.2793e+00 (1.3418e+00)\tAcc@1  62.50 ( 61.54)\tAcc@5  89.06 ( 89.11)\n",
            "Epoch: [15][240/391]\tTime  0.163 ( 0.222)\tLoss 1.4306e+00 (1.3444e+00)\tAcc@1  58.59 ( 61.53)\tAcc@5  88.28 ( 89.00)\n",
            "Epoch: [15][270/391]\tTime  0.175 ( 0.222)\tLoss 1.2775e+00 (1.3442e+00)\tAcc@1  55.47 ( 61.49)\tAcc@5  92.19 ( 88.99)\n",
            "Epoch: [15][300/391]\tTime  0.165 ( 0.222)\tLoss 1.3962e+00 (1.3528e+00)\tAcc@1  59.38 ( 61.32)\tAcc@5  88.28 ( 88.85)\n",
            "Epoch: [15][330/391]\tTime  0.158 ( 0.222)\tLoss 1.3698e+00 (1.3526e+00)\tAcc@1  59.38 ( 61.24)\tAcc@5  86.72 ( 88.85)\n",
            "Epoch: [15][360/391]\tTime  0.175 ( 0.222)\tLoss 1.2593e+00 (1.3511e+00)\tAcc@1  64.84 ( 61.26)\tAcc@5  92.97 ( 88.82)\n",
            "Epoch: [15][390/391]\tTime  0.152 ( 0.222)\tLoss 1.3826e+00 (1.3542e+00)\tAcc@1  61.25 ( 61.20)\tAcc@5  87.50 ( 88.75)\n",
            "==> Train Accuracy: Acc@1 61.202 || Acc@5 88.750\n",
            "==> Test Accuracy:  Acc@1 55.610 || Acc@5 83.970\n",
            "==> 90.80 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 16, lr: 0.1 -----\n",
            "Epoch: [16][  0/391]\tTime  0.560 ( 0.560)\tLoss 1.4588e+00 (1.4588e+00)\tAcc@1  58.59 ( 58.59)\tAcc@5  85.94 ( 85.94)\n",
            "Epoch: [16][ 30/391]\tTime  0.170 ( 0.232)\tLoss 1.2342e+00 (1.2538e+00)\tAcc@1  67.19 ( 64.31)\tAcc@5  89.84 ( 89.89)\n",
            "Epoch: [16][ 60/391]\tTime  0.179 ( 0.230)\tLoss 1.0766e+00 (1.2549e+00)\tAcc@1  69.53 ( 64.11)\tAcc@5  91.41 ( 90.13)\n",
            "Epoch: [16][ 90/391]\tTime  0.189 ( 0.225)\tLoss 1.4767e+00 (1.2688e+00)\tAcc@1  56.25 ( 63.78)\tAcc@5  85.94 ( 89.87)\n",
            "Epoch: [16][120/391]\tTime  0.235 ( 0.225)\tLoss 1.3796e+00 (1.2779e+00)\tAcc@1  66.41 ( 63.62)\tAcc@5  86.72 ( 89.58)\n",
            "Epoch: [16][150/391]\tTime  0.180 ( 0.223)\tLoss 1.4627e+00 (1.2851e+00)\tAcc@1  62.50 ( 63.32)\tAcc@5  89.06 ( 89.52)\n",
            "Epoch: [16][180/391]\tTime  0.167 ( 0.222)\tLoss 1.5197e+00 (1.2892e+00)\tAcc@1  54.69 ( 63.12)\tAcc@5  84.38 ( 89.40)\n",
            "Epoch: [16][210/391]\tTime  0.311 ( 0.222)\tLoss 1.4117e+00 (1.2977e+00)\tAcc@1  60.16 ( 62.86)\tAcc@5  91.41 ( 89.36)\n",
            "Epoch: [16][240/391]\tTime  0.238 ( 0.222)\tLoss 1.5026e+00 (1.2999e+00)\tAcc@1  60.16 ( 62.78)\tAcc@5  84.38 ( 89.33)\n",
            "Epoch: [16][270/391]\tTime  0.298 ( 0.223)\tLoss 1.0538e+00 (1.3080e+00)\tAcc@1  70.31 ( 62.67)\tAcc@5  92.97 ( 89.14)\n",
            "Epoch: [16][300/391]\tTime  0.284 ( 0.223)\tLoss 1.3016e+00 (1.3154e+00)\tAcc@1  61.72 ( 62.40)\tAcc@5  86.72 ( 89.05)\n",
            "Epoch: [16][330/391]\tTime  0.168 ( 0.223)\tLoss 1.0815e+00 (1.3191e+00)\tAcc@1  69.53 ( 62.25)\tAcc@5  93.75 ( 88.99)\n",
            "Epoch: [16][360/391]\tTime  0.165 ( 0.223)\tLoss 1.2405e+00 (1.3194e+00)\tAcc@1  62.50 ( 62.14)\tAcc@5  92.19 ( 89.09)\n",
            "Epoch: [16][390/391]\tTime  0.150 ( 0.223)\tLoss 1.4533e+00 (1.3255e+00)\tAcc@1  53.75 ( 61.98)\tAcc@5  82.50 ( 88.99)\n",
            "==> Train Accuracy: Acc@1 61.980 || Acc@5 88.986\n",
            "==> Test Accuracy:  Acc@1 54.320 || Acc@5 83.680\n",
            "==> 91.22 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 17, lr: 0.1 -----\n",
            "Epoch: [17][  0/391]\tTime  0.553 ( 0.553)\tLoss 1.2531e+00 (1.2531e+00)\tAcc@1  62.50 ( 62.50)\tAcc@5  89.84 ( 89.84)\n",
            "Epoch: [17][ 30/391]\tTime  0.172 ( 0.227)\tLoss 1.2190e+00 (1.2472e+00)\tAcc@1  60.16 ( 63.28)\tAcc@5  92.19 ( 90.80)\n",
            "Epoch: [17][ 60/391]\tTime  0.169 ( 0.223)\tLoss 1.2945e+00 (1.2468e+00)\tAcc@1  62.50 ( 63.83)\tAcc@5  89.06 ( 90.54)\n",
            "Epoch: [17][ 90/391]\tTime  0.177 ( 0.223)\tLoss 1.1948e+00 (1.2478e+00)\tAcc@1  67.97 ( 63.80)\tAcc@5  88.28 ( 90.53)\n",
            "Epoch: [17][120/391]\tTime  0.169 ( 0.224)\tLoss 1.1149e+00 (1.2516e+00)\tAcc@1  69.53 ( 63.71)\tAcc@5  91.41 ( 90.31)\n",
            "Epoch: [17][150/391]\tTime  0.173 ( 0.224)\tLoss 1.2035e+00 (1.2604e+00)\tAcc@1  66.41 ( 63.50)\tAcc@5  89.84 ( 90.17)\n",
            "Epoch: [17][180/391]\tTime  0.169 ( 0.224)\tLoss 1.0989e+00 (1.2684e+00)\tAcc@1  67.19 ( 63.29)\tAcc@5  93.75 ( 90.03)\n",
            "Epoch: [17][210/391]\tTime  0.183 ( 0.223)\tLoss 1.1885e+00 (1.2765e+00)\tAcc@1  64.84 ( 62.95)\tAcc@5  91.41 ( 89.95)\n",
            "Epoch: [17][240/391]\tTime  0.167 ( 0.224)\tLoss 1.5281e+00 (1.2805e+00)\tAcc@1  57.81 ( 62.94)\tAcc@5  84.38 ( 89.87)\n",
            "Epoch: [17][270/391]\tTime  0.324 ( 0.223)\tLoss 1.0092e+00 (1.2780e+00)\tAcc@1  72.66 ( 63.01)\tAcc@5  95.31 ( 89.92)\n",
            "Epoch: [17][300/391]\tTime  0.275 ( 0.224)\tLoss 1.2438e+00 (1.2848e+00)\tAcc@1  64.06 ( 62.95)\tAcc@5  86.72 ( 89.77)\n",
            "Epoch: [17][330/391]\tTime  0.331 ( 0.223)\tLoss 1.7434e+00 (1.2936e+00)\tAcc@1  49.22 ( 62.67)\tAcc@5  81.25 ( 89.63)\n",
            "Epoch: [17][360/391]\tTime  0.236 ( 0.224)\tLoss 1.2150e+00 (1.2968e+00)\tAcc@1  60.94 ( 62.56)\tAcc@5  93.75 ( 89.59)\n",
            "Epoch: [17][390/391]\tTime  0.150 ( 0.222)\tLoss 1.4092e+00 (1.2969e+00)\tAcc@1  60.00 ( 62.55)\tAcc@5  87.50 ( 89.58)\n",
            "==> Train Accuracy: Acc@1 62.546 || Acc@5 89.584\n",
            "==> Test Accuracy:  Acc@1 52.670 || Acc@5 82.110\n",
            "==> 91.14 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 18, lr: 0.1 -----\n",
            "Epoch: [18][  0/391]\tTime  0.552 ( 0.552)\tLoss 1.2155e+00 (1.2155e+00)\tAcc@1  64.06 ( 64.06)\tAcc@5  89.84 ( 89.84)\n",
            "Epoch: [18][ 30/391]\tTime  0.185 ( 0.224)\tLoss 1.1025e+00 (1.2410e+00)\tAcc@1  70.31 ( 64.26)\tAcc@5  92.19 ( 91.05)\n",
            "Epoch: [18][ 60/391]\tTime  0.161 ( 0.223)\tLoss 9.8644e-01 (1.2381e+00)\tAcc@1  76.56 ( 64.23)\tAcc@5  89.84 ( 90.89)\n",
            "Epoch: [18][ 90/391]\tTime  0.173 ( 0.222)\tLoss 1.1785e+00 (1.2292e+00)\tAcc@1  67.97 ( 64.56)\tAcc@5  89.06 ( 90.85)\n",
            "Epoch: [18][120/391]\tTime  0.314 ( 0.222)\tLoss 1.2024e+00 (1.2369e+00)\tAcc@1  65.62 ( 64.22)\tAcc@5  89.84 ( 90.71)\n",
            "Epoch: [18][150/391]\tTime  0.186 ( 0.221)\tLoss 1.6251e+00 (1.2380e+00)\tAcc@1  56.25 ( 64.28)\tAcc@5  89.06 ( 90.70)\n",
            "Epoch: [18][180/391]\tTime  0.171 ( 0.221)\tLoss 1.3763e+00 (1.2498e+00)\tAcc@1  60.94 ( 63.90)\tAcc@5  86.72 ( 90.53)\n",
            "Epoch: [18][210/391]\tTime  0.186 ( 0.222)\tLoss 1.4462e+00 (1.2568e+00)\tAcc@1  62.50 ( 63.70)\tAcc@5  83.59 ( 90.50)\n",
            "Epoch: [18][240/391]\tTime  0.164 ( 0.222)\tLoss 1.2806e+00 (1.2623e+00)\tAcc@1  64.06 ( 63.56)\tAcc@5  85.94 ( 90.36)\n",
            "Epoch: [18][270/391]\tTime  0.303 ( 0.222)\tLoss 1.1331e+00 (1.2644e+00)\tAcc@1  65.62 ( 63.49)\tAcc@5  92.97 ( 90.33)\n",
            "Epoch: [18][300/391]\tTime  0.278 ( 0.222)\tLoss 1.3025e+00 (1.2688e+00)\tAcc@1  60.16 ( 63.33)\tAcc@5  90.62 ( 90.22)\n",
            "Epoch: [18][330/391]\tTime  0.227 ( 0.222)\tLoss 1.2292e+00 (1.2720e+00)\tAcc@1  60.16 ( 63.28)\tAcc@5  92.19 ( 90.15)\n",
            "Epoch: [18][360/391]\tTime  0.174 ( 0.222)\tLoss 1.2796e+00 (1.2738e+00)\tAcc@1  61.72 ( 63.18)\tAcc@5  89.06 ( 90.14)\n",
            "Epoch: [18][390/391]\tTime  0.152 ( 0.222)\tLoss 1.4019e+00 (1.2749e+00)\tAcc@1  57.50 ( 63.15)\tAcc@5  90.00 ( 90.11)\n",
            "==> Train Accuracy: Acc@1 63.154 || Acc@5 90.110\n",
            "==> Test Accuracy:  Acc@1 57.990 || Acc@5 85.470\n",
            "==> 90.83 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 19, lr: 0.1 -----\n",
            "Epoch: [19][  0/391]\tTime  0.560 ( 0.560)\tLoss 1.0499e+00 (1.0499e+00)\tAcc@1  65.62 ( 65.62)\tAcc@5  92.97 ( 92.97)\n",
            "Epoch: [19][ 30/391]\tTime  0.283 ( 0.242)\tLoss 1.2049e+00 (1.1577e+00)\tAcc@1  64.06 ( 66.83)\tAcc@5  91.41 ( 91.13)\n",
            "Epoch: [19][ 60/391]\tTime  0.341 ( 0.229)\tLoss 1.2783e+00 (1.1803e+00)\tAcc@1  61.72 ( 65.80)\tAcc@5  89.84 ( 91.01)\n",
            "Epoch: [19][ 90/391]\tTime  0.170 ( 0.224)\tLoss 1.2057e+00 (1.1795e+00)\tAcc@1  66.41 ( 66.03)\tAcc@5  90.62 ( 91.03)\n",
            "Epoch: [19][120/391]\tTime  0.168 ( 0.226)\tLoss 1.2977e+00 (1.1996e+00)\tAcc@1  59.38 ( 65.55)\tAcc@5  89.06 ( 90.75)\n",
            "Epoch: [19][150/391]\tTime  0.187 ( 0.226)\tLoss 1.2673e+00 (1.2087e+00)\tAcc@1  64.06 ( 65.21)\tAcc@5  86.72 ( 90.71)\n",
            "Epoch: [19][180/391]\tTime  0.238 ( 0.225)\tLoss 1.0049e+00 (1.2203e+00)\tAcc@1  69.53 ( 64.84)\tAcc@5  93.75 ( 90.58)\n",
            "Epoch: [19][210/391]\tTime  0.235 ( 0.223)\tLoss 1.0024e+00 (1.2229e+00)\tAcc@1  72.66 ( 64.75)\tAcc@5  94.53 ( 90.56)\n",
            "Epoch: [19][240/391]\tTime  0.172 ( 0.222)\tLoss 1.2577e+00 (1.2271e+00)\tAcc@1  58.59 ( 64.59)\tAcc@5  89.84 ( 90.53)\n",
            "Epoch: [19][270/391]\tTime  0.278 ( 0.223)\tLoss 1.4184e+00 (1.2304e+00)\tAcc@1  61.72 ( 64.52)\tAcc@5  88.28 ( 90.53)\n",
            "Epoch: [19][300/391]\tTime  0.196 ( 0.223)\tLoss 1.4355e+00 (1.2349e+00)\tAcc@1  53.12 ( 64.31)\tAcc@5  83.59 ( 90.48)\n",
            "Epoch: [19][330/391]\tTime  0.320 ( 0.223)\tLoss 1.3204e+00 (1.2404e+00)\tAcc@1  60.16 ( 64.24)\tAcc@5  92.19 ( 90.35)\n",
            "Epoch: [19][360/391]\tTime  0.300 ( 0.223)\tLoss 1.1850e+00 (1.2467e+00)\tAcc@1  65.62 ( 64.07)\tAcc@5  93.75 ( 90.27)\n",
            "Epoch: [19][390/391]\tTime  0.150 ( 0.223)\tLoss 1.6182e+00 (1.2476e+00)\tAcc@1  57.50 ( 64.07)\tAcc@5  91.25 ( 90.31)\n",
            "==> Train Accuracy: Acc@1 64.072 || Acc@5 90.306\n",
            "==> Test Accuracy:  Acc@1 53.490 || Acc@5 82.680\n",
            "==> 91.22 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 20, lr: 0.1 -----\n",
            "Epoch: [20][  0/391]\tTime  0.586 ( 0.586)\tLoss 1.0053e+00 (1.0053e+00)\tAcc@1  69.53 ( 69.53)\tAcc@5  94.53 ( 94.53)\n",
            "Epoch: [20][ 30/391]\tTime  0.283 ( 0.232)\tLoss 1.1671e+00 (1.1383e+00)\tAcc@1  64.84 ( 66.66)\tAcc@5  91.41 ( 92.09)\n",
            "Epoch: [20][ 60/391]\tTime  0.317 ( 0.228)\tLoss 1.5100e+00 (1.1437e+00)\tAcc@1  54.69 ( 66.97)\tAcc@5  85.16 ( 91.75)\n",
            "Epoch: [20][ 90/391]\tTime  0.274 ( 0.226)\tLoss 1.3527e+00 (1.1652e+00)\tAcc@1  61.72 ( 66.37)\tAcc@5  88.28 ( 91.20)\n",
            "Epoch: [20][120/391]\tTime  0.213 ( 0.225)\tLoss 1.3163e+00 (1.1717e+00)\tAcc@1  61.72 ( 66.03)\tAcc@5  90.62 ( 91.14)\n",
            "Epoch: [20][150/391]\tTime  0.297 ( 0.226)\tLoss 1.0276e+00 (1.1746e+00)\tAcc@1  68.75 ( 65.90)\tAcc@5  95.31 ( 91.06)\n",
            "Epoch: [20][180/391]\tTime  0.261 ( 0.225)\tLoss 1.1536e+00 (1.1858e+00)\tAcc@1  63.28 ( 65.59)\tAcc@5  93.75 ( 90.84)\n",
            "Epoch: [20][210/391]\tTime  0.169 ( 0.224)\tLoss 1.3826e+00 (1.1974e+00)\tAcc@1  64.06 ( 65.36)\tAcc@5  88.28 ( 90.74)\n",
            "Epoch: [20][240/391]\tTime  0.231 ( 0.224)\tLoss 1.2741e+00 (1.2034e+00)\tAcc@1  61.72 ( 65.28)\tAcc@5  89.06 ( 90.69)\n",
            "Epoch: [20][270/391]\tTime  0.255 ( 0.224)\tLoss 1.3227e+00 (1.2092e+00)\tAcc@1  64.84 ( 64.99)\tAcc@5  85.94 ( 90.62)\n",
            "Epoch: [20][300/391]\tTime  0.174 ( 0.224)\tLoss 1.2211e+00 (1.2128e+00)\tAcc@1  68.75 ( 64.96)\tAcc@5  89.06 ( 90.59)\n",
            "Epoch: [20][330/391]\tTime  0.233 ( 0.223)\tLoss 1.0506e+00 (1.2196e+00)\tAcc@1  67.97 ( 64.72)\tAcc@5  92.19 ( 90.50)\n",
            "Epoch: [20][360/391]\tTime  0.214 ( 0.223)\tLoss 1.3142e+00 (1.2240e+00)\tAcc@1  60.94 ( 64.62)\tAcc@5  89.84 ( 90.43)\n",
            "Epoch: [20][390/391]\tTime  0.151 ( 0.223)\tLoss 1.5377e+00 (1.2278e+00)\tAcc@1  60.00 ( 64.54)\tAcc@5  82.50 ( 90.41)\n",
            "==> Train Accuracy: Acc@1 64.544 || Acc@5 90.406\n",
            "==> Test Accuracy:  Acc@1 55.690 || Acc@5 83.800\n",
            "==> 91.31 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 21, lr: 0.1 -----\n",
            "Epoch: [21][  0/391]\tTime  0.546 ( 0.546)\tLoss 1.1856e+00 (1.1856e+00)\tAcc@1  67.19 ( 67.19)\tAcc@5  87.50 ( 87.50)\n",
            "Epoch: [21][ 30/391]\tTime  0.181 ( 0.233)\tLoss 1.1606e+00 (1.0894e+00)\tAcc@1  67.97 ( 68.17)\tAcc@5  92.19 ( 92.67)\n",
            "Epoch: [21][ 60/391]\tTime  0.236 ( 0.229)\tLoss 1.1187e+00 (1.1059e+00)\tAcc@1  66.41 ( 67.80)\tAcc@5  89.06 ( 92.32)\n",
            "Epoch: [21][ 90/391]\tTime  0.167 ( 0.228)\tLoss 1.2826e+00 (1.1162e+00)\tAcc@1  63.28 ( 67.57)\tAcc@5  89.84 ( 92.33)\n",
            "Epoch: [21][120/391]\tTime  0.238 ( 0.225)\tLoss 9.7179e-01 (1.1323e+00)\tAcc@1  67.97 ( 67.23)\tAcc@5  93.75 ( 91.97)\n",
            "Epoch: [21][150/391]\tTime  0.245 ( 0.225)\tLoss 1.2102e+00 (1.1420e+00)\tAcc@1  64.84 ( 66.85)\tAcc@5  90.62 ( 91.93)\n",
            "Epoch: [21][180/391]\tTime  0.345 ( 0.224)\tLoss 1.3127e+00 (1.1562e+00)\tAcc@1  58.59 ( 66.51)\tAcc@5  89.06 ( 91.72)\n",
            "Epoch: [21][210/391]\tTime  0.202 ( 0.224)\tLoss 1.2787e+00 (1.1686e+00)\tAcc@1  63.28 ( 66.17)\tAcc@5  90.62 ( 91.60)\n",
            "Epoch: [21][240/391]\tTime  0.172 ( 0.223)\tLoss 1.1494e+00 (1.1781e+00)\tAcc@1  64.06 ( 65.89)\tAcc@5  93.75 ( 91.45)\n",
            "Epoch: [21][270/391]\tTime  0.170 ( 0.222)\tLoss 9.7583e-01 (1.1850e+00)\tAcc@1  73.44 ( 65.69)\tAcc@5  94.53 ( 91.26)\n",
            "Epoch: [21][300/391]\tTime  0.309 ( 0.222)\tLoss 1.1453e+00 (1.1912e+00)\tAcc@1  61.72 ( 65.48)\tAcc@5  94.53 ( 91.17)\n",
            "Epoch: [21][330/391]\tTime  0.230 ( 0.222)\tLoss 1.2569e+00 (1.1967e+00)\tAcc@1  64.06 ( 65.32)\tAcc@5  89.84 ( 91.04)\n",
            "Epoch: [21][360/391]\tTime  0.190 ( 0.222)\tLoss 1.0737e+00 (1.2027e+00)\tAcc@1  68.75 ( 65.14)\tAcc@5  92.97 ( 90.93)\n",
            "Epoch: [21][390/391]\tTime  0.144 ( 0.221)\tLoss 1.4186e+00 (1.2085e+00)\tAcc@1  60.00 ( 65.00)\tAcc@5  86.25 ( 90.87)\n",
            "==> Train Accuracy: Acc@1 64.996 || Acc@5 90.872\n",
            "==> Test Accuracy:  Acc@1 57.280 || Acc@5 85.080\n",
            "==> 90.78 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 22, lr: 0.1 -----\n",
            "Epoch: [22][  0/391]\tTime  0.569 ( 0.569)\tLoss 1.0682e+00 (1.0682e+00)\tAcc@1  69.53 ( 69.53)\tAcc@5  92.19 ( 92.19)\n",
            "Epoch: [22][ 30/391]\tTime  0.321 ( 0.235)\tLoss 1.2017e+00 (1.1067e+00)\tAcc@1  67.19 ( 67.41)\tAcc@5  90.62 ( 92.34)\n",
            "Epoch: [22][ 60/391]\tTime  0.325 ( 0.230)\tLoss 1.0390e+00 (1.1026e+00)\tAcc@1  71.09 ( 67.90)\tAcc@5  92.97 ( 92.25)\n",
            "Epoch: [22][ 90/391]\tTime  0.243 ( 0.226)\tLoss 1.1453e+00 (1.1123e+00)\tAcc@1  67.19 ( 67.63)\tAcc@5  92.97 ( 92.18)\n",
            "Epoch: [22][120/391]\tTime  0.173 ( 0.225)\tLoss 1.1404e+00 (1.1290e+00)\tAcc@1  66.41 ( 67.38)\tAcc@5  90.62 ( 92.06)\n",
            "Epoch: [22][150/391]\tTime  0.176 ( 0.225)\tLoss 1.1054e+00 (1.1368e+00)\tAcc@1  65.62 ( 67.15)\tAcc@5  92.97 ( 91.81)\n",
            "Epoch: [22][180/391]\tTime  0.176 ( 0.223)\tLoss 1.5446e+00 (1.1493e+00)\tAcc@1  57.81 ( 66.76)\tAcc@5  85.94 ( 91.63)\n",
            "Epoch: [22][210/391]\tTime  0.175 ( 0.224)\tLoss 1.0887e+00 (1.1553e+00)\tAcc@1  69.53 ( 66.57)\tAcc@5  94.53 ( 91.58)\n",
            "Epoch: [22][240/391]\tTime  0.172 ( 0.223)\tLoss 1.2047e+00 (1.1627e+00)\tAcc@1  62.50 ( 66.31)\tAcc@5  92.97 ( 91.43)\n",
            "Epoch: [22][270/391]\tTime  0.168 ( 0.222)\tLoss 1.0586e+00 (1.1702e+00)\tAcc@1  70.31 ( 66.12)\tAcc@5  92.97 ( 91.33)\n",
            "Epoch: [22][300/391]\tTime  0.187 ( 0.222)\tLoss 1.1841e+00 (1.1769e+00)\tAcc@1  64.06 ( 65.95)\tAcc@5  90.62 ( 91.26)\n",
            "Epoch: [22][330/391]\tTime  0.305 ( 0.222)\tLoss 9.7454e-01 (1.1856e+00)\tAcc@1  69.53 ( 65.75)\tAcc@5  93.75 ( 91.16)\n",
            "Epoch: [22][360/391]\tTime  0.321 ( 0.222)\tLoss 1.1359e+00 (1.1910e+00)\tAcc@1  67.97 ( 65.67)\tAcc@5  92.19 ( 91.09)\n",
            "Epoch: [22][390/391]\tTime  0.145 ( 0.221)\tLoss 1.3962e+00 (1.1964e+00)\tAcc@1  63.75 ( 65.58)\tAcc@5  83.75 ( 90.99)\n",
            "==> Train Accuracy: Acc@1 65.584 || Acc@5 90.988\n",
            "==> Test Accuracy:  Acc@1 57.920 || Acc@5 84.660\n",
            "==> 90.65 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 23, lr: 0.1 -----\n",
            "Epoch: [23][  0/391]\tTime  0.541 ( 0.541)\tLoss 9.9426e-01 (9.9426e-01)\tAcc@1  69.53 ( 69.53)\tAcc@5  93.75 ( 93.75)\n",
            "Epoch: [23][ 30/391]\tTime  0.218 ( 0.231)\tLoss 1.0502e+00 (1.0994e+00)\tAcc@1  69.53 ( 67.82)\tAcc@5  92.19 ( 92.59)\n",
            "Epoch: [23][ 60/391]\tTime  0.302 ( 0.226)\tLoss 9.5849e-01 (1.0816e+00)\tAcc@1  74.22 ( 68.49)\tAcc@5  94.53 ( 92.49)\n",
            "Epoch: [23][ 90/391]\tTime  0.286 ( 0.225)\tLoss 1.1425e+00 (1.1119e+00)\tAcc@1  67.19 ( 67.63)\tAcc@5  87.50 ( 92.15)\n",
            "Epoch: [23][120/391]\tTime  0.177 ( 0.223)\tLoss 1.4280e+00 (1.1284e+00)\tAcc@1  59.38 ( 67.37)\tAcc@5  88.28 ( 91.85)\n",
            "Epoch: [23][150/391]\tTime  0.296 ( 0.223)\tLoss 1.3413e+00 (1.1441e+00)\tAcc@1  64.06 ( 67.00)\tAcc@5  87.50 ( 91.70)\n",
            "Epoch: [23][180/391]\tTime  0.296 ( 0.223)\tLoss 1.3953e+00 (1.1544e+00)\tAcc@1  61.72 ( 66.67)\tAcc@5  86.72 ( 91.64)\n",
            "Epoch: [23][210/391]\tTime  0.289 ( 0.223)\tLoss 1.1036e+00 (1.1601e+00)\tAcc@1  67.97 ( 66.64)\tAcc@5  93.75 ( 91.59)\n",
            "Epoch: [23][240/391]\tTime  0.300 ( 0.223)\tLoss 1.2508e+00 (1.1678e+00)\tAcc@1  61.72 ( 66.37)\tAcc@5  90.62 ( 91.44)\n",
            "Epoch: [23][270/391]\tTime  0.157 ( 0.222)\tLoss 1.4160e+00 (1.1719e+00)\tAcc@1  58.59 ( 66.24)\tAcc@5  88.28 ( 91.42)\n",
            "Epoch: [23][300/391]\tTime  0.169 ( 0.222)\tLoss 1.2646e+00 (1.1753e+00)\tAcc@1  64.06 ( 66.11)\tAcc@5  89.84 ( 91.35)\n",
            "Epoch: [23][330/391]\tTime  0.166 ( 0.222)\tLoss 1.0577e+00 (1.1778e+00)\tAcc@1  68.75 ( 65.98)\tAcc@5  92.97 ( 91.34)\n",
            "Epoch: [23][360/391]\tTime  0.169 ( 0.222)\tLoss 1.2057e+00 (1.1822e+00)\tAcc@1  63.28 ( 65.86)\tAcc@5  92.19 ( 91.27)\n",
            "Epoch: [23][390/391]\tTime  0.152 ( 0.222)\tLoss 1.0239e+00 (1.1830e+00)\tAcc@1  76.25 ( 65.86)\tAcc@5  92.50 ( 91.26)\n",
            "==> Train Accuracy: Acc@1 65.858 || Acc@5 91.260\n",
            "==> Test Accuracy:  Acc@1 58.130 || Acc@5 86.160\n",
            "==> 90.88 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 24, lr: 0.1 -----\n",
            "Epoch: [24][  0/391]\tTime  0.564 ( 0.564)\tLoss 1.0784e+00 (1.0784e+00)\tAcc@1  71.09 ( 71.09)\tAcc@5  91.41 ( 91.41)\n",
            "Epoch: [24][ 30/391]\tTime  0.288 ( 0.238)\tLoss 1.2773e+00 (1.1081e+00)\tAcc@1  66.41 ( 68.25)\tAcc@5  86.72 ( 92.16)\n",
            "Epoch: [24][ 60/391]\tTime  0.159 ( 0.228)\tLoss 1.0026e+00 (1.0868e+00)\tAcc@1  74.22 ( 68.56)\tAcc@5  92.19 ( 92.47)\n",
            "Epoch: [24][ 90/391]\tTime  0.180 ( 0.228)\tLoss 1.0784e+00 (1.1051e+00)\tAcc@1  67.97 ( 67.80)\tAcc@5  93.75 ( 92.35)\n",
            "Epoch: [24][120/391]\tTime  0.162 ( 0.226)\tLoss 1.0979e+00 (1.1270e+00)\tAcc@1  67.19 ( 67.36)\tAcc@5  91.41 ( 92.01)\n",
            "Epoch: [24][150/391]\tTime  0.174 ( 0.225)\tLoss 1.1350e+00 (1.1358e+00)\tAcc@1  64.84 ( 67.28)\tAcc@5  92.19 ( 91.87)\n",
            "Epoch: [24][180/391]\tTime  0.278 ( 0.225)\tLoss 1.0386e+00 (1.1413e+00)\tAcc@1  71.88 ( 67.15)\tAcc@5  92.97 ( 91.78)\n",
            "Epoch: [24][210/391]\tTime  0.276 ( 0.225)\tLoss 1.3384e+00 (1.1496e+00)\tAcc@1  62.50 ( 66.85)\tAcc@5  89.84 ( 91.71)\n",
            "Epoch: [24][240/391]\tTime  0.259 ( 0.224)\tLoss 1.1262e+00 (1.1492e+00)\tAcc@1  67.97 ( 66.76)\tAcc@5  89.84 ( 91.69)\n",
            "Epoch: [24][270/391]\tTime  0.172 ( 0.223)\tLoss 1.2789e+00 (1.1555e+00)\tAcc@1  60.94 ( 66.50)\tAcc@5  88.28 ( 91.59)\n",
            "Epoch: [24][300/391]\tTime  0.303 ( 0.224)\tLoss 1.1303e+00 (1.1622e+00)\tAcc@1  64.06 ( 66.28)\tAcc@5  92.19 ( 91.46)\n",
            "Epoch: [24][330/391]\tTime  0.244 ( 0.223)\tLoss 1.1980e+00 (1.1667e+00)\tAcc@1  66.41 ( 66.10)\tAcc@5  88.28 ( 91.40)\n",
            "Epoch: [24][360/391]\tTime  0.169 ( 0.223)\tLoss 1.2790e+00 (1.1664e+00)\tAcc@1  65.62 ( 66.06)\tAcc@5  87.50 ( 91.36)\n",
            "Epoch: [24][390/391]\tTime  0.150 ( 0.223)\tLoss 1.5094e+00 (1.1692e+00)\tAcc@1  62.50 ( 65.99)\tAcc@5  83.75 ( 91.35)\n",
            "==> Train Accuracy: Acc@1 65.992 || Acc@5 91.348\n",
            "==> Test Accuracy:  Acc@1 56.070 || Acc@5 84.730\n",
            "==> 91.16 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 25, lr: 0.1 -----\n",
            "Epoch: [25][  0/391]\tTime  0.559 ( 0.559)\tLoss 9.6888e-01 (9.6888e-01)\tAcc@1  71.88 ( 71.88)\tAcc@5  93.75 ( 93.75)\n",
            "Epoch: [25][ 30/391]\tTime  0.200 ( 0.232)\tLoss 1.1910e+00 (1.0302e+00)\tAcc@1  66.41 ( 70.69)\tAcc@5  89.84 ( 93.09)\n",
            "Epoch: [25][ 60/391]\tTime  0.284 ( 0.230)\tLoss 9.8484e-01 (1.0669e+00)\tAcc@1  73.44 ( 69.21)\tAcc@5  91.41 ( 92.47)\n",
            "Epoch: [25][ 90/391]\tTime  0.174 ( 0.223)\tLoss 1.2342e+00 (1.0829e+00)\tAcc@1  66.41 ( 68.58)\tAcc@5  89.06 ( 92.34)\n",
            "Epoch: [25][120/391]\tTime  0.163 ( 0.223)\tLoss 1.1011e+00 (1.0805e+00)\tAcc@1  71.88 ( 68.70)\tAcc@5  92.97 ( 92.56)\n",
            "Epoch: [25][150/391]\tTime  0.257 ( 0.222)\tLoss 1.1080e+00 (1.0850e+00)\tAcc@1  64.06 ( 68.56)\tAcc@5  95.31 ( 92.48)\n",
            "Epoch: [25][180/391]\tTime  0.175 ( 0.221)\tLoss 1.3331e+00 (1.0953e+00)\tAcc@1  62.50 ( 68.34)\tAcc@5  90.62 ( 92.34)\n",
            "Epoch: [25][210/391]\tTime  0.245 ( 0.221)\tLoss 1.0537e+00 (1.1038e+00)\tAcc@1  73.44 ( 68.11)\tAcc@5  92.97 ( 92.21)\n",
            "Epoch: [25][240/391]\tTime  0.276 ( 0.222)\tLoss 1.0851e+00 (1.1125e+00)\tAcc@1  64.06 ( 67.81)\tAcc@5  91.41 ( 92.11)\n",
            "Epoch: [25][270/391]\tTime  0.193 ( 0.221)\tLoss 1.2179e+00 (1.1217e+00)\tAcc@1  63.28 ( 67.49)\tAcc@5  92.97 ( 91.98)\n",
            "Epoch: [25][300/391]\tTime  0.238 ( 0.221)\tLoss 1.2196e+00 (1.1287e+00)\tAcc@1  67.19 ( 67.35)\tAcc@5  91.41 ( 91.91)\n",
            "Epoch: [25][330/391]\tTime  0.273 ( 0.222)\tLoss 1.3275e+00 (1.1359e+00)\tAcc@1  60.94 ( 67.11)\tAcc@5  90.62 ( 91.84)\n",
            "Epoch: [25][360/391]\tTime  0.280 ( 0.222)\tLoss 1.2855e+00 (1.1430e+00)\tAcc@1  64.06 ( 66.92)\tAcc@5  89.84 ( 91.77)\n",
            "Epoch: [25][390/391]\tTime  0.151 ( 0.221)\tLoss 1.1443e+00 (1.1499e+00)\tAcc@1  65.00 ( 66.74)\tAcc@5  95.00 ( 91.64)\n",
            "==> Train Accuracy: Acc@1 66.744 || Acc@5 91.644\n",
            "==> Test Accuracy:  Acc@1 55.510 || Acc@5 83.530\n",
            "==> 90.73 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 26, lr: 0.1 -----\n",
            "Epoch: [26][  0/391]\tTime  0.549 ( 0.549)\tLoss 1.1178e+00 (1.1178e+00)\tAcc@1  66.41 ( 66.41)\tAcc@5  91.41 ( 91.41)\n",
            "Epoch: [26][ 30/391]\tTime  0.244 ( 0.232)\tLoss 1.0080e+00 (1.0315e+00)\tAcc@1  68.75 ( 69.48)\tAcc@5  94.53 ( 92.89)\n",
            "Epoch: [26][ 60/391]\tTime  0.165 ( 0.226)\tLoss 1.1337e+00 (1.0586e+00)\tAcc@1  68.75 ( 69.16)\tAcc@5  91.41 ( 92.58)\n",
            "Epoch: [26][ 90/391]\tTime  0.268 ( 0.224)\tLoss 1.0218e+00 (1.0639e+00)\tAcc@1  69.53 ( 68.81)\tAcc@5  93.75 ( 92.69)\n",
            "Epoch: [26][120/391]\tTime  0.313 ( 0.226)\tLoss 1.1620e+00 (1.0666e+00)\tAcc@1  67.97 ( 68.58)\tAcc@5  92.97 ( 92.83)\n",
            "Epoch: [26][150/391]\tTime  0.181 ( 0.226)\tLoss 1.0152e+00 (1.0807e+00)\tAcc@1  71.09 ( 68.34)\tAcc@5  92.97 ( 92.66)\n",
            "Epoch: [26][180/391]\tTime  0.299 ( 0.228)\tLoss 1.1151e+00 (1.0852e+00)\tAcc@1  64.06 ( 68.16)\tAcc@5  92.19 ( 92.58)\n",
            "Epoch: [26][210/391]\tTime  0.250 ( 0.229)\tLoss 1.1061e+00 (1.0911e+00)\tAcc@1  69.53 ( 68.08)\tAcc@5  91.41 ( 92.45)\n",
            "Epoch: [26][240/391]\tTime  0.281 ( 0.229)\tLoss 1.1739e+00 (1.1032e+00)\tAcc@1  61.72 ( 67.63)\tAcc@5  93.75 ( 92.28)\n",
            "Epoch: [26][270/391]\tTime  0.282 ( 0.229)\tLoss 1.1850e+00 (1.1124e+00)\tAcc@1  65.62 ( 67.36)\tAcc@5  91.41 ( 92.24)\n",
            "Epoch: [26][300/391]\tTime  0.227 ( 0.228)\tLoss 1.1042e+00 (1.1166e+00)\tAcc@1  70.31 ( 67.28)\tAcc@5  91.41 ( 92.13)\n",
            "Epoch: [26][330/391]\tTime  0.275 ( 0.228)\tLoss 1.2382e+00 (1.1233e+00)\tAcc@1  66.41 ( 67.17)\tAcc@5  87.50 ( 92.02)\n",
            "Epoch: [26][360/391]\tTime  0.249 ( 0.228)\tLoss 1.3098e+00 (1.1303e+00)\tAcc@1  59.38 ( 67.04)\tAcc@5  88.28 ( 91.88)\n",
            "Epoch: [26][390/391]\tTime  0.146 ( 0.228)\tLoss 1.3781e+00 (1.1331e+00)\tAcc@1  66.25 ( 66.96)\tAcc@5  83.75 ( 91.88)\n",
            "==> Train Accuracy: Acc@1 66.964 || Acc@5 91.884\n",
            "==> Test Accuracy:  Acc@1 54.450 || Acc@5 83.060\n",
            "==> 93.14 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 27, lr: 0.1 -----\n",
            "Epoch: [27][  0/391]\tTime  0.558 ( 0.558)\tLoss 1.1172e+00 (1.1172e+00)\tAcc@1  68.75 ( 68.75)\tAcc@5  90.62 ( 90.62)\n",
            "Epoch: [27][ 30/391]\tTime  0.301 ( 0.235)\tLoss 8.9053e-01 (1.0219e+00)\tAcc@1  72.66 ( 69.18)\tAcc@5  93.75 ( 93.20)\n",
            "Epoch: [27][ 60/391]\tTime  0.251 ( 0.228)\tLoss 8.3294e-01 (1.0165e+00)\tAcc@1  69.53 ( 69.40)\tAcc@5  97.66 ( 93.39)\n",
            "Epoch: [27][ 90/391]\tTime  0.167 ( 0.227)\tLoss 8.4039e-01 (1.0339e+00)\tAcc@1  77.34 ( 68.93)\tAcc@5  95.31 ( 93.06)\n",
            "Epoch: [27][120/391]\tTime  0.289 ( 0.227)\tLoss 1.1568e+00 (1.0506e+00)\tAcc@1  64.84 ( 68.68)\tAcc@5  94.53 ( 92.81)\n",
            "Epoch: [27][150/391]\tTime  0.233 ( 0.226)\tLoss 9.4339e-01 (1.0691e+00)\tAcc@1  69.53 ( 68.34)\tAcc@5  95.31 ( 92.72)\n",
            "Epoch: [27][180/391]\tTime  0.174 ( 0.226)\tLoss 1.1828e+00 (1.0798e+00)\tAcc@1  66.41 ( 68.18)\tAcc@5  96.09 ( 92.54)\n",
            "Epoch: [27][210/391]\tTime  0.183 ( 0.225)\tLoss 1.1921e+00 (1.0900e+00)\tAcc@1  60.94 ( 67.86)\tAcc@5  91.41 ( 92.34)\n",
            "Epoch: [27][240/391]\tTime  0.252 ( 0.225)\tLoss 1.2116e+00 (1.0918e+00)\tAcc@1  64.84 ( 67.80)\tAcc@5  89.84 ( 92.38)\n",
            "Epoch: [27][270/391]\tTime  0.292 ( 0.225)\tLoss 1.1635e+00 (1.1027e+00)\tAcc@1  65.62 ( 67.58)\tAcc@5  89.84 ( 92.23)\n",
            "Epoch: [27][300/391]\tTime  0.312 ( 0.226)\tLoss 1.1286e+00 (1.1082e+00)\tAcc@1  71.88 ( 67.45)\tAcc@5  91.41 ( 92.15)\n",
            "Epoch: [27][330/391]\tTime  0.326 ( 0.226)\tLoss 1.0494e+00 (1.1134e+00)\tAcc@1  69.53 ( 67.35)\tAcc@5  91.41 ( 92.04)\n",
            "Epoch: [27][360/391]\tTime  0.260 ( 0.226)\tLoss 1.1100e+00 (1.1163e+00)\tAcc@1  70.31 ( 67.29)\tAcc@5  89.84 ( 91.95)\n",
            "Epoch: [27][390/391]\tTime  0.150 ( 0.226)\tLoss 1.4083e+00 (1.1209e+00)\tAcc@1  63.75 ( 67.30)\tAcc@5  88.75 ( 91.84)\n",
            "==> Train Accuracy: Acc@1 67.304 || Acc@5 91.844\n",
            "==> Test Accuracy:  Acc@1 57.730 || Acc@5 84.990\n",
            "==> 92.36 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 28, lr: 0.1 -----\n",
            "Epoch: [28][  0/391]\tTime  0.572 ( 0.572)\tLoss 9.8807e-01 (9.8807e-01)\tAcc@1  71.88 ( 71.88)\tAcc@5  95.31 ( 95.31)\n",
            "Epoch: [28][ 30/391]\tTime  0.175 ( 0.232)\tLoss 1.1330e+00 (1.0095e+00)\tAcc@1  66.41 ( 70.59)\tAcc@5  90.62 ( 93.37)\n",
            "Epoch: [28][ 60/391]\tTime  0.169 ( 0.229)\tLoss 8.6133e-01 (1.0189e+00)\tAcc@1  70.31 ( 70.18)\tAcc@5  93.75 ( 93.05)\n",
            "Epoch: [28][ 90/391]\tTime  0.172 ( 0.229)\tLoss 1.1413e+00 (1.0308e+00)\tAcc@1  68.75 ( 69.79)\tAcc@5  90.62 ( 93.13)\n",
            "Epoch: [28][120/391]\tTime  0.167 ( 0.229)\tLoss 9.3345e-01 (1.0487e+00)\tAcc@1  71.09 ( 69.40)\tAcc@5  96.09 ( 92.94)\n",
            "Epoch: [28][150/391]\tTime  0.171 ( 0.227)\tLoss 1.0533e+00 (1.0526e+00)\tAcc@1  70.31 ( 69.24)\tAcc@5  93.75 ( 93.03)\n",
            "Epoch: [28][180/391]\tTime  0.156 ( 0.226)\tLoss 1.0014e+00 (1.0646e+00)\tAcc@1  71.09 ( 68.96)\tAcc@5  94.53 ( 92.97)\n",
            "Epoch: [28][210/391]\tTime  0.181 ( 0.225)\tLoss 1.1714e+00 (1.0824e+00)\tAcc@1  68.75 ( 68.48)\tAcc@5  89.84 ( 92.69)\n",
            "Epoch: [28][240/391]\tTime  0.259 ( 0.225)\tLoss 1.2904e+00 (1.0938e+00)\tAcc@1  65.62 ( 68.25)\tAcc@5  90.62 ( 92.47)\n",
            "Epoch: [28][270/391]\tTime  0.232 ( 0.225)\tLoss 1.2021e+00 (1.0936e+00)\tAcc@1  64.06 ( 68.24)\tAcc@5  93.75 ( 92.42)\n",
            "Epoch: [28][300/391]\tTime  0.161 ( 0.224)\tLoss 1.4231e+00 (1.0981e+00)\tAcc@1  57.81 ( 68.11)\tAcc@5  90.62 ( 92.35)\n",
            "Epoch: [28][330/391]\tTime  0.299 ( 0.224)\tLoss 1.1202e+00 (1.0990e+00)\tAcc@1  70.31 ( 68.04)\tAcc@5  90.62 ( 92.34)\n",
            "Epoch: [28][360/391]\tTime  0.233 ( 0.224)\tLoss 1.0881e+00 (1.1083e+00)\tAcc@1  64.06 ( 67.76)\tAcc@5  92.97 ( 92.17)\n",
            "Epoch: [28][390/391]\tTime  0.151 ( 0.224)\tLoss 1.2070e+00 (1.1142e+00)\tAcc@1  68.75 ( 67.65)\tAcc@5  90.00 ( 92.07)\n",
            "==> Train Accuracy: Acc@1 67.652 || Acc@5 92.074\n",
            "==> Test Accuracy:  Acc@1 56.100 || Acc@5 84.670\n",
            "==> 91.69 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 29, lr: 0.1 -----\n",
            "Epoch: [29][  0/391]\tTime  0.592 ( 0.592)\tLoss 9.7914e-01 (9.7914e-01)\tAcc@1  73.44 ( 73.44)\tAcc@5  89.84 ( 89.84)\n",
            "Epoch: [29][ 30/391]\tTime  0.297 ( 0.236)\tLoss 1.1036e+00 (1.0100e+00)\tAcc@1  67.97 ( 70.24)\tAcc@5  91.41 ( 93.27)\n",
            "Epoch: [29][ 60/391]\tTime  0.202 ( 0.231)\tLoss 8.9089e-01 (9.9579e-01)\tAcc@1  74.22 ( 70.82)\tAcc@5  96.09 ( 93.11)\n",
            "Epoch: [29][ 90/391]\tTime  0.234 ( 0.228)\tLoss 1.3734e+00 (1.0101e+00)\tAcc@1  60.16 ( 70.57)\tAcc@5  90.62 ( 93.03)\n",
            "Epoch: [29][120/391]\tTime  0.196 ( 0.225)\tLoss 1.0820e+00 (1.0186e+00)\tAcc@1  67.19 ( 70.38)\tAcc@5  91.41 ( 93.00)\n",
            "Epoch: [29][150/391]\tTime  0.160 ( 0.226)\tLoss 1.3652e+00 (1.0462e+00)\tAcc@1  58.59 ( 69.51)\tAcc@5  88.28 ( 92.65)\n",
            "Epoch: [29][180/391]\tTime  0.178 ( 0.228)\tLoss 1.3044e+00 (1.0555e+00)\tAcc@1  64.84 ( 69.21)\tAcc@5  86.72 ( 92.58)\n",
            "Epoch: [29][210/391]\tTime  0.168 ( 0.229)\tLoss 1.0713e+00 (1.0714e+00)\tAcc@1  67.19 ( 68.85)\tAcc@5  92.19 ( 92.40)\n",
            "Epoch: [29][240/391]\tTime  0.157 ( 0.229)\tLoss 1.4066e+00 (1.0796e+00)\tAcc@1  57.81 ( 68.64)\tAcc@5  92.19 ( 92.34)\n",
            "Epoch: [29][270/391]\tTime  0.164 ( 0.230)\tLoss 1.0300e+00 (1.0856e+00)\tAcc@1  69.53 ( 68.43)\tAcc@5  90.62 ( 92.33)\n",
            "Epoch: [29][300/391]\tTime  0.289 ( 0.230)\tLoss 1.1838e+00 (1.0919e+00)\tAcc@1  68.75 ( 68.30)\tAcc@5  86.72 ( 92.30)\n",
            "Epoch: [29][330/391]\tTime  0.298 ( 0.231)\tLoss 1.0744e+00 (1.0985e+00)\tAcc@1  67.19 ( 68.06)\tAcc@5  92.97 ( 92.23)\n",
            "Epoch: [29][360/391]\tTime  0.248 ( 0.231)\tLoss 1.1181e+00 (1.1027e+00)\tAcc@1  67.97 ( 67.93)\tAcc@5  91.41 ( 92.18)\n",
            "Epoch: [29][390/391]\tTime  0.142 ( 0.230)\tLoss 1.1733e+00 (1.1052e+00)\tAcc@1  66.25 ( 67.87)\tAcc@5  91.25 ( 92.13)\n",
            "==> Train Accuracy: Acc@1 67.866 || Acc@5 92.126\n",
            "==> Test Accuracy:  Acc@1 56.740 || Acc@5 85.180\n",
            "==> 94.09 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 30, lr: 0.1 -----\n",
            "Epoch: [30][  0/391]\tTime  0.564 ( 0.564)\tLoss 1.0897e+00 (1.0897e+00)\tAcc@1  67.97 ( 67.97)\tAcc@5  92.19 ( 92.19)\n",
            "Epoch: [30][ 30/391]\tTime  0.172 ( 0.231)\tLoss 1.1798e+00 (1.0362e+00)\tAcc@1  66.41 ( 70.14)\tAcc@5  91.41 ( 93.12)\n",
            "Epoch: [30][ 60/391]\tTime  0.310 ( 0.231)\tLoss 9.4775e-01 (1.0357e+00)\tAcc@1  73.44 ( 69.89)\tAcc@5  94.53 ( 92.94)\n",
            "Epoch: [30][ 90/391]\tTime  0.204 ( 0.226)\tLoss 1.0099e+00 (1.0364e+00)\tAcc@1  71.88 ( 69.72)\tAcc@5  93.75 ( 93.05)\n",
            "Epoch: [30][120/391]\tTime  0.224 ( 0.224)\tLoss 1.1876e+00 (1.0455e+00)\tAcc@1  64.84 ( 69.40)\tAcc@5  89.84 ( 93.02)\n",
            "Epoch: [30][150/391]\tTime  0.230 ( 0.224)\tLoss 1.0541e+00 (1.0435e+00)\tAcc@1  68.75 ( 69.54)\tAcc@5  93.75 ( 92.98)\n",
            "Epoch: [30][180/391]\tTime  0.165 ( 0.224)\tLoss 1.2707e+00 (1.0533e+00)\tAcc@1  67.97 ( 69.38)\tAcc@5  87.50 ( 92.85)\n",
            "Epoch: [30][210/391]\tTime  0.175 ( 0.225)\tLoss 1.0818e+00 (1.0657e+00)\tAcc@1  67.97 ( 69.06)\tAcc@5  93.75 ( 92.72)\n",
            "Epoch: [30][240/391]\tTime  0.163 ( 0.224)\tLoss 1.1956e+00 (1.0723e+00)\tAcc@1  62.50 ( 68.89)\tAcc@5  90.62 ( 92.66)\n",
            "Epoch: [30][270/391]\tTime  0.289 ( 0.225)\tLoss 1.1134e+00 (1.0816e+00)\tAcc@1  67.97 ( 68.58)\tAcc@5  92.19 ( 92.60)\n",
            "Epoch: [30][300/391]\tTime  0.199 ( 0.225)\tLoss 1.0804e+00 (1.0893e+00)\tAcc@1  74.22 ( 68.44)\tAcc@5  92.97 ( 92.54)\n",
            "Epoch: [30][330/391]\tTime  0.165 ( 0.225)\tLoss 9.0027e-01 (1.0930e+00)\tAcc@1  71.88 ( 68.36)\tAcc@5  95.31 ( 92.50)\n",
            "Epoch: [30][360/391]\tTime  0.202 ( 0.225)\tLoss 8.8961e-01 (1.0968e+00)\tAcc@1  73.44 ( 68.31)\tAcc@5  98.44 ( 92.37)\n",
            "Epoch: [30][390/391]\tTime  0.152 ( 0.225)\tLoss 1.1406e+00 (1.1011e+00)\tAcc@1  65.00 ( 68.21)\tAcc@5  92.50 ( 92.31)\n",
            "==> Train Accuracy: Acc@1 68.208 || Acc@5 92.310\n",
            "==> Test Accuracy:  Acc@1 58.040 || Acc@5 85.570\n",
            "==> 92.09 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 31, lr: 0.1 -----\n",
            "Epoch: [31][  0/391]\tTime  0.566 ( 0.566)\tLoss 1.0132e+00 (1.0132e+00)\tAcc@1  69.53 ( 69.53)\tAcc@5  89.06 ( 89.06)\n",
            "Epoch: [31][ 30/391]\tTime  0.271 ( 0.234)\tLoss 1.0307e+00 (9.6606e-01)\tAcc@1  71.09 ( 71.32)\tAcc@5  92.19 ( 93.83)\n",
            "Epoch: [31][ 60/391]\tTime  0.273 ( 0.235)\tLoss 1.0322e+00 (9.8203e-01)\tAcc@1  72.66 ( 70.89)\tAcc@5  92.97 ( 93.76)\n",
            "Epoch: [31][ 90/391]\tTime  0.224 ( 0.233)\tLoss 1.0413e+00 (1.0154e+00)\tAcc@1  65.62 ( 70.23)\tAcc@5  93.75 ( 93.37)\n",
            "Epoch: [31][120/391]\tTime  0.304 ( 0.231)\tLoss 1.0295e+00 (1.0347e+00)\tAcc@1  67.19 ( 69.56)\tAcc@5  93.75 ( 93.10)\n",
            "Epoch: [31][150/391]\tTime  0.171 ( 0.230)\tLoss 1.3087e+00 (1.0438e+00)\tAcc@1  62.50 ( 69.30)\tAcc@5  90.62 ( 93.04)\n",
            "Epoch: [31][180/391]\tTime  0.172 ( 0.231)\tLoss 1.1496e+00 (1.0513e+00)\tAcc@1  62.50 ( 69.03)\tAcc@5  91.41 ( 92.90)\n",
            "Epoch: [31][210/391]\tTime  0.173 ( 0.231)\tLoss 1.2468e+00 (1.0622e+00)\tAcc@1  64.84 ( 68.70)\tAcc@5  91.41 ( 92.80)\n",
            "Epoch: [31][240/391]\tTime  0.174 ( 0.230)\tLoss 1.0420e+00 (1.0738e+00)\tAcc@1  70.31 ( 68.40)\tAcc@5  92.19 ( 92.65)\n",
            "Epoch: [31][270/391]\tTime  0.170 ( 0.230)\tLoss 1.1642e+00 (1.0800e+00)\tAcc@1  66.41 ( 68.24)\tAcc@5  92.97 ( 92.54)\n",
            "Epoch: [31][300/391]\tTime  0.235 ( 0.228)\tLoss 1.1278e+00 (1.0838e+00)\tAcc@1  67.97 ( 68.17)\tAcc@5  90.62 ( 92.51)\n",
            "Epoch: [31][330/391]\tTime  0.172 ( 0.228)\tLoss 1.1235e+00 (1.0884e+00)\tAcc@1  67.19 ( 68.02)\tAcc@5  91.41 ( 92.40)\n",
            "Epoch: [31][360/391]\tTime  0.179 ( 0.228)\tLoss 1.1042e+00 (1.0910e+00)\tAcc@1  67.19 ( 67.95)\tAcc@5  91.41 ( 92.35)\n",
            "Epoch: [31][390/391]\tTime  0.139 ( 0.227)\tLoss 1.0044e+00 (1.0920e+00)\tAcc@1  72.50 ( 67.97)\tAcc@5  97.50 ( 92.33)\n",
            "==> Train Accuracy: Acc@1 67.972 || Acc@5 92.328\n",
            "==> Test Accuracy:  Acc@1 57.510 || Acc@5 84.870\n",
            "==> 93.01 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 32, lr: 0.1 -----\n",
            "Epoch: [32][  0/391]\tTime  0.556 ( 0.556)\tLoss 1.0769e+00 (1.0769e+00)\tAcc@1  72.66 ( 72.66)\tAcc@5  92.97 ( 92.97)\n",
            "Epoch: [32][ 30/391]\tTime  0.318 ( 0.237)\tLoss 7.7249e-01 (1.0225e+00)\tAcc@1  78.12 ( 70.34)\tAcc@5  94.53 ( 93.57)\n",
            "Epoch: [32][ 60/391]\tTime  0.256 ( 0.232)\tLoss 8.7023e-01 (1.0167e+00)\tAcc@1  75.78 ( 70.47)\tAcc@5  94.53 ( 93.29)\n",
            "Epoch: [32][ 90/391]\tTime  0.269 ( 0.229)\tLoss 1.1982e+00 (1.0248e+00)\tAcc@1  64.06 ( 70.16)\tAcc@5  93.75 ( 93.15)\n",
            "Epoch: [32][120/391]\tTime  0.297 ( 0.231)\tLoss 8.6783e-01 (1.0279e+00)\tAcc@1  76.56 ( 70.00)\tAcc@5  94.53 ( 93.07)\n",
            "Epoch: [32][150/391]\tTime  0.276 ( 0.230)\tLoss 1.2159e+00 (1.0386e+00)\tAcc@1  62.50 ( 69.65)\tAcc@5  91.41 ( 93.09)\n",
            "Epoch: [32][180/391]\tTime  0.166 ( 0.229)\tLoss 1.0689e+00 (1.0428e+00)\tAcc@1  67.19 ( 69.56)\tAcc@5  92.19 ( 92.89)\n",
            "Epoch: [32][210/391]\tTime  0.179 ( 0.229)\tLoss 1.2139e+00 (1.0526e+00)\tAcc@1  66.41 ( 69.21)\tAcc@5  92.19 ( 92.83)\n",
            "Epoch: [32][240/391]\tTime  0.161 ( 0.229)\tLoss 1.2044e+00 (1.0632e+00)\tAcc@1  63.28 ( 68.96)\tAcc@5  92.19 ( 92.79)\n",
            "Epoch: [32][270/391]\tTime  0.283 ( 0.229)\tLoss 9.7172e-01 (1.0691e+00)\tAcc@1  72.66 ( 68.72)\tAcc@5  93.75 ( 92.71)\n",
            "Epoch: [32][300/391]\tTime  0.191 ( 0.228)\tLoss 1.2129e+00 (1.0730e+00)\tAcc@1  66.41 ( 68.67)\tAcc@5  89.84 ( 92.63)\n",
            "Epoch: [32][330/391]\tTime  0.165 ( 0.228)\tLoss 1.0763e+00 (1.0755e+00)\tAcc@1  69.53 ( 68.69)\tAcc@5  89.84 ( 92.53)\n",
            "Epoch: [32][360/391]\tTime  0.172 ( 0.228)\tLoss 8.9215e-01 (1.0794e+00)\tAcc@1  75.78 ( 68.58)\tAcc@5  93.75 ( 92.49)\n",
            "Epoch: [32][390/391]\tTime  0.151 ( 0.228)\tLoss 7.7913e-01 (1.0868e+00)\tAcc@1  72.50 ( 68.39)\tAcc@5  98.75 ( 92.40)\n",
            "==> Train Accuracy: Acc@1 68.386 || Acc@5 92.404\n",
            "==> Test Accuracy:  Acc@1 52.340 || Acc@5 80.470\n",
            "==> 93.30 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 33, lr: 0.1 -----\n",
            "Epoch: [33][  0/391]\tTime  0.570 ( 0.570)\tLoss 9.6389e-01 (9.6389e-01)\tAcc@1  68.75 ( 68.75)\tAcc@5  96.09 ( 96.09)\n",
            "Epoch: [33][ 30/391]\tTime  0.165 ( 0.238)\tLoss 8.0339e-01 (9.8101e-01)\tAcc@1  72.66 ( 70.44)\tAcc@5  96.88 ( 94.41)\n",
            "Epoch: [33][ 60/391]\tTime  0.178 ( 0.237)\tLoss 7.3815e-01 (9.9917e-01)\tAcc@1  77.34 ( 69.90)\tAcc@5  96.09 ( 94.17)\n",
            "Epoch: [33][ 90/391]\tTime  0.162 ( 0.233)\tLoss 1.0646e+00 (1.0099e+00)\tAcc@1  71.88 ( 69.87)\tAcc@5  90.62 ( 93.66)\n",
            "Epoch: [33][120/391]\tTime  0.185 ( 0.232)\tLoss 1.3436e+00 (1.0321e+00)\tAcc@1  62.50 ( 69.38)\tAcc@5  88.28 ( 93.37)\n",
            "Epoch: [33][150/391]\tTime  0.187 ( 0.231)\tLoss 8.7190e-01 (1.0380e+00)\tAcc@1  72.66 ( 69.28)\tAcc@5  96.09 ( 93.25)\n",
            "Epoch: [33][180/391]\tTime  0.176 ( 0.230)\tLoss 9.7165e-01 (1.0399e+00)\tAcc@1  70.31 ( 69.18)\tAcc@5  92.19 ( 93.18)\n",
            "Epoch: [33][210/391]\tTime  0.166 ( 0.230)\tLoss 9.9004e-01 (1.0480e+00)\tAcc@1  71.09 ( 68.93)\tAcc@5  92.97 ( 93.04)\n",
            "Epoch: [33][240/391]\tTime  0.228 ( 0.229)\tLoss 1.2021e+00 (1.0528e+00)\tAcc@1  67.19 ( 68.85)\tAcc@5  91.41 ( 92.88)\n",
            "Epoch: [33][270/391]\tTime  0.276 ( 0.228)\tLoss 1.2335e+00 (1.0613e+00)\tAcc@1  64.06 ( 68.71)\tAcc@5  90.62 ( 92.82)\n",
            "Epoch: [33][300/391]\tTime  0.210 ( 0.228)\tLoss 1.1255e+00 (1.0647e+00)\tAcc@1  67.19 ( 68.69)\tAcc@5  92.19 ( 92.79)\n",
            "Epoch: [33][330/391]\tTime  0.179 ( 0.228)\tLoss 1.2116e+00 (1.0698e+00)\tAcc@1  63.28 ( 68.63)\tAcc@5  92.19 ( 92.67)\n",
            "Epoch: [33][360/391]\tTime  0.163 ( 0.228)\tLoss 1.2784e+00 (1.0784e+00)\tAcc@1  61.72 ( 68.46)\tAcc@5  89.84 ( 92.58)\n",
            "Epoch: [33][390/391]\tTime  0.152 ( 0.227)\tLoss 1.0018e+00 (1.0849e+00)\tAcc@1  68.75 ( 68.27)\tAcc@5  93.75 ( 92.52)\n",
            "==> Train Accuracy: Acc@1 68.270 || Acc@5 92.522\n",
            "==> Test Accuracy:  Acc@1 52.120 || Acc@5 80.160\n",
            "==> 92.97 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 34, lr: 0.1 -----\n",
            "Epoch: [34][  0/391]\tTime  0.554 ( 0.554)\tLoss 9.7156e-01 (9.7156e-01)\tAcc@1  68.75 ( 68.75)\tAcc@5  94.53 ( 94.53)\n",
            "Epoch: [34][ 30/391]\tTime  0.299 ( 0.244)\tLoss 8.7177e-01 (1.0306e+00)\tAcc@1  75.00 ( 70.01)\tAcc@5  95.31 ( 93.17)\n",
            "Epoch: [34][ 60/391]\tTime  0.247 ( 0.231)\tLoss 9.3457e-01 (1.0189e+00)\tAcc@1  77.34 ( 70.34)\tAcc@5  93.75 ( 93.22)\n",
            "Epoch: [34][ 90/391]\tTime  0.254 ( 0.227)\tLoss 1.0336e+00 (9.9915e-01)\tAcc@1  68.75 ( 70.79)\tAcc@5  93.75 ( 93.39)\n",
            "Epoch: [34][120/391]\tTime  0.255 ( 0.228)\tLoss 9.2553e-01 (1.0023e+00)\tAcc@1  72.66 ( 70.60)\tAcc@5  97.66 ( 93.42)\n",
            "Epoch: [34][150/391]\tTime  0.273 ( 0.229)\tLoss 1.0994e+00 (1.0167e+00)\tAcc@1  63.28 ( 70.08)\tAcc@5  93.75 ( 93.33)\n",
            "Epoch: [34][180/391]\tTime  0.235 ( 0.228)\tLoss 9.7258e-01 (1.0194e+00)\tAcc@1  73.44 ( 69.97)\tAcc@5  93.75 ( 93.25)\n",
            "Epoch: [34][210/391]\tTime  0.255 ( 0.227)\tLoss 1.1788e+00 (1.0316e+00)\tAcc@1  64.06 ( 69.68)\tAcc@5  90.62 ( 93.05)\n",
            "Epoch: [34][240/391]\tTime  0.233 ( 0.226)\tLoss 1.0069e+00 (1.0380e+00)\tAcc@1  69.53 ( 69.44)\tAcc@5  93.75 ( 93.01)\n",
            "Epoch: [34][270/391]\tTime  0.179 ( 0.226)\tLoss 1.0501e+00 (1.0447e+00)\tAcc@1  68.75 ( 69.30)\tAcc@5  96.88 ( 92.91)\n",
            "Epoch: [34][300/391]\tTime  0.167 ( 0.226)\tLoss 1.1733e+00 (1.0523e+00)\tAcc@1  66.41 ( 69.10)\tAcc@5  89.06 ( 92.77)\n",
            "Epoch: [34][330/391]\tTime  0.326 ( 0.226)\tLoss 1.1441e+00 (1.0589e+00)\tAcc@1  67.19 ( 68.91)\tAcc@5  94.53 ( 92.69)\n",
            "Epoch: [34][360/391]\tTime  0.174 ( 0.226)\tLoss 1.1711e+00 (1.0643e+00)\tAcc@1  67.19 ( 68.80)\tAcc@5  89.84 ( 92.57)\n",
            "Epoch: [34][390/391]\tTime  0.143 ( 0.225)\tLoss 8.6497e-01 (1.0693e+00)\tAcc@1  78.75 ( 68.65)\tAcc@5  93.75 ( 92.50)\n",
            "==> Train Accuracy: Acc@1 68.652 || Acc@5 92.502\n",
            "==> Test Accuracy:  Acc@1 58.790 || Acc@5 85.680\n",
            "==> 92.17 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 35, lr: 0.1 -----\n",
            "Epoch: [35][  0/391]\tTime  0.575 ( 0.575)\tLoss 7.2447e-01 (7.2447e-01)\tAcc@1  76.56 ( 76.56)\tAcc@5  98.44 ( 98.44)\n",
            "Epoch: [35][ 30/391]\tTime  0.176 ( 0.237)\tLoss 1.0542e+00 (9.5227e-01)\tAcc@1  67.19 ( 71.70)\tAcc@5  93.75 ( 94.53)\n",
            "Epoch: [35][ 60/391]\tTime  0.166 ( 0.230)\tLoss 9.4884e-01 (9.4890e-01)\tAcc@1  73.44 ( 71.90)\tAcc@5  94.53 ( 94.30)\n",
            "Epoch: [35][ 90/391]\tTime  0.172 ( 0.231)\tLoss 1.0798e+00 (9.7328e-01)\tAcc@1  63.28 ( 71.13)\tAcc@5  95.31 ( 94.09)\n",
            "Epoch: [35][120/391]\tTime  0.173 ( 0.230)\tLoss 1.0604e+00 (9.9118e-01)\tAcc@1  68.75 ( 70.89)\tAcc@5  94.53 ( 93.85)\n",
            "Epoch: [35][150/391]\tTime  0.242 ( 0.228)\tLoss 1.1595e+00 (1.0086e+00)\tAcc@1  67.97 ( 70.53)\tAcc@5  89.06 ( 93.55)\n",
            "Epoch: [35][180/391]\tTime  0.186 ( 0.229)\tLoss 1.1213e+00 (1.0193e+00)\tAcc@1  67.19 ( 70.28)\tAcc@5  94.53 ( 93.43)\n",
            "Epoch: [35][210/391]\tTime  0.166 ( 0.229)\tLoss 1.0951e+00 (1.0340e+00)\tAcc@1  66.41 ( 69.92)\tAcc@5  91.41 ( 93.23)\n",
            "Epoch: [35][240/391]\tTime  0.168 ( 0.229)\tLoss 1.1664e+00 (1.0356e+00)\tAcc@1  67.19 ( 69.96)\tAcc@5  92.19 ( 93.16)\n",
            "Epoch: [35][270/391]\tTime  0.276 ( 0.228)\tLoss 9.6513e-01 (1.0426e+00)\tAcc@1  73.44 ( 69.75)\tAcc@5  95.31 ( 93.06)\n",
            "Epoch: [35][300/391]\tTime  0.169 ( 0.227)\tLoss 1.0656e+00 (1.0487e+00)\tAcc@1  71.09 ( 69.51)\tAcc@5  92.97 ( 92.98)\n",
            "Epoch: [35][330/391]\tTime  0.315 ( 0.228)\tLoss 1.0440e+00 (1.0532e+00)\tAcc@1  67.97 ( 69.38)\tAcc@5  93.75 ( 92.91)\n",
            "Epoch: [35][360/391]\tTime  0.168 ( 0.227)\tLoss 8.1706e-01 (1.0577e+00)\tAcc@1  75.78 ( 69.16)\tAcc@5  96.09 ( 92.84)\n",
            "Epoch: [35][390/391]\tTime  0.150 ( 0.227)\tLoss 1.3055e+00 (1.0635e+00)\tAcc@1  60.00 ( 69.01)\tAcc@5  87.50 ( 92.75)\n",
            "==> Train Accuracy: Acc@1 69.006 || Acc@5 92.754\n",
            "==> Test Accuracy:  Acc@1 57.940 || Acc@5 85.280\n",
            "==> 92.75 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 36, lr: 0.1 -----\n",
            "Epoch: [36][  0/391]\tTime  0.563 ( 0.563)\tLoss 8.4393e-01 (8.4393e-01)\tAcc@1  73.44 ( 73.44)\tAcc@5  96.09 ( 96.09)\n",
            "Epoch: [36][ 30/391]\tTime  0.227 ( 0.234)\tLoss 8.1634e-01 (9.7726e-01)\tAcc@1  75.78 ( 70.46)\tAcc@5  92.97 ( 94.08)\n",
            "Epoch: [36][ 60/391]\tTime  0.175 ( 0.228)\tLoss 1.0609e+00 (9.8724e-01)\tAcc@1  67.97 ( 70.33)\tAcc@5  91.41 ( 93.65)\n",
            "Epoch: [36][ 90/391]\tTime  0.157 ( 0.229)\tLoss 9.9040e-01 (9.9289e-01)\tAcc@1  68.75 ( 70.40)\tAcc@5  96.09 ( 93.69)\n",
            "Epoch: [36][120/391]\tTime  0.173 ( 0.227)\tLoss 1.1782e+00 (9.9453e-01)\tAcc@1  66.41 ( 70.65)\tAcc@5  88.28 ( 93.58)\n",
            "Epoch: [36][150/391]\tTime  0.157 ( 0.227)\tLoss 9.8304e-01 (1.0094e+00)\tAcc@1  68.75 ( 70.25)\tAcc@5  96.09 ( 93.45)\n",
            "Epoch: [36][180/391]\tTime  0.168 ( 0.227)\tLoss 1.0738e+00 (1.0246e+00)\tAcc@1  67.19 ( 69.89)\tAcc@5  92.19 ( 93.21)\n",
            "Epoch: [36][210/391]\tTime  0.194 ( 0.227)\tLoss 1.1726e+00 (1.0281e+00)\tAcc@1  64.84 ( 69.71)\tAcc@5  90.62 ( 93.17)\n",
            "Epoch: [36][240/391]\tTime  0.169 ( 0.228)\tLoss 1.0328e+00 (1.0329e+00)\tAcc@1  66.41 ( 69.70)\tAcc@5  92.19 ( 93.05)\n",
            "Epoch: [36][270/391]\tTime  0.167 ( 0.228)\tLoss 9.0275e-01 (1.0346e+00)\tAcc@1  72.66 ( 69.65)\tAcc@5  95.31 ( 93.01)\n",
            "Epoch: [36][300/391]\tTime  0.240 ( 0.228)\tLoss 1.2620e+00 (1.0425e+00)\tAcc@1  57.03 ( 69.42)\tAcc@5  91.41 ( 92.88)\n",
            "Epoch: [36][330/391]\tTime  0.187 ( 0.227)\tLoss 1.0541e+00 (1.0489e+00)\tAcc@1  66.41 ( 69.30)\tAcc@5  92.97 ( 92.75)\n",
            "Epoch: [36][360/391]\tTime  0.171 ( 0.227)\tLoss 1.0133e+00 (1.0479e+00)\tAcc@1  76.56 ( 69.29)\tAcc@5  92.97 ( 92.78)\n",
            "Epoch: [36][390/391]\tTime  0.149 ( 0.227)\tLoss 1.6471e+00 (1.0551e+00)\tAcc@1  56.25 ( 69.16)\tAcc@5  85.00 ( 92.69)\n",
            "==> Train Accuracy: Acc@1 69.164 || Acc@5 92.692\n",
            "==> Test Accuracy:  Acc@1 57.930 || Acc@5 85.770\n",
            "==> 92.80 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 37, lr: 0.1 -----\n",
            "Epoch: [37][  0/391]\tTime  0.583 ( 0.583)\tLoss 1.1177e+00 (1.1177e+00)\tAcc@1  67.97 ( 67.97)\tAcc@5  91.41 ( 91.41)\n",
            "Epoch: [37][ 30/391]\tTime  0.295 ( 0.242)\tLoss 1.1461e+00 (9.8043e-01)\tAcc@1  66.41 ( 71.07)\tAcc@5  94.53 ( 94.48)\n",
            "Epoch: [37][ 60/391]\tTime  0.222 ( 0.232)\tLoss 8.6970e-01 (9.6896e-01)\tAcc@1  75.00 ( 71.31)\tAcc@5  95.31 ( 94.10)\n",
            "Epoch: [37][ 90/391]\tTime  0.161 ( 0.230)\tLoss 1.1280e+00 (9.7920e-01)\tAcc@1  63.28 ( 71.03)\tAcc@5  92.97 ( 94.00)\n",
            "Epoch: [37][120/391]\tTime  0.234 ( 0.231)\tLoss 1.0272e+00 (9.8433e-01)\tAcc@1  70.31 ( 70.95)\tAcc@5  91.41 ( 93.92)\n",
            "Epoch: [37][150/391]\tTime  0.223 ( 0.231)\tLoss 1.0005e+00 (9.8951e-01)\tAcc@1  69.53 ( 70.76)\tAcc@5  93.75 ( 93.95)\n",
            "Epoch: [37][180/391]\tTime  0.282 ( 0.230)\tLoss 1.0823e+00 (1.0079e+00)\tAcc@1  70.31 ( 70.24)\tAcc@5  93.75 ( 93.66)\n",
            "Epoch: [37][210/391]\tTime  0.237 ( 0.229)\tLoss 8.8875e-01 (1.0161e+00)\tAcc@1  72.66 ( 70.08)\tAcc@5  95.31 ( 93.58)\n",
            "Epoch: [37][240/391]\tTime  0.176 ( 0.228)\tLoss 1.2614e+00 (1.0257e+00)\tAcc@1  65.62 ( 69.88)\tAcc@5  89.84 ( 93.42)\n",
            "Epoch: [37][270/391]\tTime  0.253 ( 0.227)\tLoss 1.1961e+00 (1.0309e+00)\tAcc@1  67.19 ( 69.70)\tAcc@5  87.50 ( 93.30)\n",
            "Epoch: [37][300/391]\tTime  0.312 ( 0.227)\tLoss 9.1272e-01 (1.0329e+00)\tAcc@1  71.09 ( 69.57)\tAcc@5  96.09 ( 93.31)\n",
            "Epoch: [37][330/391]\tTime  0.179 ( 0.227)\tLoss 1.1881e+00 (1.0426e+00)\tAcc@1  64.84 ( 69.30)\tAcc@5  91.41 ( 93.18)\n",
            "Epoch: [37][360/391]\tTime  0.245 ( 0.227)\tLoss 1.0294e+00 (1.0472e+00)\tAcc@1  65.62 ( 69.18)\tAcc@5  96.09 ( 93.10)\n",
            "Epoch: [37][390/391]\tTime  0.149 ( 0.227)\tLoss 1.0034e+00 (1.0490e+00)\tAcc@1  73.75 ( 69.13)\tAcc@5  97.50 ( 93.08)\n",
            "==> Train Accuracy: Acc@1 69.132 || Acc@5 93.078\n",
            "==> Test Accuracy:  Acc@1 58.340 || Acc@5 85.360\n",
            "==> 92.71 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 38, lr: 0.1 -----\n",
            "Epoch: [38][  0/391]\tTime  0.574 ( 0.574)\tLoss 8.5562e-01 (8.5562e-01)\tAcc@1  75.78 ( 75.78)\tAcc@5  96.09 ( 96.09)\n",
            "Epoch: [38][ 30/391]\tTime  0.334 ( 0.236)\tLoss 1.0403e+00 (9.4935e-01)\tAcc@1  67.19 ( 72.18)\tAcc@5  91.41 ( 94.18)\n",
            "Epoch: [38][ 60/391]\tTime  0.159 ( 0.231)\tLoss 1.0126e+00 (9.6433e-01)\tAcc@1  69.53 ( 71.72)\tAcc@5  94.53 ( 93.97)\n",
            "Epoch: [38][ 90/391]\tTime  0.164 ( 0.231)\tLoss 7.8697e-01 (9.6990e-01)\tAcc@1  69.53 ( 71.47)\tAcc@5  98.44 ( 93.96)\n",
            "Epoch: [38][120/391]\tTime  0.155 ( 0.230)\tLoss 1.1194e+00 (9.7727e-01)\tAcc@1  67.19 ( 71.38)\tAcc@5  90.62 ( 93.88)\n",
            "Epoch: [38][150/391]\tTime  0.213 ( 0.228)\tLoss 9.4320e-01 (9.8618e-01)\tAcc@1  71.09 ( 71.24)\tAcc@5  91.41 ( 93.84)\n",
            "Epoch: [38][180/391]\tTime  0.212 ( 0.228)\tLoss 1.0897e+00 (1.0007e+00)\tAcc@1  67.97 ( 70.70)\tAcc@5  93.75 ( 93.73)\n",
            "Epoch: [38][210/391]\tTime  0.236 ( 0.228)\tLoss 1.0427e+00 (1.0100e+00)\tAcc@1  67.97 ( 70.49)\tAcc@5  95.31 ( 93.56)\n",
            "Epoch: [38][240/391]\tTime  0.200 ( 0.227)\tLoss 1.1595e+00 (1.0177e+00)\tAcc@1  64.84 ( 70.25)\tAcc@5  92.97 ( 93.53)\n",
            "Epoch: [38][270/391]\tTime  0.250 ( 0.227)\tLoss 1.1977e+00 (1.0229e+00)\tAcc@1  62.50 ( 70.07)\tAcc@5  90.62 ( 93.42)\n",
            "Epoch: [38][300/391]\tTime  0.177 ( 0.227)\tLoss 1.2938e+00 (1.0277e+00)\tAcc@1  60.94 ( 69.92)\tAcc@5  94.53 ( 93.39)\n",
            "Epoch: [38][330/391]\tTime  0.155 ( 0.227)\tLoss 1.0717e+00 (1.0370e+00)\tAcc@1  68.75 ( 69.70)\tAcc@5  91.41 ( 93.23)\n",
            "Epoch: [38][360/391]\tTime  0.249 ( 0.227)\tLoss 1.0438e+00 (1.0454e+00)\tAcc@1  71.88 ( 69.43)\tAcc@5  90.62 ( 93.15)\n",
            "Epoch: [38][390/391]\tTime  0.148 ( 0.226)\tLoss 8.3898e-01 (1.0519e+00)\tAcc@1  77.50 ( 69.27)\tAcc@5  93.75 ( 93.04)\n",
            "==> Train Accuracy: Acc@1 69.274 || Acc@5 93.040\n",
            "==> Test Accuracy:  Acc@1 57.020 || Acc@5 85.400\n",
            "==> 92.58 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 39, lr: 0.1 -----\n",
            "Epoch: [39][  0/391]\tTime  0.566 ( 0.566)\tLoss 9.2524e-01 (9.2524e-01)\tAcc@1  72.66 ( 72.66)\tAcc@5  95.31 ( 95.31)\n",
            "Epoch: [39][ 30/391]\tTime  0.298 ( 0.239)\tLoss 7.5414e-01 (8.9122e-01)\tAcc@1  75.78 ( 74.22)\tAcc@5  97.66 ( 94.41)\n",
            "Epoch: [39][ 60/391]\tTime  0.151 ( 0.230)\tLoss 9.6642e-01 (9.1336e-01)\tAcc@1  71.88 ( 73.34)\tAcc@5  97.66 ( 94.48)\n",
            "Epoch: [39][ 90/391]\tTime  0.176 ( 0.227)\tLoss 1.0225e+00 (9.2655e-01)\tAcc@1  68.75 ( 72.81)\tAcc@5  95.31 ( 94.44)\n",
            "Epoch: [39][120/391]\tTime  0.214 ( 0.226)\tLoss 1.0477e+00 (9.4937e-01)\tAcc@1  71.88 ( 72.00)\tAcc@5  89.84 ( 94.05)\n",
            "Epoch: [39][150/391]\tTime  0.324 ( 0.228)\tLoss 8.3369e-01 (9.7296e-01)\tAcc@1  72.66 ( 71.29)\tAcc@5  92.19 ( 93.82)\n",
            "Epoch: [39][180/391]\tTime  0.296 ( 0.228)\tLoss 1.3892e+00 (9.8965e-01)\tAcc@1  59.38 ( 70.94)\tAcc@5  89.06 ( 93.54)\n",
            "Epoch: [39][210/391]\tTime  0.288 ( 0.228)\tLoss 1.0656e+00 (9.9745e-01)\tAcc@1  62.50 ( 70.70)\tAcc@5  93.75 ( 93.50)\n",
            "Epoch: [39][240/391]\tTime  0.180 ( 0.227)\tLoss 1.0828e+00 (9.9745e-01)\tAcc@1  69.53 ( 70.66)\tAcc@5  91.41 ( 93.46)\n",
            "Epoch: [39][270/391]\tTime  0.174 ( 0.228)\tLoss 9.9358e-01 (1.0062e+00)\tAcc@1  71.88 ( 70.51)\tAcc@5  89.84 ( 93.36)\n",
            "Epoch: [39][300/391]\tTime  0.167 ( 0.228)\tLoss 1.2050e+00 (1.0157e+00)\tAcc@1  67.97 ( 70.26)\tAcc@5  92.19 ( 93.24)\n",
            "Epoch: [39][330/391]\tTime  0.185 ( 0.228)\tLoss 8.7802e-01 (1.0213e+00)\tAcc@1  77.34 ( 70.11)\tAcc@5  94.53 ( 93.17)\n",
            "Epoch: [39][360/391]\tTime  0.181 ( 0.228)\tLoss 1.1408e+00 (1.0267e+00)\tAcc@1  64.06 ( 69.88)\tAcc@5  94.53 ( 93.15)\n",
            "Epoch: [39][390/391]\tTime  0.148 ( 0.227)\tLoss 1.0548e+00 (1.0349e+00)\tAcc@1  71.25 ( 69.68)\tAcc@5  93.75 ( 93.05)\n",
            "==> Train Accuracy: Acc@1 69.682 || Acc@5 93.046\n",
            "==> Test Accuracy:  Acc@1 58.610 || Acc@5 84.850\n",
            "==> 93.05 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 40, lr: 0.1 -----\n",
            "Epoch: [40][  0/391]\tTime  0.565 ( 0.565)\tLoss 6.9338e-01 (6.9338e-01)\tAcc@1  77.34 ( 77.34)\tAcc@5  97.66 ( 97.66)\n",
            "Epoch: [40][ 30/391]\tTime  0.166 ( 0.239)\tLoss 1.1193e+00 (9.2414e-01)\tAcc@1  65.62 ( 72.35)\tAcc@5  93.75 ( 94.28)\n",
            "Epoch: [40][ 60/391]\tTime  0.168 ( 0.232)\tLoss 8.4616e-01 (9.1921e-01)\tAcc@1  75.78 ( 72.46)\tAcc@5  94.53 ( 94.36)\n",
            "Epoch: [40][ 90/391]\tTime  0.164 ( 0.228)\tLoss 1.1923e+00 (9.4176e-01)\tAcc@1  65.62 ( 71.95)\tAcc@5  92.19 ( 94.14)\n",
            "Epoch: [40][120/391]\tTime  0.279 ( 0.228)\tLoss 1.0714e+00 (9.6144e-01)\tAcc@1  67.97 ( 71.48)\tAcc@5  94.53 ( 93.85)\n",
            "Epoch: [40][150/391]\tTime  0.266 ( 0.228)\tLoss 8.8812e-01 (9.7469e-01)\tAcc@1  71.09 ( 71.13)\tAcc@5  94.53 ( 93.81)\n",
            "Epoch: [40][180/391]\tTime  0.160 ( 0.228)\tLoss 1.0110e+00 (9.8433e-01)\tAcc@1  68.75 ( 70.99)\tAcc@5  95.31 ( 93.69)\n",
            "Epoch: [40][210/391]\tTime  0.170 ( 0.228)\tLoss 1.0472e+00 (9.9714e-01)\tAcc@1  67.97 ( 70.66)\tAcc@5  94.53 ( 93.58)\n",
            "Epoch: [40][240/391]\tTime  0.171 ( 0.228)\tLoss 8.8354e-01 (9.9725e-01)\tAcc@1  73.44 ( 70.60)\tAcc@5  96.09 ( 93.58)\n",
            "Epoch: [40][270/391]\tTime  0.354 ( 0.228)\tLoss 1.1117e+00 (1.0056e+00)\tAcc@1  65.62 ( 70.42)\tAcc@5  94.53 ( 93.39)\n",
            "Epoch: [40][300/391]\tTime  0.284 ( 0.228)\tLoss 1.0079e+00 (1.0138e+00)\tAcc@1  72.66 ( 70.25)\tAcc@5  92.97 ( 93.31)\n",
            "Epoch: [40][330/391]\tTime  0.319 ( 0.228)\tLoss 1.1478e+00 (1.0227e+00)\tAcc@1  67.97 ( 70.00)\tAcc@5  90.62 ( 93.15)\n",
            "Epoch: [40][360/391]\tTime  0.279 ( 0.228)\tLoss 1.2783e+00 (1.0289e+00)\tAcc@1  64.06 ( 69.78)\tAcc@5  91.41 ( 93.07)\n",
            "Epoch: [40][390/391]\tTime  0.147 ( 0.228)\tLoss 9.3436e-01 (1.0402e+00)\tAcc@1  80.00 ( 69.49)\tAcc@5  92.50 ( 92.89)\n",
            "==> Train Accuracy: Acc@1 69.494 || Acc@5 92.888\n",
            "==> Test Accuracy:  Acc@1 55.280 || Acc@5 84.740\n",
            "==> 93.15 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 41, lr: 0.1 -----\n",
            "Epoch: [41][  0/391]\tTime  0.585 ( 0.585)\tLoss 1.0684e+00 (1.0684e+00)\tAcc@1  68.75 ( 68.75)\tAcc@5  91.41 ( 91.41)\n",
            "Epoch: [41][ 30/391]\tTime  0.190 ( 0.234)\tLoss 8.4909e-01 (9.2401e-01)\tAcc@1  75.78 ( 72.23)\tAcc@5  96.09 ( 94.51)\n",
            "Epoch: [41][ 60/391]\tTime  0.164 ( 0.234)\tLoss 9.2260e-01 (9.4496e-01)\tAcc@1  71.09 ( 72.07)\tAcc@5  95.31 ( 94.02)\n",
            "Epoch: [41][ 90/391]\tTime  0.307 ( 0.231)\tLoss 7.9967e-01 (9.5300e-01)\tAcc@1  75.78 ( 71.92)\tAcc@5  94.53 ( 93.84)\n",
            "Epoch: [41][120/391]\tTime  0.357 ( 0.231)\tLoss 1.0146e+00 (9.5836e-01)\tAcc@1  71.09 ( 71.77)\tAcc@5  94.53 ( 93.83)\n",
            "Epoch: [41][150/391]\tTime  0.297 ( 0.231)\tLoss 8.2187e-01 (9.6350e-01)\tAcc@1  77.34 ( 71.62)\tAcc@5  96.88 ( 93.83)\n",
            "Epoch: [41][180/391]\tTime  0.271 ( 0.230)\tLoss 1.0938e+00 (9.7825e-01)\tAcc@1  70.31 ( 71.26)\tAcc@5  92.97 ( 93.66)\n",
            "Epoch: [41][210/391]\tTime  0.258 ( 0.228)\tLoss 9.2268e-01 (9.9307e-01)\tAcc@1  75.78 ( 70.82)\tAcc@5  93.75 ( 93.52)\n",
            "Epoch: [41][240/391]\tTime  0.275 ( 0.229)\tLoss 1.2231e+00 (1.0017e+00)\tAcc@1  63.28 ( 70.51)\tAcc@5  90.62 ( 93.49)\n",
            "Epoch: [41][270/391]\tTime  0.181 ( 0.228)\tLoss 1.4132e+00 (1.0071e+00)\tAcc@1  63.28 ( 70.49)\tAcc@5  85.16 ( 93.36)\n",
            "Epoch: [41][300/391]\tTime  0.253 ( 0.228)\tLoss 9.6201e-01 (1.0157e+00)\tAcc@1  76.56 ( 70.26)\tAcc@5  89.84 ( 93.24)\n",
            "Epoch: [41][330/391]\tTime  0.297 ( 0.228)\tLoss 9.2940e-01 (1.0178e+00)\tAcc@1  68.75 ( 70.23)\tAcc@5  96.88 ( 93.22)\n",
            "Epoch: [41][360/391]\tTime  0.246 ( 0.228)\tLoss 1.1661e+00 (1.0268e+00)\tAcc@1  67.19 ( 69.97)\tAcc@5  92.97 ( 93.13)\n",
            "Epoch: [41][390/391]\tTime  0.138 ( 0.227)\tLoss 1.2526e+00 (1.0341e+00)\tAcc@1  58.75 ( 69.75)\tAcc@5  92.50 ( 93.08)\n",
            "==> Train Accuracy: Acc@1 69.746 || Acc@5 93.080\n",
            "==> Test Accuracy:  Acc@1 58.800 || Acc@5 86.010\n",
            "==> 92.96 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 42, lr: 0.1 -----\n",
            "Epoch: [42][  0/391]\tTime  0.576 ( 0.576)\tLoss 8.7947e-01 (8.7947e-01)\tAcc@1  72.66 ( 72.66)\tAcc@5  96.88 ( 96.88)\n",
            "Epoch: [42][ 30/391]\tTime  0.169 ( 0.230)\tLoss 8.7672e-01 (9.4666e-01)\tAcc@1  73.44 ( 72.28)\tAcc@5  94.53 ( 94.00)\n",
            "Epoch: [42][ 60/391]\tTime  0.176 ( 0.226)\tLoss 1.0771e+00 (9.3669e-01)\tAcc@1  66.41 ( 72.55)\tAcc@5  92.19 ( 93.98)\n",
            "Epoch: [42][ 90/391]\tTime  0.289 ( 0.226)\tLoss 1.0394e+00 (9.4097e-01)\tAcc@1  67.97 ( 72.16)\tAcc@5  90.62 ( 94.08)\n",
            "Epoch: [42][120/391]\tTime  0.164 ( 0.224)\tLoss 9.0480e-01 (9.5601e-01)\tAcc@1  71.88 ( 71.74)\tAcc@5  93.75 ( 93.90)\n",
            "Epoch: [42][150/391]\tTime  0.166 ( 0.226)\tLoss 9.6704e-01 (9.7435e-01)\tAcc@1  71.88 ( 71.16)\tAcc@5  92.97 ( 93.72)\n",
            "Epoch: [42][180/391]\tTime  0.182 ( 0.227)\tLoss 1.0488e+00 (9.8606e-01)\tAcc@1  71.09 ( 70.83)\tAcc@5  91.41 ( 93.53)\n",
            "Epoch: [42][210/391]\tTime  0.174 ( 0.226)\tLoss 1.0481e+00 (9.9336e-01)\tAcc@1  75.78 ( 70.62)\tAcc@5  89.84 ( 93.51)\n",
            "Epoch: [42][240/391]\tTime  0.296 ( 0.226)\tLoss 1.3776e+00 (1.0005e+00)\tAcc@1  65.62 ( 70.44)\tAcc@5  91.41 ( 93.44)\n",
            "Epoch: [42][270/391]\tTime  0.291 ( 0.226)\tLoss 1.0128e+00 (1.0023e+00)\tAcc@1  69.53 ( 70.35)\tAcc@5  94.53 ( 93.46)\n",
            "Epoch: [42][300/391]\tTime  0.267 ( 0.227)\tLoss 9.7385e-01 (1.0086e+00)\tAcc@1  69.53 ( 70.21)\tAcc@5  95.31 ( 93.38)\n",
            "Epoch: [42][330/391]\tTime  0.275 ( 0.226)\tLoss 1.1699e+00 (1.0176e+00)\tAcc@1  67.19 ( 69.97)\tAcc@5  89.84 ( 93.27)\n",
            "Epoch: [42][360/391]\tTime  0.312 ( 0.227)\tLoss 9.7656e-01 (1.0225e+00)\tAcc@1  67.97 ( 69.86)\tAcc@5  93.75 ( 93.23)\n",
            "Epoch: [42][390/391]\tTime  0.146 ( 0.226)\tLoss 1.2483e+00 (1.0275e+00)\tAcc@1  58.75 ( 69.71)\tAcc@5  91.25 ( 93.12)\n",
            "==> Train Accuracy: Acc@1 69.706 || Acc@5 93.124\n",
            "==> Test Accuracy:  Acc@1 58.570 || Acc@5 85.580\n",
            "==> 92.54 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 43, lr: 0.1 -----\n",
            "Epoch: [43][  0/391]\tTime  0.561 ( 0.561)\tLoss 1.0676e+00 (1.0676e+00)\tAcc@1  67.97 ( 67.97)\tAcc@5  92.97 ( 92.97)\n",
            "Epoch: [43][ 30/391]\tTime  0.312 ( 0.236)\tLoss 9.3412e-01 (9.3072e-01)\tAcc@1  71.09 ( 73.11)\tAcc@5  94.53 ( 94.30)\n",
            "Epoch: [43][ 60/391]\tTime  0.153 ( 0.228)\tLoss 9.0995e-01 (9.3073e-01)\tAcc@1  72.66 ( 72.49)\tAcc@5  93.75 ( 94.39)\n",
            "Epoch: [43][ 90/391]\tTime  0.168 ( 0.226)\tLoss 1.1071e+00 (9.5547e-01)\tAcc@1  69.53 ( 71.84)\tAcc@5  94.53 ( 94.22)\n",
            "Epoch: [43][120/391]\tTime  0.179 ( 0.228)\tLoss 8.9788e-01 (9.5727e-01)\tAcc@1  70.31 ( 71.66)\tAcc@5  95.31 ( 94.18)\n",
            "Epoch: [43][150/391]\tTime  0.173 ( 0.226)\tLoss 1.0418e+00 (9.6702e-01)\tAcc@1  68.75 ( 71.33)\tAcc@5  91.41 ( 94.07)\n",
            "Epoch: [43][180/391]\tTime  0.165 ( 0.225)\tLoss 9.6170e-01 (9.7949e-01)\tAcc@1  75.00 ( 71.08)\tAcc@5  90.62 ( 93.80)\n",
            "Epoch: [43][210/391]\tTime  0.182 ( 0.224)\tLoss 9.9228e-01 (9.8741e-01)\tAcc@1  72.66 ( 70.88)\tAcc@5  92.19 ( 93.74)\n",
            "Epoch: [43][240/391]\tTime  0.181 ( 0.226)\tLoss 1.0271e+00 (9.9574e-01)\tAcc@1  67.97 ( 70.67)\tAcc@5  94.53 ( 93.62)\n",
            "Epoch: [43][270/391]\tTime  0.172 ( 0.225)\tLoss 1.0107e+00 (1.0012e+00)\tAcc@1  71.09 ( 70.62)\tAcc@5  92.97 ( 93.50)\n",
            "Epoch: [43][300/391]\tTime  0.165 ( 0.225)\tLoss 1.2580e+00 (1.0094e+00)\tAcc@1  64.84 ( 70.45)\tAcc@5  90.62 ( 93.41)\n",
            "Epoch: [43][330/391]\tTime  0.177 ( 0.225)\tLoss 1.0979e+00 (1.0150e+00)\tAcc@1  67.19 ( 70.31)\tAcc@5  93.75 ( 93.32)\n",
            "Epoch: [43][360/391]\tTime  0.264 ( 0.225)\tLoss 8.4491e-01 (1.0232e+00)\tAcc@1  73.44 ( 70.07)\tAcc@5  95.31 ( 93.21)\n",
            "Epoch: [43][390/391]\tTime  0.148 ( 0.225)\tLoss 1.1128e+00 (1.0234e+00)\tAcc@1  65.00 ( 70.11)\tAcc@5  95.00 ( 93.22)\n",
            "==> Train Accuracy: Acc@1 70.106 || Acc@5 93.222\n",
            "==> Test Accuracy:  Acc@1 59.800 || Acc@5 86.530\n",
            "==> 92.08 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 44, lr: 0.1 -----\n",
            "Epoch: [44][  0/391]\tTime  0.582 ( 0.582)\tLoss 9.5123e-01 (9.5123e-01)\tAcc@1  72.66 ( 72.66)\tAcc@5  92.97 ( 92.97)\n",
            "Epoch: [44][ 30/391]\tTime  0.164 ( 0.236)\tLoss 9.0182e-01 (9.3710e-01)\tAcc@1  71.09 ( 72.18)\tAcc@5  96.09 ( 94.28)\n",
            "Epoch: [44][ 60/391]\tTime  0.287 ( 0.234)\tLoss 8.7053e-01 (9.5248e-01)\tAcc@1  73.44 ( 71.53)\tAcc@5  96.88 ( 94.33)\n",
            "Epoch: [44][ 90/391]\tTime  0.270 ( 0.233)\tLoss 1.0016e+00 (9.6298e-01)\tAcc@1  67.97 ( 71.47)\tAcc@5  96.09 ( 94.33)\n",
            "Epoch: [44][120/391]\tTime  0.287 ( 0.232)\tLoss 9.8093e-01 (9.5765e-01)\tAcc@1  73.44 ( 71.67)\tAcc@5  95.31 ( 94.32)\n",
            "Epoch: [44][150/391]\tTime  0.170 ( 0.232)\tLoss 9.7576e-01 (9.7374e-01)\tAcc@1  68.75 ( 71.29)\tAcc@5  94.53 ( 94.13)\n",
            "Epoch: [44][180/391]\tTime  0.162 ( 0.231)\tLoss 8.5630e-01 (9.8539e-01)\tAcc@1  75.78 ( 70.96)\tAcc@5  94.53 ( 93.92)\n",
            "Epoch: [44][210/391]\tTime  0.184 ( 0.231)\tLoss 9.4994e-01 (9.9103e-01)\tAcc@1  67.97 ( 70.82)\tAcc@5  96.09 ( 93.86)\n",
            "Epoch: [44][240/391]\tTime  0.179 ( 0.230)\tLoss 9.7042e-01 (9.9947e-01)\tAcc@1  71.88 ( 70.70)\tAcc@5  95.31 ( 93.71)\n",
            "Epoch: [44][270/391]\tTime  0.178 ( 0.230)\tLoss 1.0055e+00 (1.0060e+00)\tAcc@1  67.97 ( 70.51)\tAcc@5  90.62 ( 93.59)\n",
            "Epoch: [44][300/391]\tTime  0.168 ( 0.230)\tLoss 1.0643e+00 (1.0096e+00)\tAcc@1  65.62 ( 70.41)\tAcc@5  92.19 ( 93.57)\n",
            "Epoch: [44][330/391]\tTime  0.173 ( 0.231)\tLoss 1.3916e+00 (1.0111e+00)\tAcc@1  57.81 ( 70.31)\tAcc@5  89.06 ( 93.52)\n",
            "Epoch: [44][360/391]\tTime  0.265 ( 0.230)\tLoss 1.0513e+00 (1.0175e+00)\tAcc@1  67.97 ( 70.16)\tAcc@5  93.75 ( 93.43)\n",
            "Epoch: [44][390/391]\tTime  0.147 ( 0.229)\tLoss 9.8461e-01 (1.0252e+00)\tAcc@1  68.75 ( 69.96)\tAcc@5  93.75 ( 93.34)\n",
            "==> Train Accuracy: Acc@1 69.964 || Acc@5 93.344\n",
            "==> Test Accuracy:  Acc@1 56.700 || Acc@5 83.850\n",
            "==> 93.68 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 45, lr: 0.1 -----\n",
            "Epoch: [45][  0/391]\tTime  0.555 ( 0.555)\tLoss 1.0372e+00 (1.0372e+00)\tAcc@1  70.31 ( 70.31)\tAcc@5  92.97 ( 92.97)\n",
            "Epoch: [45][ 30/391]\tTime  0.183 ( 0.235)\tLoss 1.1623e+00 (8.8500e-01)\tAcc@1  64.84 ( 73.56)\tAcc@5  90.62 ( 94.56)\n",
            "Epoch: [45][ 60/391]\tTime  0.245 ( 0.231)\tLoss 9.1391e-01 (9.0002e-01)\tAcc@1  72.66 ( 73.25)\tAcc@5  94.53 ( 94.68)\n",
            "Epoch: [45][ 90/391]\tTime  0.323 ( 0.228)\tLoss 9.3940e-01 (9.1524e-01)\tAcc@1  72.66 ( 72.77)\tAcc@5  92.97 ( 94.59)\n",
            "Epoch: [45][120/391]\tTime  0.172 ( 0.226)\tLoss 9.9253e-01 (9.4135e-01)\tAcc@1  71.09 ( 71.99)\tAcc@5  95.31 ( 94.41)\n",
            "Epoch: [45][150/391]\tTime  0.168 ( 0.227)\tLoss 1.0777e+00 (9.5501e-01)\tAcc@1  67.19 ( 71.68)\tAcc@5  90.62 ( 94.18)\n",
            "Epoch: [45][180/391]\tTime  0.176 ( 0.228)\tLoss 1.0664e+00 (9.6752e-01)\tAcc@1  73.44 ( 71.51)\tAcc@5  92.97 ( 94.01)\n",
            "Epoch: [45][210/391]\tTime  0.241 ( 0.227)\tLoss 9.7714e-01 (9.7794e-01)\tAcc@1  74.22 ( 71.25)\tAcc@5  91.41 ( 93.86)\n",
            "Epoch: [45][240/391]\tTime  0.169 ( 0.227)\tLoss 9.8874e-01 (9.8247e-01)\tAcc@1  70.31 ( 71.13)\tAcc@5  93.75 ( 93.81)\n",
            "Epoch: [45][270/391]\tTime  0.157 ( 0.228)\tLoss 1.2254e+00 (9.8902e-01)\tAcc@1  64.84 ( 70.99)\tAcc@5  86.72 ( 93.74)\n",
            "Epoch: [45][300/391]\tTime  0.177 ( 0.228)\tLoss 1.1856e+00 (9.9515e-01)\tAcc@1  64.84 ( 70.84)\tAcc@5  89.06 ( 93.62)\n",
            "Epoch: [45][330/391]\tTime  0.159 ( 0.228)\tLoss 1.2614e+00 (1.0013e+00)\tAcc@1  63.28 ( 70.71)\tAcc@5  90.62 ( 93.51)\n",
            "Epoch: [45][360/391]\tTime  0.193 ( 0.228)\tLoss 9.8177e-01 (1.0080e+00)\tAcc@1  70.31 ( 70.52)\tAcc@5  93.75 ( 93.43)\n",
            "Epoch: [45][390/391]\tTime  0.149 ( 0.227)\tLoss 1.4586e+00 (1.0170e+00)\tAcc@1  56.25 ( 70.31)\tAcc@5  86.25 ( 93.30)\n",
            "==> Train Accuracy: Acc@1 70.310 || Acc@5 93.298\n",
            "==> Test Accuracy:  Acc@1 54.980 || Acc@5 84.050\n",
            "==> 92.83 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 46, lr: 0.1 -----\n",
            "Epoch: [46][  0/391]\tTime  0.566 ( 0.566)\tLoss 9.4068e-01 (9.4068e-01)\tAcc@1  74.22 ( 74.22)\tAcc@5  92.97 ( 92.97)\n",
            "Epoch: [46][ 30/391]\tTime  0.294 ( 0.235)\tLoss 1.0806e+00 (9.5679e-01)\tAcc@1  67.19 ( 72.13)\tAcc@5  92.19 ( 94.41)\n",
            "Epoch: [46][ 60/391]\tTime  0.174 ( 0.232)\tLoss 8.0265e-01 (9.3427e-01)\tAcc@1  78.12 ( 72.50)\tAcc@5  94.53 ( 94.26)\n",
            "Epoch: [46][ 90/391]\tTime  0.169 ( 0.231)\tLoss 9.2618e-01 (9.4948e-01)\tAcc@1  73.44 ( 71.94)\tAcc@5  93.75 ( 94.34)\n",
            "Epoch: [46][120/391]\tTime  0.170 ( 0.230)\tLoss 8.4876e-01 (9.5136e-01)\tAcc@1  71.88 ( 71.80)\tAcc@5  96.88 ( 94.28)\n",
            "Epoch: [46][150/391]\tTime  0.315 ( 0.232)\tLoss 9.0853e-01 (9.7100e-01)\tAcc@1  71.09 ( 71.36)\tAcc@5  92.97 ( 94.00)\n",
            "Epoch: [46][180/391]\tTime  0.351 ( 0.233)\tLoss 8.6363e-01 (9.7141e-01)\tAcc@1  72.66 ( 71.42)\tAcc@5  92.19 ( 93.93)\n",
            "Epoch: [46][210/391]\tTime  0.259 ( 0.233)\tLoss 1.2298e+00 (9.7845e-01)\tAcc@1  66.41 ( 71.11)\tAcc@5  92.97 ( 93.88)\n",
            "Epoch: [46][240/391]\tTime  0.327 ( 0.232)\tLoss 1.0984e+00 (9.8452e-01)\tAcc@1  65.62 ( 70.79)\tAcc@5  93.75 ( 93.86)\n",
            "Epoch: [46][270/391]\tTime  0.317 ( 0.232)\tLoss 9.4559e-01 (9.9571e-01)\tAcc@1  73.44 ( 70.51)\tAcc@5  93.75 ( 93.67)\n",
            "Epoch: [46][300/391]\tTime  0.292 ( 0.233)\tLoss 1.0987e+00 (1.0074e+00)\tAcc@1  69.53 ( 70.32)\tAcc@5  92.97 ( 93.48)\n",
            "Epoch: [46][330/391]\tTime  0.324 ( 0.233)\tLoss 1.0550e+00 (1.0093e+00)\tAcc@1  71.09 ( 70.31)\tAcc@5  92.97 ( 93.44)\n",
            "Epoch: [46][360/391]\tTime  0.228 ( 0.233)\tLoss 1.0111e+00 (1.0125e+00)\tAcc@1  68.75 ( 70.19)\tAcc@5  93.75 ( 93.40)\n",
            "Epoch: [46][390/391]\tTime  0.141 ( 0.232)\tLoss 1.2191e+00 (1.0179e+00)\tAcc@1  71.25 ( 70.10)\tAcc@5  92.50 ( 93.36)\n",
            "==> Train Accuracy: Acc@1 70.104 || Acc@5 93.362\n",
            "==> Test Accuracy:  Acc@1 58.110 || Acc@5 85.410\n",
            "==> 94.91 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 47, lr: 0.1 -----\n",
            "Epoch: [47][  0/391]\tTime  0.593 ( 0.593)\tLoss 1.0543e+00 (1.0543e+00)\tAcc@1  65.62 ( 65.62)\tAcc@5  92.97 ( 92.97)\n",
            "Epoch: [47][ 30/391]\tTime  0.162 ( 0.238)\tLoss 8.9689e-01 (9.1973e-01)\tAcc@1  70.31 ( 72.56)\tAcc@5  96.09 ( 94.63)\n",
            "Epoch: [47][ 60/391]\tTime  0.176 ( 0.238)\tLoss 9.1939e-01 (9.0481e-01)\tAcc@1  72.66 ( 73.23)\tAcc@5  92.97 ( 94.53)\n",
            "Epoch: [47][ 90/391]\tTime  0.171 ( 0.236)\tLoss 8.7444e-01 (9.1349e-01)\tAcc@1  72.66 ( 73.18)\tAcc@5  92.19 ( 94.33)\n",
            "Epoch: [47][120/391]\tTime  0.259 ( 0.234)\tLoss 1.0587e+00 (9.2754e-01)\tAcc@1  69.53 ( 72.63)\tAcc@5  92.97 ( 94.23)\n",
            "Epoch: [47][150/391]\tTime  0.298 ( 0.233)\tLoss 1.0726e+00 (9.4144e-01)\tAcc@1  67.19 ( 72.29)\tAcc@5  92.97 ( 94.03)\n",
            "Epoch: [47][180/391]\tTime  0.165 ( 0.232)\tLoss 1.0105e+00 (9.6074e-01)\tAcc@1  71.09 ( 71.76)\tAcc@5  91.41 ( 93.78)\n",
            "Epoch: [47][210/391]\tTime  0.289 ( 0.233)\tLoss 9.4914e-01 (9.7138e-01)\tAcc@1  71.09 ( 71.50)\tAcc@5  96.88 ( 93.64)\n",
            "Epoch: [47][240/391]\tTime  0.296 ( 0.234)\tLoss 1.0404e+00 (9.7882e-01)\tAcc@1  64.06 ( 71.28)\tAcc@5  94.53 ( 93.59)\n",
            "Epoch: [47][270/391]\tTime  0.209 ( 0.233)\tLoss 7.4718e-01 (9.8200e-01)\tAcc@1  77.34 ( 71.18)\tAcc@5  96.09 ( 93.57)\n",
            "Epoch: [47][300/391]\tTime  0.225 ( 0.232)\tLoss 9.8063e-01 (9.8404e-01)\tAcc@1  71.88 ( 71.19)\tAcc@5  95.31 ( 93.58)\n",
            "Epoch: [47][330/391]\tTime  0.152 ( 0.231)\tLoss 1.0997e+00 (9.9049e-01)\tAcc@1  66.41 ( 70.96)\tAcc@5  93.75 ( 93.52)\n",
            "Epoch: [47][360/391]\tTime  0.152 ( 0.231)\tLoss 1.0713e+00 (1.0029e+00)\tAcc@1  71.09 ( 70.65)\tAcc@5  90.62 ( 93.38)\n",
            "Epoch: [47][390/391]\tTime  0.149 ( 0.231)\tLoss 1.1835e+00 (1.0091e+00)\tAcc@1  62.50 ( 70.46)\tAcc@5  92.50 ( 93.32)\n",
            "==> Train Accuracy: Acc@1 70.464 || Acc@5 93.316\n",
            "==> Test Accuracy:  Acc@1 58.590 || Acc@5 85.870\n",
            "==> 94.33 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 48, lr: 0.1 -----\n",
            "Epoch: [48][  0/391]\tTime  0.556 ( 0.556)\tLoss 9.3786e-01 (9.3786e-01)\tAcc@1  75.78 ( 75.78)\tAcc@5  95.31 ( 95.31)\n",
            "Epoch: [48][ 30/391]\tTime  0.168 ( 0.236)\tLoss 8.7506e-01 (8.9486e-01)\tAcc@1  71.09 ( 74.14)\tAcc@5  96.09 ( 94.96)\n",
            "Epoch: [48][ 60/391]\tTime  0.163 ( 0.232)\tLoss 1.1092e+00 (8.9327e-01)\tAcc@1  65.62 ( 73.66)\tAcc@5  91.41 ( 94.93)\n",
            "Epoch: [48][ 90/391]\tTime  0.178 ( 0.234)\tLoss 1.1542e+00 (9.1549e-01)\tAcc@1  64.06 ( 73.03)\tAcc@5  90.62 ( 94.67)\n",
            "Epoch: [48][120/391]\tTime  0.162 ( 0.233)\tLoss 1.0512e+00 (9.3480e-01)\tAcc@1  67.97 ( 72.57)\tAcc@5  92.97 ( 94.38)\n",
            "Epoch: [48][150/391]\tTime  0.173 ( 0.234)\tLoss 1.2019e+00 (9.4448e-01)\tAcc@1  64.84 ( 72.18)\tAcc@5  89.84 ( 94.25)\n",
            "Epoch: [48][180/391]\tTime  0.157 ( 0.233)\tLoss 9.2321e-01 (9.5176e-01)\tAcc@1  75.00 ( 71.88)\tAcc@5  94.53 ( 94.17)\n",
            "Epoch: [48][210/391]\tTime  0.166 ( 0.232)\tLoss 1.0360e+00 (9.6114e-01)\tAcc@1  71.09 ( 71.70)\tAcc@5  92.19 ( 93.97)\n",
            "Epoch: [48][240/391]\tTime  0.180 ( 0.232)\tLoss 9.0436e-01 (9.7432e-01)\tAcc@1  73.44 ( 71.33)\tAcc@5  92.19 ( 93.82)\n",
            "Epoch: [48][270/391]\tTime  0.181 ( 0.231)\tLoss 9.6731e-01 (9.7835e-01)\tAcc@1  68.75 ( 71.25)\tAcc@5  94.53 ( 93.74)\n",
            "Epoch: [48][300/391]\tTime  0.179 ( 0.230)\tLoss 1.1010e+00 (9.8207e-01)\tAcc@1  68.75 ( 71.14)\tAcc@5  92.97 ( 93.71)\n",
            "Epoch: [48][330/391]\tTime  0.305 ( 0.231)\tLoss 1.2532e+00 (9.8864e-01)\tAcc@1  60.16 ( 70.91)\tAcc@5  93.75 ( 93.58)\n",
            "Epoch: [48][360/391]\tTime  0.166 ( 0.230)\tLoss 1.0150e+00 (9.9409e-01)\tAcc@1  72.66 ( 70.75)\tAcc@5  92.97 ( 93.52)\n",
            "Epoch: [48][390/391]\tTime  0.147 ( 0.231)\tLoss 1.1035e+00 (9.9603e-01)\tAcc@1  67.50 ( 70.66)\tAcc@5  93.75 ( 93.51)\n",
            "==> Train Accuracy: Acc@1 70.662 || Acc@5 93.510\n",
            "==> Test Accuracy:  Acc@1 56.680 || Acc@5 84.640\n",
            "==> 94.36 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 49, lr: 0.1 -----\n",
            "Epoch: [49][  0/391]\tTime  0.573 ( 0.573)\tLoss 8.1274e-01 (8.1274e-01)\tAcc@1  74.22 ( 74.22)\tAcc@5  95.31 ( 95.31)\n",
            "Epoch: [49][ 30/391]\tTime  0.254 ( 0.242)\tLoss 9.8014e-01 (8.9901e-01)\tAcc@1  67.97 ( 74.04)\tAcc@5  92.19 ( 94.73)\n",
            "Epoch: [49][ 60/391]\tTime  0.160 ( 0.233)\tLoss 1.0048e+00 (9.1942e-01)\tAcc@1  69.53 ( 73.14)\tAcc@5  91.41 ( 94.72)\n",
            "Epoch: [49][ 90/391]\tTime  0.351 ( 0.233)\tLoss 1.0228e+00 (9.3631e-01)\tAcc@1  68.75 ( 72.43)\tAcc@5  93.75 ( 94.54)\n",
            "Epoch: [49][120/391]\tTime  0.163 ( 0.230)\tLoss 1.0838e+00 (9.4470e-01)\tAcc@1  69.53 ( 72.33)\tAcc@5  92.19 ( 94.41)\n",
            "Epoch: [49][150/391]\tTime  0.337 ( 0.232)\tLoss 8.9026e-01 (9.5397e-01)\tAcc@1  72.66 ( 71.95)\tAcc@5  96.88 ( 94.35)\n",
            "Epoch: [49][180/391]\tTime  0.269 ( 0.230)\tLoss 1.1449e+00 (9.6599e-01)\tAcc@1  66.41 ( 71.72)\tAcc@5  92.19 ( 94.15)\n",
            "Epoch: [49][210/391]\tTime  0.166 ( 0.230)\tLoss 1.1909e+00 (9.7639e-01)\tAcc@1  66.41 ( 71.33)\tAcc@5  90.62 ( 94.01)\n",
            "Epoch: [49][240/391]\tTime  0.244 ( 0.230)\tLoss 1.0246e+00 (9.8201e-01)\tAcc@1  71.09 ( 71.10)\tAcc@5  93.75 ( 93.95)\n",
            "Epoch: [49][270/391]\tTime  0.269 ( 0.231)\tLoss 9.4373e-01 (9.8421e-01)\tAcc@1  75.00 ( 71.09)\tAcc@5  91.41 ( 93.90)\n",
            "Epoch: [49][300/391]\tTime  0.214 ( 0.229)\tLoss 1.0823e+00 (9.9395e-01)\tAcc@1  67.19 ( 70.81)\tAcc@5  94.53 ( 93.80)\n",
            "Epoch: [49][330/391]\tTime  0.261 ( 0.229)\tLoss 1.2294e+00 (9.9949e-01)\tAcc@1  63.28 ( 70.63)\tAcc@5  93.75 ( 93.71)\n",
            "Epoch: [49][360/391]\tTime  0.290 ( 0.228)\tLoss 1.1412e+00 (1.0014e+00)\tAcc@1  64.06 ( 70.54)\tAcc@5  92.19 ( 93.70)\n",
            "Epoch: [49][390/391]\tTime  0.148 ( 0.228)\tLoss 8.8647e-01 (1.0039e+00)\tAcc@1  71.25 ( 70.47)\tAcc@5  95.00 ( 93.68)\n",
            "==> Train Accuracy: Acc@1 70.468 || Acc@5 93.676\n",
            "==> Test Accuracy:  Acc@1 59.590 || Acc@5 86.300\n",
            "==> 93.08 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 50, lr: 0.1 -----\n",
            "Epoch: [50][  0/391]\tTime  0.567 ( 0.567)\tLoss 7.5346e-01 (7.5346e-01)\tAcc@1  75.00 ( 75.00)\tAcc@5  96.09 ( 96.09)\n",
            "Epoch: [50][ 30/391]\tTime  0.246 ( 0.233)\tLoss 9.8784e-01 (8.5912e-01)\tAcc@1  72.66 ( 73.59)\tAcc@5  93.75 ( 95.36)\n",
            "Epoch: [50][ 60/391]\tTime  0.324 ( 0.232)\tLoss 8.3181e-01 (8.9373e-01)\tAcc@1  75.00 ( 73.13)\tAcc@5  95.31 ( 94.86)\n",
            "Epoch: [50][ 90/391]\tTime  0.289 ( 0.230)\tLoss 8.1580e-01 (9.0073e-01)\tAcc@1  73.44 ( 73.04)\tAcc@5  96.88 ( 94.76)\n",
            "Epoch: [50][120/391]\tTime  0.249 ( 0.230)\tLoss 1.3676e+00 (9.1790e-01)\tAcc@1  62.50 ( 72.70)\tAcc@5  92.97 ( 94.54)\n",
            "Epoch: [50][150/391]\tTime  0.323 ( 0.231)\tLoss 8.5201e-01 (9.3744e-01)\tAcc@1  76.56 ( 72.19)\tAcc@5  95.31 ( 94.28)\n",
            "Epoch: [50][180/391]\tTime  0.326 ( 0.230)\tLoss 8.8873e-01 (9.4055e-01)\tAcc@1  76.56 ( 72.10)\tAcc@5  94.53 ( 94.25)\n",
            "Epoch: [50][210/391]\tTime  0.268 ( 0.231)\tLoss 1.0285e+00 (9.5248e-01)\tAcc@1  64.84 ( 71.67)\tAcc@5  95.31 ( 94.23)\n",
            "Epoch: [50][240/391]\tTime  0.182 ( 0.230)\tLoss 8.7577e-01 (9.6277e-01)\tAcc@1  74.22 ( 71.47)\tAcc@5  93.75 ( 94.07)\n",
            "Epoch: [50][270/391]\tTime  0.158 ( 0.230)\tLoss 1.0813e+00 (9.7136e-01)\tAcc@1  71.88 ( 71.21)\tAcc@5  94.53 ( 94.00)\n",
            "Epoch: [50][300/391]\tTime  0.251 ( 0.230)\tLoss 1.0832e+00 (9.7911e-01)\tAcc@1  69.53 ( 71.08)\tAcc@5  92.19 ( 93.93)\n",
            "Epoch: [50][330/391]\tTime  0.244 ( 0.230)\tLoss 8.1231e-01 (9.8340e-01)\tAcc@1  76.56 ( 71.03)\tAcc@5  94.53 ( 93.81)\n",
            "Epoch: [50][360/391]\tTime  0.172 ( 0.230)\tLoss 1.2167e+00 (9.9042e-01)\tAcc@1  67.19 ( 70.90)\tAcc@5  91.41 ( 93.68)\n",
            "Epoch: [50][390/391]\tTime  0.149 ( 0.230)\tLoss 1.1035e+00 (9.9775e-01)\tAcc@1  66.25 ( 70.67)\tAcc@5  93.75 ( 93.56)\n",
            "==> Train Accuracy: Acc@1 70.666 || Acc@5 93.564\n",
            "==> Test Accuracy:  Acc@1 57.620 || Acc@5 84.960\n",
            "==> 94.09 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 51, lr: 0.1 -----\n",
            "Epoch: [51][  0/391]\tTime  0.575 ( 0.575)\tLoss 9.4777e-01 (9.4777e-01)\tAcc@1  70.31 ( 70.31)\tAcc@5  93.75 ( 93.75)\n",
            "Epoch: [51][ 30/391]\tTime  0.293 ( 0.253)\tLoss 8.9870e-01 (9.1570e-01)\tAcc@1  74.22 ( 72.71)\tAcc@5  96.09 ( 94.71)\n",
            "Epoch: [51][ 60/391]\tTime  0.218 ( 0.240)\tLoss 9.0416e-01 (9.2409e-01)\tAcc@1  72.66 ( 72.44)\tAcc@5  92.97 ( 94.47)\n",
            "Epoch: [51][ 90/391]\tTime  0.370 ( 0.238)\tLoss 7.9950e-01 (9.3607e-01)\tAcc@1  75.78 ( 72.18)\tAcc@5  95.31 ( 94.27)\n",
            "Epoch: [51][120/391]\tTime  0.304 ( 0.237)\tLoss 9.6151e-01 (9.4590e-01)\tAcc@1  74.22 ( 72.00)\tAcc@5  94.53 ( 94.15)\n",
            "Epoch: [51][150/391]\tTime  0.247 ( 0.236)\tLoss 1.0389e+00 (9.5094e-01)\tAcc@1  69.53 ( 71.99)\tAcc@5  91.41 ( 94.05)\n",
            "Epoch: [51][180/391]\tTime  0.170 ( 0.235)\tLoss 9.8690e-01 (9.6252e-01)\tAcc@1  73.44 ( 71.73)\tAcc@5  93.75 ( 93.99)\n",
            "Epoch: [51][210/391]\tTime  0.280 ( 0.235)\tLoss 1.0886e+00 (9.6719e-01)\tAcc@1  67.19 ( 71.52)\tAcc@5  89.84 ( 93.92)\n",
            "Epoch: [51][240/391]\tTime  0.327 ( 0.234)\tLoss 1.0334e+00 (9.7387e-01)\tAcc@1  70.31 ( 71.28)\tAcc@5  92.19 ( 93.79)\n",
            "Epoch: [51][270/391]\tTime  0.219 ( 0.233)\tLoss 1.2058e+00 (9.7707e-01)\tAcc@1  67.19 ( 71.35)\tAcc@5  92.19 ( 93.74)\n",
            "Epoch: [51][300/391]\tTime  0.255 ( 0.234)\tLoss 8.1716e-01 (9.8167e-01)\tAcc@1  78.12 ( 71.23)\tAcc@5  92.97 ( 93.69)\n",
            "Epoch: [51][330/391]\tTime  0.313 ( 0.233)\tLoss 9.6155e-01 (9.8755e-01)\tAcc@1  74.22 ( 71.03)\tAcc@5  94.53 ( 93.66)\n",
            "Epoch: [51][360/391]\tTime  0.313 ( 0.234)\tLoss 1.0618e+00 (9.9564e-01)\tAcc@1  67.97 ( 70.79)\tAcc@5  93.75 ( 93.56)\n",
            "Epoch: [51][390/391]\tTime  0.146 ( 0.233)\tLoss 1.3973e+00 (9.9941e-01)\tAcc@1  60.00 ( 70.71)\tAcc@5  85.00 ( 93.51)\n",
            "==> Train Accuracy: Acc@1 70.712 || Acc@5 93.514\n",
            "==> Test Accuracy:  Acc@1 60.650 || Acc@5 86.390\n",
            "==> 95.02 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 52, lr: 0.1 -----\n",
            "Epoch: [52][  0/391]\tTime  0.565 ( 0.565)\tLoss 9.8863e-01 (9.8863e-01)\tAcc@1  64.84 ( 64.84)\tAcc@5  93.75 ( 93.75)\n",
            "Epoch: [52][ 30/391]\tTime  0.172 ( 0.239)\tLoss 6.6896e-01 (8.9945e-01)\tAcc@1  78.91 ( 73.46)\tAcc@5  98.44 ( 94.63)\n",
            "Epoch: [52][ 60/391]\tTime  0.336 ( 0.239)\tLoss 7.7921e-01 (8.8829e-01)\tAcc@1  77.34 ( 73.90)\tAcc@5  96.09 ( 94.80)\n",
            "Epoch: [52][ 90/391]\tTime  0.287 ( 0.236)\tLoss 1.0938e+00 (8.9105e-01)\tAcc@1  65.62 ( 73.80)\tAcc@5  92.19 ( 94.84)\n",
            "Epoch: [52][120/391]\tTime  0.315 ( 0.234)\tLoss 1.0425e+00 (9.1714e-01)\tAcc@1  72.66 ( 73.10)\tAcc@5  93.75 ( 94.54)\n",
            "Epoch: [52][150/391]\tTime  0.181 ( 0.233)\tLoss 1.0817e+00 (9.2750e-01)\tAcc@1  67.19 ( 72.73)\tAcc@5  91.41 ( 94.42)\n",
            "Epoch: [52][180/391]\tTime  0.175 ( 0.232)\tLoss 1.1940e+00 (9.4738e-01)\tAcc@1  65.62 ( 72.13)\tAcc@5  92.97 ( 94.08)\n",
            "Epoch: [52][210/391]\tTime  0.169 ( 0.233)\tLoss 9.8577e-01 (9.5543e-01)\tAcc@1  67.97 ( 71.92)\tAcc@5  95.31 ( 94.02)\n",
            "Epoch: [52][240/391]\tTime  0.186 ( 0.231)\tLoss 1.1008e+00 (9.5539e-01)\tAcc@1  67.19 ( 71.89)\tAcc@5  92.19 ( 94.08)\n",
            "Epoch: [52][270/391]\tTime  0.167 ( 0.232)\tLoss 1.0298e+00 (9.6761e-01)\tAcc@1  71.09 ( 71.50)\tAcc@5  92.97 ( 93.95)\n",
            "Epoch: [52][300/391]\tTime  0.167 ( 0.232)\tLoss 1.0783e+00 (9.7723e-01)\tAcc@1  70.31 ( 71.25)\tAcc@5  93.75 ( 93.88)\n",
            "Epoch: [52][330/391]\tTime  0.292 ( 0.231)\tLoss 1.0051e+00 (9.8637e-01)\tAcc@1  67.97 ( 70.93)\tAcc@5  92.97 ( 93.74)\n",
            "Epoch: [52][360/391]\tTime  0.168 ( 0.230)\tLoss 1.2067e+00 (9.9560e-01)\tAcc@1  67.97 ( 70.65)\tAcc@5  92.97 ( 93.65)\n",
            "Epoch: [52][390/391]\tTime  0.142 ( 0.230)\tLoss 1.1136e+00 (9.9731e-01)\tAcc@1  65.00 ( 70.57)\tAcc@5  91.25 ( 93.62)\n",
            "==> Train Accuracy: Acc@1 70.566 || Acc@5 93.618\n",
            "==> Test Accuracy:  Acc@1 57.770 || Acc@5 84.700\n",
            "==> 93.87 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 53, lr: 0.1 -----\n",
            "Epoch: [53][  0/391]\tTime  0.598 ( 0.598)\tLoss 8.5322e-01 (8.5322e-01)\tAcc@1  77.34 ( 77.34)\tAcc@5  96.09 ( 96.09)\n",
            "Epoch: [53][ 30/391]\tTime  0.169 ( 0.236)\tLoss 8.5345e-01 (8.9018e-01)\tAcc@1  71.88 ( 73.69)\tAcc@5  96.09 ( 94.76)\n",
            "Epoch: [53][ 60/391]\tTime  0.158 ( 0.231)\tLoss 1.3080e+00 (8.9914e-01)\tAcc@1  60.94 ( 73.18)\tAcc@5  92.19 ( 94.63)\n",
            "Epoch: [53][ 90/391]\tTime  0.301 ( 0.230)\tLoss 1.0295e+00 (9.0536e-01)\tAcc@1  68.75 ( 72.60)\tAcc@5  96.09 ( 94.73)\n",
            "Epoch: [53][120/391]\tTime  0.152 ( 0.229)\tLoss 9.0193e-01 (9.1859e-01)\tAcc@1  78.91 ( 72.26)\tAcc@5  93.75 ( 94.60)\n",
            "Epoch: [53][150/391]\tTime  0.183 ( 0.232)\tLoss 1.1464e+00 (9.3074e-01)\tAcc@1  67.97 ( 71.80)\tAcc@5  92.19 ( 94.54)\n",
            "Epoch: [53][180/391]\tTime  0.179 ( 0.231)\tLoss 1.0790e+00 (9.4851e-01)\tAcc@1  64.84 ( 71.40)\tAcc@5  95.31 ( 94.30)\n",
            "Epoch: [53][210/391]\tTime  0.156 ( 0.231)\tLoss 9.2127e-01 (9.5347e-01)\tAcc@1  74.22 ( 71.35)\tAcc@5  94.53 ( 94.18)\n",
            "Epoch: [53][240/391]\tTime  0.162 ( 0.231)\tLoss 9.0667e-01 (9.5824e-01)\tAcc@1  70.31 ( 71.35)\tAcc@5  94.53 ( 94.11)\n",
            "Epoch: [53][270/391]\tTime  0.160 ( 0.230)\tLoss 1.1947e+00 (9.6713e-01)\tAcc@1  61.72 ( 71.07)\tAcc@5  92.97 ( 94.02)\n",
            "Epoch: [53][300/391]\tTime  0.164 ( 0.231)\tLoss 1.0219e+00 (9.7030e-01)\tAcc@1  67.97 ( 70.98)\tAcc@5  94.53 ( 93.95)\n",
            "Epoch: [53][330/391]\tTime  0.189 ( 0.231)\tLoss 1.0219e+00 (9.7586e-01)\tAcc@1  71.88 ( 70.90)\tAcc@5  95.31 ( 93.88)\n",
            "Epoch: [53][360/391]\tTime  0.173 ( 0.231)\tLoss 1.3036e+00 (9.8712e-01)\tAcc@1  62.50 ( 70.64)\tAcc@5  88.28 ( 93.74)\n",
            "Epoch: [53][390/391]\tTime  0.149 ( 0.231)\tLoss 1.5080e+00 (9.9120e-01)\tAcc@1  56.25 ( 70.50)\tAcc@5  92.50 ( 93.71)\n",
            "==> Train Accuracy: Acc@1 70.496 || Acc@5 93.708\n",
            "==> Test Accuracy:  Acc@1 59.010 || Acc@5 85.620\n",
            "==> 94.25 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 54, lr: 0.1 -----\n",
            "Epoch: [54][  0/391]\tTime  0.566 ( 0.566)\tLoss 8.4371e-01 (8.4371e-01)\tAcc@1  74.22 ( 74.22)\tAcc@5  96.88 ( 96.88)\n",
            "Epoch: [54][ 30/391]\tTime  0.160 ( 0.235)\tLoss 6.8360e-01 (9.2211e-01)\tAcc@1  79.69 ( 72.73)\tAcc@5  98.44 ( 94.51)\n",
            "Epoch: [54][ 60/391]\tTime  0.173 ( 0.230)\tLoss 8.0929e-01 (9.1409e-01)\tAcc@1  75.00 ( 73.07)\tAcc@5  95.31 ( 94.57)\n",
            "Epoch: [54][ 90/391]\tTime  0.253 ( 0.230)\tLoss 9.2839e-01 (9.1011e-01)\tAcc@1  69.53 ( 73.17)\tAcc@5  94.53 ( 94.70)\n",
            "Epoch: [54][120/391]\tTime  0.272 ( 0.231)\tLoss 7.8238e-01 (9.2791e-01)\tAcc@1  78.12 ( 72.59)\tAcc@5  95.31 ( 94.64)\n",
            "Epoch: [54][150/391]\tTime  0.325 ( 0.230)\tLoss 1.0192e+00 (9.2579e-01)\tAcc@1  72.66 ( 72.53)\tAcc@5  92.19 ( 94.58)\n",
            "Epoch: [54][180/391]\tTime  0.260 ( 0.230)\tLoss 9.7043e-01 (9.4026e-01)\tAcc@1  67.97 ( 72.13)\tAcc@5  96.88 ( 94.37)\n",
            "Epoch: [54][210/391]\tTime  0.289 ( 0.230)\tLoss 1.1132e+00 (9.5544e-01)\tAcc@1  67.97 ( 71.67)\tAcc@5  94.53 ( 94.17)\n",
            "Epoch: [54][240/391]\tTime  0.305 ( 0.231)\tLoss 9.4732e-01 (9.7212e-01)\tAcc@1  69.53 ( 71.19)\tAcc@5  96.09 ( 93.97)\n",
            "Epoch: [54][270/391]\tTime  0.293 ( 0.231)\tLoss 1.2975e+00 (9.8267e-01)\tAcc@1  60.94 ( 70.86)\tAcc@5  89.84 ( 93.81)\n",
            "Epoch: [54][300/391]\tTime  0.202 ( 0.231)\tLoss 8.7339e-01 (9.8782e-01)\tAcc@1  73.44 ( 70.72)\tAcc@5  95.31 ( 93.75)\n",
            "Epoch: [54][330/391]\tTime  0.255 ( 0.231)\tLoss 1.0662e+00 (9.9625e-01)\tAcc@1  64.06 ( 70.52)\tAcc@5  93.75 ( 93.65)\n",
            "Epoch: [54][360/391]\tTime  0.184 ( 0.230)\tLoss 9.5457e-01 (9.9702e-01)\tAcc@1  71.09 ( 70.54)\tAcc@5  94.53 ( 93.67)\n",
            "Epoch: [54][390/391]\tTime  0.144 ( 0.230)\tLoss 1.0606e+00 (9.9899e-01)\tAcc@1  66.25 ( 70.52)\tAcc@5  95.00 ( 93.63)\n",
            "==> Train Accuracy: Acc@1 70.520 || Acc@5 93.632\n",
            "==> Test Accuracy:  Acc@1 59.800 || Acc@5 86.730\n",
            "==> 93.98 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 55, lr: 0.1 -----\n",
            "Epoch: [55][  0/391]\tTime  0.596 ( 0.596)\tLoss 8.8324e-01 (8.8324e-01)\tAcc@1  72.66 ( 72.66)\tAcc@5  92.97 ( 92.97)\n",
            "Epoch: [55][ 30/391]\tTime  0.239 ( 0.236)\tLoss 9.5744e-01 (9.1034e-01)\tAcc@1  72.66 ( 72.83)\tAcc@5  95.31 ( 95.31)\n",
            "Epoch: [55][ 60/391]\tTime  0.149 ( 0.232)\tLoss 9.3270e-01 (9.0759e-01)\tAcc@1  74.22 ( 72.90)\tAcc@5  92.97 ( 95.16)\n",
            "Epoch: [55][ 90/391]\tTime  0.166 ( 0.233)\tLoss 1.0710e+00 (9.1953e-01)\tAcc@1  68.75 ( 72.44)\tAcc@5  95.31 ( 94.97)\n",
            "Epoch: [55][120/391]\tTime  0.167 ( 0.232)\tLoss 1.1915e+00 (9.2536e-01)\tAcc@1  67.97 ( 72.36)\tAcc@5  89.84 ( 94.73)\n",
            "Epoch: [55][150/391]\tTime  0.192 ( 0.231)\tLoss 1.0213e+00 (9.4392e-01)\tAcc@1  71.09 ( 71.99)\tAcc@5  92.19 ( 94.44)\n",
            "Epoch: [55][180/391]\tTime  0.216 ( 0.231)\tLoss 8.8779e-01 (9.4594e-01)\tAcc@1  72.66 ( 71.84)\tAcc@5  97.66 ( 94.35)\n",
            "Epoch: [55][210/391]\tTime  0.270 ( 0.232)\tLoss 1.0636e+00 (9.5332e-01)\tAcc@1  71.09 ( 71.66)\tAcc@5  92.97 ( 94.21)\n",
            "Epoch: [55][240/391]\tTime  0.280 ( 0.232)\tLoss 8.7965e-01 (9.6055e-01)\tAcc@1  72.66 ( 71.46)\tAcc@5  95.31 ( 94.08)\n",
            "Epoch: [55][270/391]\tTime  0.178 ( 0.231)\tLoss 1.0418e+00 (9.6991e-01)\tAcc@1  71.09 ( 71.28)\tAcc@5  92.97 ( 93.99)\n",
            "Epoch: [55][300/391]\tTime  0.178 ( 0.232)\tLoss 1.0657e+00 (9.7579e-01)\tAcc@1  66.41 ( 71.16)\tAcc@5  90.62 ( 93.87)\n",
            "Epoch: [55][330/391]\tTime  0.166 ( 0.232)\tLoss 1.0417e+00 (9.8115e-01)\tAcc@1  68.75 ( 71.04)\tAcc@5  89.06 ( 93.83)\n",
            "Epoch: [55][360/391]\tTime  0.181 ( 0.232)\tLoss 8.7743e-01 (9.8514e-01)\tAcc@1  75.00 ( 70.88)\tAcc@5  94.53 ( 93.80)\n",
            "Epoch: [55][390/391]\tTime  0.148 ( 0.231)\tLoss 1.0406e+00 (9.8959e-01)\tAcc@1  68.75 ( 70.75)\tAcc@5  92.50 ( 93.72)\n",
            "==> Train Accuracy: Acc@1 70.748 || Acc@5 93.724\n",
            "==> Test Accuracy:  Acc@1 59.450 || Acc@5 86.790\n",
            "==> 94.55 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 56, lr: 0.1 -----\n",
            "Epoch: [56][  0/391]\tTime  0.587 ( 0.587)\tLoss 1.2128e+00 (1.2128e+00)\tAcc@1  64.06 ( 64.06)\tAcc@5  92.97 ( 92.97)\n",
            "Epoch: [56][ 30/391]\tTime  0.242 ( 0.246)\tLoss 1.1551e+00 (9.1657e-01)\tAcc@1  63.28 ( 72.83)\tAcc@5  94.53 ( 94.51)\n",
            "Epoch: [56][ 60/391]\tTime  0.252 ( 0.237)\tLoss 7.8693e-01 (9.1732e-01)\tAcc@1  75.00 ( 72.71)\tAcc@5  96.88 ( 94.51)\n",
            "Epoch: [56][ 90/391]\tTime  0.315 ( 0.234)\tLoss 9.5277e-01 (9.2610e-01)\tAcc@1  72.66 ( 72.61)\tAcc@5  96.09 ( 94.48)\n",
            "Epoch: [56][120/391]\tTime  0.170 ( 0.232)\tLoss 9.2152e-01 (9.3349e-01)\tAcc@1  74.22 ( 72.37)\tAcc@5  93.75 ( 94.42)\n",
            "Epoch: [56][150/391]\tTime  0.298 ( 0.232)\tLoss 8.1761e-01 (9.4447e-01)\tAcc@1  75.78 ( 72.05)\tAcc@5  95.31 ( 94.26)\n",
            "Epoch: [56][180/391]\tTime  0.218 ( 0.232)\tLoss 1.0757e+00 (9.4765e-01)\tAcc@1  66.41 ( 71.84)\tAcc@5  93.75 ( 94.24)\n",
            "Epoch: [56][210/391]\tTime  0.318 ( 0.233)\tLoss 9.4474e-01 (9.5058e-01)\tAcc@1  68.75 ( 71.74)\tAcc@5  96.09 ( 94.18)\n",
            "Epoch: [56][240/391]\tTime  0.260 ( 0.233)\tLoss 9.2874e-01 (9.5496e-01)\tAcc@1  73.44 ( 71.60)\tAcc@5  92.97 ( 94.14)\n",
            "Epoch: [56][270/391]\tTime  0.330 ( 0.233)\tLoss 1.2997e+00 (9.6923e-01)\tAcc@1  63.28 ( 71.21)\tAcc@5  89.84 ( 94.00)\n",
            "Epoch: [56][300/391]\tTime  0.334 ( 0.232)\tLoss 1.0625e+00 (9.7983e-01)\tAcc@1  67.19 ( 70.98)\tAcc@5  95.31 ( 93.87)\n",
            "Epoch: [56][330/391]\tTime  0.280 ( 0.233)\tLoss 1.1226e+00 (9.8481e-01)\tAcc@1  69.53 ( 70.92)\tAcc@5  91.41 ( 93.78)\n",
            "Epoch: [56][360/391]\tTime  0.224 ( 0.232)\tLoss 9.7016e-01 (9.8905e-01)\tAcc@1  73.44 ( 70.81)\tAcc@5  95.31 ( 93.70)\n",
            "Epoch: [56][390/391]\tTime  0.148 ( 0.232)\tLoss 8.6508e-01 (9.9207e-01)\tAcc@1  78.75 ( 70.73)\tAcc@5  95.00 ( 93.64)\n",
            "==> Train Accuracy: Acc@1 70.730 || Acc@5 93.640\n",
            "==> Test Accuracy:  Acc@1 61.130 || Acc@5 86.790\n",
            "==> 94.84 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 57, lr: 0.1 -----\n",
            "Epoch: [57][  0/391]\tTime  0.608 ( 0.608)\tLoss 1.0300e+00 (1.0300e+00)\tAcc@1  68.75 ( 68.75)\tAcc@5  92.97 ( 92.97)\n",
            "Epoch: [57][ 30/391]\tTime  0.225 ( 0.243)\tLoss 8.2692e-01 (9.2138e-01)\tAcc@1  73.44 ( 72.86)\tAcc@5  96.09 ( 94.35)\n",
            "Epoch: [57][ 60/391]\tTime  0.305 ( 0.238)\tLoss 9.8874e-01 (9.1302e-01)\tAcc@1  67.19 ( 72.72)\tAcc@5  94.53 ( 94.54)\n",
            "Epoch: [57][ 90/391]\tTime  0.300 ( 0.237)\tLoss 6.7788e-01 (8.9837e-01)\tAcc@1  77.34 ( 73.27)\tAcc@5  97.66 ( 94.69)\n",
            "Epoch: [57][120/391]\tTime  0.310 ( 0.236)\tLoss 8.5923e-01 (9.0757e-01)\tAcc@1  77.34 ( 72.95)\tAcc@5  95.31 ( 94.56)\n",
            "Epoch: [57][150/391]\tTime  0.265 ( 0.234)\tLoss 9.3488e-01 (9.1488e-01)\tAcc@1  75.00 ( 72.72)\tAcc@5  93.75 ( 94.45)\n",
            "Epoch: [57][180/391]\tTime  0.261 ( 0.233)\tLoss 1.0351e+00 (9.2904e-01)\tAcc@1  70.31 ( 72.37)\tAcc@5  91.41 ( 94.26)\n",
            "Epoch: [57][210/391]\tTime  0.242 ( 0.232)\tLoss 1.0968e+00 (9.3942e-01)\tAcc@1  69.53 ( 72.04)\tAcc@5  93.75 ( 94.16)\n",
            "Epoch: [57][240/391]\tTime  0.178 ( 0.232)\tLoss 1.0086e+00 (9.5237e-01)\tAcc@1  71.88 ( 71.65)\tAcc@5  92.19 ( 94.07)\n",
            "Epoch: [57][270/391]\tTime  0.158 ( 0.231)\tLoss 8.9674e-01 (9.6097e-01)\tAcc@1  74.22 ( 71.42)\tAcc@5  92.19 ( 93.94)\n",
            "Epoch: [57][300/391]\tTime  0.177 ( 0.231)\tLoss 9.9821e-01 (9.6827e-01)\tAcc@1  71.88 ( 71.27)\tAcc@5  93.75 ( 93.89)\n",
            "Epoch: [57][330/391]\tTime  0.175 ( 0.231)\tLoss 9.3144e-01 (9.7216e-01)\tAcc@1  71.09 ( 71.14)\tAcc@5  93.75 ( 93.89)\n",
            "Epoch: [57][360/391]\tTime  0.276 ( 0.232)\tLoss 1.0966e+00 (9.8048e-01)\tAcc@1  64.84 ( 70.95)\tAcc@5  95.31 ( 93.81)\n",
            "Epoch: [57][390/391]\tTime  0.147 ( 0.231)\tLoss 9.6490e-01 (9.8619e-01)\tAcc@1  73.75 ( 70.80)\tAcc@5  92.50 ( 93.75)\n",
            "==> Train Accuracy: Acc@1 70.802 || Acc@5 93.746\n",
            "==> Test Accuracy:  Acc@1 60.370 || Acc@5 86.180\n",
            "==> 94.43 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 58, lr: 0.1 -----\n",
            "Epoch: [58][  0/391]\tTime  0.583 ( 0.583)\tLoss 1.0176e+00 (1.0176e+00)\tAcc@1  75.00 ( 75.00)\tAcc@5  91.41 ( 91.41)\n",
            "Epoch: [58][ 30/391]\tTime  0.314 ( 0.245)\tLoss 8.6614e-01 (8.7803e-01)\tAcc@1  74.22 ( 74.42)\tAcc@5  91.41 ( 94.66)\n",
            "Epoch: [58][ 60/391]\tTime  0.296 ( 0.236)\tLoss 8.9941e-01 (8.8774e-01)\tAcc@1  74.22 ( 74.10)\tAcc@5  97.66 ( 94.67)\n",
            "Epoch: [58][ 90/391]\tTime  0.180 ( 0.230)\tLoss 8.0753e-01 (8.7663e-01)\tAcc@1  75.00 ( 74.05)\tAcc@5  96.09 ( 94.79)\n",
            "Epoch: [58][120/391]\tTime  0.222 ( 0.230)\tLoss 9.3886e-01 (8.9432e-01)\tAcc@1  71.09 ( 73.53)\tAcc@5  92.19 ( 94.58)\n",
            "Epoch: [58][150/391]\tTime  0.281 ( 0.230)\tLoss 8.5592e-01 (9.0244e-01)\tAcc@1  72.66 ( 73.25)\tAcc@5  93.75 ( 94.54)\n",
            "Epoch: [58][180/391]\tTime  0.214 ( 0.229)\tLoss 8.4710e-01 (9.1580e-01)\tAcc@1  73.44 ( 72.89)\tAcc@5  97.66 ( 94.49)\n",
            "Epoch: [58][210/391]\tTime  0.162 ( 0.229)\tLoss 9.6309e-01 (9.2372e-01)\tAcc@1  75.78 ( 72.67)\tAcc@5  94.53 ( 94.39)\n",
            "Epoch: [58][240/391]\tTime  0.181 ( 0.228)\tLoss 1.1565e+00 (9.3618e-01)\tAcc@1  67.19 ( 72.36)\tAcc@5  91.41 ( 94.27)\n",
            "Epoch: [58][270/391]\tTime  0.277 ( 0.228)\tLoss 1.1527e+00 (9.4239e-01)\tAcc@1  72.66 ( 72.23)\tAcc@5  93.75 ( 94.23)\n",
            "Epoch: [58][300/391]\tTime  0.305 ( 0.228)\tLoss 7.9008e-01 (9.4991e-01)\tAcc@1  75.00 ( 72.03)\tAcc@5  97.66 ( 94.13)\n",
            "Epoch: [58][330/391]\tTime  0.316 ( 0.228)\tLoss 7.9358e-01 (9.5865e-01)\tAcc@1  75.00 ( 71.86)\tAcc@5  95.31 ( 93.95)\n",
            "Epoch: [58][360/391]\tTime  0.203 ( 0.227)\tLoss 1.1442e+00 (9.7094e-01)\tAcc@1  70.31 ( 71.50)\tAcc@5  91.41 ( 93.81)\n",
            "Epoch: [58][390/391]\tTime  0.150 ( 0.227)\tLoss 1.2933e+00 (9.7875e-01)\tAcc@1  66.25 ( 71.26)\tAcc@5  87.50 ( 93.78)\n",
            "==> Train Accuracy: Acc@1 71.260 || Acc@5 93.776\n",
            "==> Test Accuracy:  Acc@1 59.210 || Acc@5 86.040\n",
            "==> 92.88 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 59, lr: 0.1 -----\n",
            "Epoch: [59][  0/391]\tTime  0.581 ( 0.581)\tLoss 8.2637e-01 (8.2637e-01)\tAcc@1  78.91 ( 78.91)\tAcc@5  94.53 ( 94.53)\n",
            "Epoch: [59][ 30/391]\tTime  0.276 ( 0.241)\tLoss 9.1826e-01 (8.5993e-01)\tAcc@1  76.56 ( 74.29)\tAcc@5  94.53 ( 95.16)\n",
            "Epoch: [59][ 60/391]\tTime  0.270 ( 0.238)\tLoss 9.9627e-01 (8.7936e-01)\tAcc@1  67.97 ( 73.71)\tAcc@5  95.31 ( 94.89)\n",
            "Epoch: [59][ 90/391]\tTime  0.284 ( 0.236)\tLoss 8.8518e-01 (8.9434e-01)\tAcc@1  71.88 ( 73.21)\tAcc@5  94.53 ( 94.67)\n",
            "Epoch: [59][120/391]\tTime  0.299 ( 0.234)\tLoss 1.2758e+00 (9.1542e-01)\tAcc@1  59.38 ( 72.77)\tAcc@5  95.31 ( 94.51)\n",
            "Epoch: [59][150/391]\tTime  0.246 ( 0.233)\tLoss 1.1816e+00 (9.3472e-01)\tAcc@1  64.06 ( 72.25)\tAcc@5  95.31 ( 94.42)\n",
            "Epoch: [59][180/391]\tTime  0.256 ( 0.233)\tLoss 1.3347e+00 (9.5119e-01)\tAcc@1  62.50 ( 71.78)\tAcc@5  85.94 ( 94.22)\n",
            "Epoch: [59][210/391]\tTime  0.196 ( 0.231)\tLoss 1.0461e+00 (9.6243e-01)\tAcc@1  68.75 ( 71.58)\tAcc@5  92.97 ( 94.04)\n",
            "Epoch: [59][240/391]\tTime  0.172 ( 0.230)\tLoss 9.1597e-01 (9.6557e-01)\tAcc@1  71.88 ( 71.48)\tAcc@5  95.31 ( 94.05)\n",
            "Epoch: [59][270/391]\tTime  0.172 ( 0.229)\tLoss 1.0558e+00 (9.7014e-01)\tAcc@1  71.09 ( 71.41)\tAcc@5  92.97 ( 94.01)\n",
            "Epoch: [59][300/391]\tTime  0.178 ( 0.230)\tLoss 8.2689e-01 (9.6941e-01)\tAcc@1  77.34 ( 71.46)\tAcc@5  92.19 ( 93.97)\n",
            "Epoch: [59][330/391]\tTime  0.159 ( 0.230)\tLoss 7.7763e-01 (9.7147e-01)\tAcc@1  75.78 ( 71.38)\tAcc@5  95.31 ( 93.95)\n",
            "Epoch: [59][360/391]\tTime  0.172 ( 0.229)\tLoss 1.0047e+00 (9.7945e-01)\tAcc@1  68.75 ( 71.19)\tAcc@5  92.97 ( 93.79)\n",
            "Epoch: [59][390/391]\tTime  0.141 ( 0.229)\tLoss 8.8244e-01 (9.8140e-01)\tAcc@1  72.50 ( 71.10)\tAcc@5  96.25 ( 93.80)\n",
            "==> Train Accuracy: Acc@1 71.098 || Acc@5 93.798\n",
            "==> Test Accuracy:  Acc@1 58.720 || Acc@5 85.480\n",
            "==> 93.68 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 60, lr: 0.020000000000000004 -----\n",
            "Epoch: [60][  0/391]\tTime  0.589 ( 0.589)\tLoss 8.7802e-01 (8.7802e-01)\tAcc@1  71.88 ( 71.88)\tAcc@5  95.31 ( 95.31)\n",
            "Epoch: [60][ 30/391]\tTime  0.272 ( 0.239)\tLoss 4.1582e-01 (7.5658e-01)\tAcc@1  89.06 ( 77.75)\tAcc@5  97.66 ( 96.35)\n",
            "Epoch: [60][ 60/391]\tTime  0.264 ( 0.236)\tLoss 6.6886e-01 (7.0319e-01)\tAcc@1  79.69 ( 79.14)\tAcc@5  95.31 ( 96.70)\n",
            "Epoch: [60][ 90/391]\tTime  0.233 ( 0.231)\tLoss 6.2327e-01 (6.6311e-01)\tAcc@1  79.69 ( 80.39)\tAcc@5  96.88 ( 97.06)\n",
            "Epoch: [60][120/391]\tTime  0.175 ( 0.228)\tLoss 5.2377e-01 (6.3150e-01)\tAcc@1  82.81 ( 81.31)\tAcc@5  98.44 ( 97.32)\n",
            "Epoch: [60][150/391]\tTime  0.331 ( 0.229)\tLoss 5.9880e-01 (6.1097e-01)\tAcc@1  79.69 ( 81.86)\tAcc@5 100.00 ( 97.46)\n",
            "Epoch: [60][180/391]\tTime  0.268 ( 0.230)\tLoss 4.7078e-01 (5.9136e-01)\tAcc@1  85.94 ( 82.42)\tAcc@5  98.44 ( 97.58)\n",
            "Epoch: [60][210/391]\tTime  0.164 ( 0.229)\tLoss 4.8039e-01 (5.8054e-01)\tAcc@1  85.16 ( 82.68)\tAcc@5  97.66 ( 97.66)\n",
            "Epoch: [60][240/391]\tTime  0.168 ( 0.230)\tLoss 4.1788e-01 (5.6622e-01)\tAcc@1  85.94 ( 83.16)\tAcc@5  97.66 ( 97.74)\n",
            "Epoch: [60][270/391]\tTime  0.186 ( 0.229)\tLoss 4.5648e-01 (5.5754e-01)\tAcc@1  85.94 ( 83.41)\tAcc@5  98.44 ( 97.80)\n",
            "Epoch: [60][300/391]\tTime  0.163 ( 0.230)\tLoss 3.9561e-01 (5.5088e-01)\tAcc@1  88.28 ( 83.60)\tAcc@5  96.88 ( 97.82)\n",
            "Epoch: [60][330/391]\tTime  0.164 ( 0.229)\tLoss 4.4966e-01 (5.4301e-01)\tAcc@1  89.84 ( 83.87)\tAcc@5  97.66 ( 97.86)\n",
            "Epoch: [60][360/391]\tTime  0.172 ( 0.229)\tLoss 5.1391e-01 (5.3677e-01)\tAcc@1  87.50 ( 84.03)\tAcc@5  95.31 ( 97.91)\n",
            "Epoch: [60][390/391]\tTime  0.150 ( 0.229)\tLoss 7.6719e-01 (5.3019e-01)\tAcc@1  76.25 ( 84.19)\tAcc@5  96.25 ( 97.95)\n",
            "==> Train Accuracy: Acc@1 84.192 || Acc@5 97.954\n",
            "==> Test Accuracy:  Acc@1 74.170 || Acc@5 93.790\n",
            "==> 93.83 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 61, lr: 0.020000000000000004 -----\n",
            "Epoch: [61][  0/391]\tTime  0.590 ( 0.590)\tLoss 4.1805e-01 (4.1805e-01)\tAcc@1  89.06 ( 89.06)\tAcc@5  97.66 ( 97.66)\n",
            "Epoch: [61][ 30/391]\tTime  0.268 ( 0.242)\tLoss 3.4123e-01 (3.8056e-01)\tAcc@1  87.50 ( 88.81)\tAcc@5  99.22 ( 98.92)\n",
            "Epoch: [61][ 60/391]\tTime  0.314 ( 0.239)\tLoss 3.4368e-01 (3.8960e-01)\tAcc@1  87.50 ( 88.56)\tAcc@5  99.22 ( 98.67)\n",
            "Epoch: [61][ 90/391]\tTime  0.323 ( 0.237)\tLoss 3.8230e-01 (3.8766e-01)\tAcc@1  89.84 ( 88.65)\tAcc@5  99.22 ( 98.77)\n",
            "Epoch: [61][120/391]\tTime  0.184 ( 0.234)\tLoss 2.7624e-01 (3.8137e-01)\tAcc@1  91.41 ( 88.80)\tAcc@5 100.00 ( 98.88)\n",
            "Epoch: [61][150/391]\tTime  0.211 ( 0.233)\tLoss 4.2756e-01 (3.8298e-01)\tAcc@1  87.50 ( 88.72)\tAcc@5  99.22 ( 98.89)\n",
            "Epoch: [61][180/391]\tTime  0.336 ( 0.232)\tLoss 3.7880e-01 (3.8207e-01)\tAcc@1  85.94 ( 88.70)\tAcc@5  98.44 ( 98.87)\n",
            "Epoch: [61][210/391]\tTime  0.325 ( 0.232)\tLoss 3.9671e-01 (3.7860e-01)\tAcc@1  89.84 ( 88.77)\tAcc@5  98.44 ( 98.93)\n",
            "Epoch: [61][240/391]\tTime  0.268 ( 0.232)\tLoss 2.8537e-01 (3.7553e-01)\tAcc@1  92.97 ( 88.86)\tAcc@5  98.44 ( 98.95)\n",
            "Epoch: [61][270/391]\tTime  0.302 ( 0.232)\tLoss 4.3002e-01 (3.7593e-01)\tAcc@1  88.28 ( 88.84)\tAcc@5  98.44 ( 98.93)\n",
            "Epoch: [61][300/391]\tTime  0.231 ( 0.231)\tLoss 2.9087e-01 (3.7473e-01)\tAcc@1  92.97 ( 88.82)\tAcc@5 100.00 ( 98.95)\n",
            "Epoch: [61][330/391]\tTime  0.176 ( 0.231)\tLoss 3.9872e-01 (3.7460e-01)\tAcc@1  88.28 ( 88.82)\tAcc@5  97.66 ( 98.94)\n",
            "Epoch: [61][360/391]\tTime  0.168 ( 0.231)\tLoss 3.1615e-01 (3.7423e-01)\tAcc@1  92.19 ( 88.81)\tAcc@5 100.00 ( 98.95)\n",
            "Epoch: [61][390/391]\tTime  0.156 ( 0.231)\tLoss 3.6881e-01 (3.7333e-01)\tAcc@1  93.75 ( 88.82)\tAcc@5  98.75 ( 98.95)\n",
            "==> Train Accuracy: Acc@1 88.822 || Acc@5 98.946\n",
            "==> Test Accuracy:  Acc@1 73.850 || Acc@5 93.870\n",
            "==> 94.50 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 62, lr: 0.020000000000000004 -----\n",
            "Epoch: [62][  0/391]\tTime  0.552 ( 0.552)\tLoss 2.2972e-01 (2.2972e-01)\tAcc@1  92.97 ( 92.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [62][ 30/391]\tTime  0.277 ( 0.232)\tLoss 3.8298e-01 (2.9620e-01)\tAcc@1  89.84 ( 91.43)\tAcc@5  99.22 ( 99.34)\n",
            "Epoch: [62][ 60/391]\tTime  0.201 ( 0.227)\tLoss 3.2512e-01 (2.8119e-01)\tAcc@1  90.62 ( 91.65)\tAcc@5 100.00 ( 99.44)\n",
            "Epoch: [62][ 90/391]\tTime  0.319 ( 0.230)\tLoss 2.8270e-01 (2.8703e-01)\tAcc@1  91.41 ( 91.60)\tAcc@5  99.22 ( 99.30)\n",
            "Epoch: [62][120/391]\tTime  0.261 ( 0.231)\tLoss 2.3433e-01 (2.8595e-01)\tAcc@1  92.19 ( 91.65)\tAcc@5  99.22 ( 99.33)\n",
            "Epoch: [62][150/391]\tTime  0.269 ( 0.231)\tLoss 2.7918e-01 (2.9123e-01)\tAcc@1  92.97 ( 91.34)\tAcc@5 100.00 ( 99.34)\n",
            "Epoch: [62][180/391]\tTime  0.294 ( 0.230)\tLoss 3.4991e-01 (2.9296e-01)\tAcc@1  89.06 ( 91.29)\tAcc@5 100.00 ( 99.37)\n",
            "Epoch: [62][210/391]\tTime  0.158 ( 0.230)\tLoss 2.9681e-01 (2.9672e-01)\tAcc@1  91.41 ( 91.15)\tAcc@5 100.00 ( 99.37)\n",
            "Epoch: [62][240/391]\tTime  0.177 ( 0.230)\tLoss 2.4681e-01 (2.9790e-01)\tAcc@1  94.53 ( 91.06)\tAcc@5  98.44 ( 99.36)\n",
            "Epoch: [62][270/391]\tTime  0.165 ( 0.230)\tLoss 3.3785e-01 (3.0091e-01)\tAcc@1  89.06 ( 90.95)\tAcc@5  98.44 ( 99.34)\n",
            "Epoch: [62][300/391]\tTime  0.181 ( 0.230)\tLoss 4.6482e-01 (3.0323e-01)\tAcc@1  77.34 ( 90.82)\tAcc@5 100.00 ( 99.35)\n",
            "Epoch: [62][330/391]\tTime  0.171 ( 0.230)\tLoss 4.5945e-01 (3.0492e-01)\tAcc@1  85.16 ( 90.72)\tAcc@5  97.66 ( 99.36)\n",
            "Epoch: [62][360/391]\tTime  0.311 ( 0.230)\tLoss 2.3270e-01 (3.0596e-01)\tAcc@1  93.75 ( 90.70)\tAcc@5 100.00 ( 99.36)\n",
            "Epoch: [62][390/391]\tTime  0.149 ( 0.230)\tLoss 1.8720e-01 (3.0634e-01)\tAcc@1  95.00 ( 90.70)\tAcc@5  98.75 ( 99.35)\n",
            "==> Train Accuracy: Acc@1 90.700 || Acc@5 99.348\n",
            "==> Test Accuracy:  Acc@1 73.900 || Acc@5 93.420\n",
            "==> 94.15 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 63, lr: 0.020000000000000004 -----\n",
            "Epoch: [63][  0/391]\tTime  0.574 ( 0.574)\tLoss 2.6910e-01 (2.6910e-01)\tAcc@1  92.97 ( 92.97)\tAcc@5  99.22 ( 99.22)\n",
            "Epoch: [63][ 30/391]\tTime  0.362 ( 0.243)\tLoss 2.4248e-01 (2.5379e-01)\tAcc@1  92.19 ( 92.72)\tAcc@5 100.00 ( 99.65)\n",
            "Epoch: [63][ 60/391]\tTime  0.166 ( 0.233)\tLoss 2.0086e-01 (2.5037e-01)\tAcc@1  93.75 ( 92.71)\tAcc@5 100.00 ( 99.58)\n",
            "Epoch: [63][ 90/391]\tTime  0.164 ( 0.233)\tLoss 2.3813e-01 (2.5072e-01)\tAcc@1  93.75 ( 92.66)\tAcc@5 100.00 ( 99.58)\n",
            "Epoch: [63][120/391]\tTime  0.323 ( 0.233)\tLoss 2.2262e-01 (2.5133e-01)\tAcc@1  92.19 ( 92.71)\tAcc@5 100.00 ( 99.57)\n",
            "Epoch: [63][150/391]\tTime  0.230 ( 0.231)\tLoss 2.2872e-01 (2.5364e-01)\tAcc@1  90.62 ( 92.52)\tAcc@5 100.00 ( 99.57)\n",
            "Epoch: [63][180/391]\tTime  0.351 ( 0.232)\tLoss 1.8434e-01 (2.5546e-01)\tAcc@1  94.53 ( 92.43)\tAcc@5 100.00 ( 99.53)\n",
            "Epoch: [63][210/391]\tTime  0.296 ( 0.232)\tLoss 2.1589e-01 (2.5902e-01)\tAcc@1  92.97 ( 92.34)\tAcc@5 100.00 ( 99.49)\n",
            "Epoch: [63][240/391]\tTime  0.214 ( 0.231)\tLoss 3.3840e-01 (2.6210e-01)\tAcc@1  91.41 ( 92.19)\tAcc@5 100.00 ( 99.47)\n",
            "Epoch: [63][270/391]\tTime  0.297 ( 0.231)\tLoss 2.8378e-01 (2.6290e-01)\tAcc@1  89.06 ( 92.19)\tAcc@5 100.00 ( 99.46)\n",
            "Epoch: [63][300/391]\tTime  0.303 ( 0.231)\tLoss 2.9501e-01 (2.6329e-01)\tAcc@1  92.19 ( 92.16)\tAcc@5  98.44 ( 99.48)\n",
            "Epoch: [63][330/391]\tTime  0.275 ( 0.231)\tLoss 3.0853e-01 (2.6563e-01)\tAcc@1  89.84 ( 92.03)\tAcc@5 100.00 ( 99.48)\n",
            "Epoch: [63][360/391]\tTime  0.277 ( 0.231)\tLoss 2.1256e-01 (2.6622e-01)\tAcc@1  93.75 ( 91.98)\tAcc@5  98.44 ( 99.49)\n",
            "Epoch: [63][390/391]\tTime  0.145 ( 0.231)\tLoss 2.6108e-01 (2.6815e-01)\tAcc@1  88.75 ( 91.87)\tAcc@5 100.00 ( 99.46)\n",
            "==> Train Accuracy: Acc@1 91.874 || Acc@5 99.464\n",
            "==> Test Accuracy:  Acc@1 74.070 || Acc@5 93.500\n",
            "==> 94.23 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 64, lr: 0.020000000000000004 -----\n",
            "Epoch: [64][  0/391]\tTime  0.557 ( 0.557)\tLoss 2.1004e-01 (2.1004e-01)\tAcc@1  92.19 ( 92.19)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [64][ 30/391]\tTime  0.174 ( 0.236)\tLoss 1.8868e-01 (2.1572e-01)\tAcc@1  94.53 ( 93.25)\tAcc@5 100.00 ( 99.85)\n",
            "Epoch: [64][ 60/391]\tTime  0.176 ( 0.231)\tLoss 2.6852e-01 (2.1420e-01)\tAcc@1  93.75 ( 93.55)\tAcc@5 100.00 ( 99.82)\n",
            "Epoch: [64][ 90/391]\tTime  0.155 ( 0.232)\tLoss 2.3323e-01 (2.1668e-01)\tAcc@1  92.19 ( 93.45)\tAcc@5 100.00 ( 99.77)\n",
            "Epoch: [64][120/391]\tTime  0.171 ( 0.229)\tLoss 3.5888e-01 (2.2066e-01)\tAcc@1  86.72 ( 93.27)\tAcc@5  99.22 ( 99.78)\n",
            "Epoch: [64][150/391]\tTime  0.289 ( 0.229)\tLoss 2.7141e-01 (2.2024e-01)\tAcc@1  93.75 ( 93.38)\tAcc@5  99.22 ( 99.76)\n",
            "Epoch: [64][180/391]\tTime  0.358 ( 0.230)\tLoss 1.2900e-01 (2.1878e-01)\tAcc@1  97.66 ( 93.47)\tAcc@5 100.00 ( 99.75)\n",
            "Epoch: [64][210/391]\tTime  0.258 ( 0.230)\tLoss 1.9401e-01 (2.2095e-01)\tAcc@1  94.53 ( 93.39)\tAcc@5 100.00 ( 99.72)\n",
            "Epoch: [64][240/391]\tTime  0.250 ( 0.229)\tLoss 2.0999e-01 (2.2197e-01)\tAcc@1  92.97 ( 93.40)\tAcc@5 100.00 ( 99.73)\n",
            "Epoch: [64][270/391]\tTime  0.300 ( 0.230)\tLoss 2.1341e-01 (2.2266e-01)\tAcc@1  93.75 ( 93.36)\tAcc@5  99.22 ( 99.73)\n",
            "Epoch: [64][300/391]\tTime  0.215 ( 0.230)\tLoss 2.7619e-01 (2.2457e-01)\tAcc@1  89.06 ( 93.29)\tAcc@5 100.00 ( 99.70)\n",
            "Epoch: [64][330/391]\tTime  0.305 ( 0.230)\tLoss 2.2712e-01 (2.2883e-01)\tAcc@1  92.97 ( 93.15)\tAcc@5 100.00 ( 99.68)\n",
            "Epoch: [64][360/391]\tTime  0.326 ( 0.230)\tLoss 1.9051e-01 (2.2989e-01)\tAcc@1  95.31 ( 93.11)\tAcc@5 100.00 ( 99.68)\n",
            "Epoch: [64][390/391]\tTime  0.147 ( 0.230)\tLoss 2.1566e-01 (2.3153e-01)\tAcc@1  92.50 ( 93.03)\tAcc@5 100.00 ( 99.67)\n",
            "==> Train Accuracy: Acc@1 93.028 || Acc@5 99.674\n",
            "==> Test Accuracy:  Acc@1 73.860 || Acc@5 93.090\n",
            "==> 93.94 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 65, lr: 0.020000000000000004 -----\n",
            "Epoch: [65][  0/391]\tTime  0.563 ( 0.563)\tLoss 1.0696e-01 (1.0696e-01)\tAcc@1  96.88 ( 96.88)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [65][ 30/391]\tTime  0.204 ( 0.234)\tLoss 8.9561e-02 (1.7734e-01)\tAcc@1  97.66 ( 95.19)\tAcc@5 100.00 ( 99.82)\n",
            "Epoch: [65][ 60/391]\tTime  0.237 ( 0.228)\tLoss 1.0114e-01 (1.8209e-01)\tAcc@1  97.66 ( 94.79)\tAcc@5 100.00 ( 99.83)\n",
            "Epoch: [65][ 90/391]\tTime  0.317 ( 0.229)\tLoss 1.5996e-01 (1.8413e-01)\tAcc@1  92.97 ( 94.71)\tAcc@5 100.00 ( 99.75)\n",
            "Epoch: [65][120/391]\tTime  0.332 ( 0.229)\tLoss 1.5726e-01 (1.8404e-01)\tAcc@1  95.31 ( 94.72)\tAcc@5 100.00 ( 99.78)\n",
            "Epoch: [65][150/391]\tTime  0.215 ( 0.228)\tLoss 2.1435e-01 (1.8622e-01)\tAcc@1  92.19 ( 94.59)\tAcc@5 100.00 ( 99.80)\n",
            "Epoch: [65][180/391]\tTime  0.170 ( 0.228)\tLoss 1.6290e-01 (1.8565e-01)\tAcc@1  94.53 ( 94.62)\tAcc@5 100.00 ( 99.81)\n",
            "Epoch: [65][210/391]\tTime  0.194 ( 0.228)\tLoss 2.6377e-01 (1.8685e-01)\tAcc@1  90.62 ( 94.58)\tAcc@5  99.22 ( 99.80)\n",
            "Epoch: [65][240/391]\tTime  0.162 ( 0.229)\tLoss 1.8573e-01 (1.9021e-01)\tAcc@1  96.09 ( 94.49)\tAcc@5 100.00 ( 99.81)\n",
            "Epoch: [65][270/391]\tTime  0.288 ( 0.229)\tLoss 1.7839e-01 (1.9210e-01)\tAcc@1  96.09 ( 94.43)\tAcc@5 100.00 ( 99.81)\n",
            "Epoch: [65][300/391]\tTime  0.240 ( 0.229)\tLoss 1.8720e-01 (1.9523e-01)\tAcc@1  94.53 ( 94.32)\tAcc@5 100.00 ( 99.80)\n",
            "Epoch: [65][330/391]\tTime  0.163 ( 0.228)\tLoss 1.6479e-01 (1.9833e-01)\tAcc@1  95.31 ( 94.19)\tAcc@5 100.00 ( 99.79)\n",
            "Epoch: [65][360/391]\tTime  0.161 ( 0.229)\tLoss 1.7576e-01 (2.0048e-01)\tAcc@1  95.31 ( 94.13)\tAcc@5 100.00 ( 99.78)\n",
            "Epoch: [65][390/391]\tTime  0.154 ( 0.228)\tLoss 2.8166e-01 (2.0455e-01)\tAcc@1  92.50 ( 93.97)\tAcc@5 100.00 ( 99.76)\n",
            "==> Train Accuracy: Acc@1 93.968 || Acc@5 99.762\n",
            "==> Test Accuracy:  Acc@1 73.180 || Acc@5 92.530\n",
            "==> 93.27 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 66, lr: 0.020000000000000004 -----\n",
            "Epoch: [66][  0/391]\tTime  0.581 ( 0.581)\tLoss 2.4396e-01 (2.4396e-01)\tAcc@1  92.19 ( 92.19)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [66][ 30/391]\tTime  0.261 ( 0.236)\tLoss 1.6803e-01 (1.7743e-01)\tAcc@1  96.09 ( 94.86)\tAcc@5  99.22 ( 99.85)\n",
            "Epoch: [66][ 60/391]\tTime  0.332 ( 0.235)\tLoss 1.2295e-01 (1.7752e-01)\tAcc@1  96.88 ( 95.04)\tAcc@5 100.00 ( 99.78)\n",
            "Epoch: [66][ 90/391]\tTime  0.172 ( 0.232)\tLoss 1.7936e-01 (1.7360e-01)\tAcc@1  95.31 ( 95.16)\tAcc@5 100.00 ( 99.83)\n",
            "Epoch: [66][120/391]\tTime  0.300 ( 0.230)\tLoss 1.4784e-01 (1.7720e-01)\tAcc@1  95.31 ( 94.99)\tAcc@5 100.00 ( 99.82)\n",
            "Epoch: [66][150/391]\tTime  0.358 ( 0.230)\tLoss 1.4914e-01 (1.7770e-01)\tAcc@1  95.31 ( 94.91)\tAcc@5 100.00 ( 99.84)\n",
            "Epoch: [66][180/391]\tTime  0.248 ( 0.230)\tLoss 1.7452e-01 (1.7737e-01)\tAcc@1  96.09 ( 94.97)\tAcc@5 100.00 ( 99.85)\n",
            "Epoch: [66][210/391]\tTime  0.165 ( 0.230)\tLoss 1.5089e-01 (1.7826e-01)\tAcc@1  95.31 ( 94.93)\tAcc@5 100.00 ( 99.86)\n",
            "Epoch: [66][240/391]\tTime  0.183 ( 0.229)\tLoss 2.3932e-01 (1.7944e-01)\tAcc@1  95.31 ( 94.90)\tAcc@5 100.00 ( 99.86)\n",
            "Epoch: [66][270/391]\tTime  0.272 ( 0.229)\tLoss 1.9334e-01 (1.8238e-01)\tAcc@1  95.31 ( 94.80)\tAcc@5 100.00 ( 99.86)\n",
            "Epoch: [66][300/391]\tTime  0.168 ( 0.228)\tLoss 1.4971e-01 (1.8309e-01)\tAcc@1  95.31 ( 94.77)\tAcc@5  99.22 ( 99.84)\n",
            "Epoch: [66][330/391]\tTime  0.292 ( 0.229)\tLoss 2.2285e-01 (1.8530e-01)\tAcc@1  92.19 ( 94.68)\tAcc@5  99.22 ( 99.83)\n",
            "Epoch: [66][360/391]\tTime  0.189 ( 0.229)\tLoss 2.6809e-01 (1.8747e-01)\tAcc@1  93.75 ( 94.59)\tAcc@5  99.22 ( 99.82)\n",
            "Epoch: [66][390/391]\tTime  0.150 ( 0.229)\tLoss 1.8444e-01 (1.8964e-01)\tAcc@1  91.25 ( 94.53)\tAcc@5 100.00 ( 99.81)\n",
            "==> Train Accuracy: Acc@1 94.526 || Acc@5 99.806\n",
            "==> Test Accuracy:  Acc@1 72.910 || Acc@5 92.950\n",
            "==> 93.49 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 67, lr: 0.020000000000000004 -----\n",
            "Epoch: [67][  0/391]\tTime  0.585 ( 0.585)\tLoss 1.0019e-01 (1.0019e-01)\tAcc@1  96.88 ( 96.88)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [67][ 30/391]\tTime  0.264 ( 0.234)\tLoss 1.4063e-01 (1.5497e-01)\tAcc@1  96.09 ( 95.72)\tAcc@5 100.00 ( 99.80)\n",
            "Epoch: [67][ 60/391]\tTime  0.168 ( 0.231)\tLoss 2.3198e-01 (1.6097e-01)\tAcc@1  91.41 ( 95.40)\tAcc@5 100.00 ( 99.82)\n",
            "Epoch: [67][ 90/391]\tTime  0.300 ( 0.232)\tLoss 1.3407e-01 (1.6209e-01)\tAcc@1  95.31 ( 95.24)\tAcc@5 100.00 ( 99.87)\n",
            "Epoch: [67][120/391]\tTime  0.314 ( 0.232)\tLoss 1.8594e-01 (1.6520e-01)\tAcc@1  92.19 ( 95.11)\tAcc@5 100.00 ( 99.86)\n",
            "Epoch: [67][150/391]\tTime  0.164 ( 0.230)\tLoss 1.3717e-01 (1.6573e-01)\tAcc@1  96.88 ( 95.05)\tAcc@5 100.00 ( 99.88)\n",
            "Epoch: [67][180/391]\tTime  0.239 ( 0.230)\tLoss 1.7372e-01 (1.6850e-01)\tAcc@1  96.09 ( 94.99)\tAcc@5 100.00 ( 99.87)\n",
            "Epoch: [67][210/391]\tTime  0.304 ( 0.230)\tLoss 9.9591e-02 (1.7077e-01)\tAcc@1  97.66 ( 94.88)\tAcc@5 100.00 ( 99.88)\n",
            "Epoch: [67][240/391]\tTime  0.246 ( 0.229)\tLoss 1.9748e-01 (1.7336e-01)\tAcc@1  92.19 ( 94.80)\tAcc@5 100.00 ( 99.87)\n",
            "Epoch: [67][270/391]\tTime  0.314 ( 0.229)\tLoss 1.1208e-01 (1.7638e-01)\tAcc@1  97.66 ( 94.74)\tAcc@5 100.00 ( 99.86)\n",
            "Epoch: [67][300/391]\tTime  0.322 ( 0.229)\tLoss 2.1703e-01 (1.7895e-01)\tAcc@1  92.97 ( 94.68)\tAcc@5 100.00 ( 99.86)\n",
            "Epoch: [67][330/391]\tTime  0.191 ( 0.228)\tLoss 3.7104e-01 (1.8159e-01)\tAcc@1  90.62 ( 94.62)\tAcc@5 100.00 ( 99.86)\n",
            "Epoch: [67][360/391]\tTime  0.276 ( 0.228)\tLoss 1.1578e-01 (1.8284e-01)\tAcc@1  98.44 ( 94.60)\tAcc@5 100.00 ( 99.85)\n",
            "Epoch: [67][390/391]\tTime  0.147 ( 0.227)\tLoss 2.5532e-01 (1.8401e-01)\tAcc@1  91.25 ( 94.54)\tAcc@5  98.75 ( 99.85)\n",
            "==> Train Accuracy: Acc@1 94.544 || Acc@5 99.848\n",
            "==> Test Accuracy:  Acc@1 73.370 || Acc@5 93.010\n",
            "==> 93.01 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 68, lr: 0.020000000000000004 -----\n",
            "Epoch: [68][  0/391]\tTime  0.557 ( 0.557)\tLoss 1.1045e-01 (1.1045e-01)\tAcc@1  96.09 ( 96.09)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [68][ 30/391]\tTime  0.174 ( 0.238)\tLoss 1.6690e-01 (1.4583e-01)\tAcc@1  93.75 ( 95.74)\tAcc@5 100.00 ( 99.92)\n",
            "Epoch: [68][ 60/391]\tTime  0.164 ( 0.235)\tLoss 1.7646e-01 (1.5263e-01)\tAcc@1  95.31 ( 95.52)\tAcc@5 100.00 ( 99.88)\n",
            "Epoch: [68][ 90/391]\tTime  0.267 ( 0.232)\tLoss 2.3627e-01 (1.5352e-01)\tAcc@1  92.97 ( 95.53)\tAcc@5 100.00 ( 99.87)\n",
            "Epoch: [68][120/391]\tTime  0.162 ( 0.232)\tLoss 1.2117e-01 (1.5440e-01)\tAcc@1  96.88 ( 95.48)\tAcc@5 100.00 ( 99.88)\n",
            "Epoch: [68][150/391]\tTime  0.164 ( 0.231)\tLoss 1.3141e-01 (1.5493e-01)\tAcc@1  96.88 ( 95.60)\tAcc@5 100.00 ( 99.88)\n",
            "Epoch: [68][180/391]\tTime  0.248 ( 0.231)\tLoss 1.3877e-01 (1.5536e-01)\tAcc@1  96.09 ( 95.58)\tAcc@5 100.00 ( 99.89)\n",
            "Epoch: [68][210/391]\tTime  0.180 ( 0.229)\tLoss 1.9494e-01 (1.5838e-01)\tAcc@1  94.53 ( 95.51)\tAcc@5 100.00 ( 99.88)\n",
            "Epoch: [68][240/391]\tTime  0.194 ( 0.229)\tLoss 2.0221e-01 (1.6242e-01)\tAcc@1  94.53 ( 95.34)\tAcc@5 100.00 ( 99.87)\n",
            "Epoch: [68][270/391]\tTime  0.156 ( 0.229)\tLoss 1.6573e-01 (1.6334e-01)\tAcc@1  96.09 ( 95.34)\tAcc@5 100.00 ( 99.85)\n",
            "Epoch: [68][300/391]\tTime  0.169 ( 0.229)\tLoss 2.1245e-01 (1.6495e-01)\tAcc@1  93.75 ( 95.27)\tAcc@5  99.22 ( 99.84)\n",
            "Epoch: [68][330/391]\tTime  0.174 ( 0.229)\tLoss 2.5930e-01 (1.6707e-01)\tAcc@1  89.84 ( 95.14)\tAcc@5 100.00 ( 99.85)\n",
            "Epoch: [68][360/391]\tTime  0.310 ( 0.229)\tLoss 2.1199e-01 (1.6832e-01)\tAcc@1  93.75 ( 95.09)\tAcc@5 100.00 ( 99.85)\n",
            "Epoch: [68][390/391]\tTime  0.148 ( 0.229)\tLoss 1.3697e-01 (1.7091e-01)\tAcc@1  97.50 ( 95.01)\tAcc@5 100.00 ( 99.85)\n",
            "==> Train Accuracy: Acc@1 95.012 || Acc@5 99.848\n",
            "==> Test Accuracy:  Acc@1 72.240 || Acc@5 92.190\n",
            "==> 93.49 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 69, lr: 0.020000000000000004 -----\n",
            "Epoch: [69][  0/391]\tTime  0.590 ( 0.590)\tLoss 1.4636e-01 (1.4636e-01)\tAcc@1  96.09 ( 96.09)\tAcc@5  99.22 ( 99.22)\n",
            "Epoch: [69][ 30/391]\tTime  0.175 ( 0.231)\tLoss 1.2064e-01 (1.4569e-01)\tAcc@1  96.88 ( 95.99)\tAcc@5 100.00 ( 99.90)\n",
            "Epoch: [69][ 60/391]\tTime  0.166 ( 0.232)\tLoss 1.2488e-01 (1.4254e-01)\tAcc@1  97.66 ( 96.13)\tAcc@5 100.00 ( 99.94)\n",
            "Epoch: [69][ 90/391]\tTime  0.165 ( 0.232)\tLoss 1.0903e-01 (1.4515e-01)\tAcc@1  97.66 ( 96.05)\tAcc@5 100.00 ( 99.95)\n",
            "Epoch: [69][120/391]\tTime  0.201 ( 0.231)\tLoss 1.2166e-01 (1.4856e-01)\tAcc@1  97.66 ( 95.88)\tAcc@5 100.00 ( 99.95)\n",
            "Epoch: [69][150/391]\tTime  0.264 ( 0.230)\tLoss 1.3115e-01 (1.5188e-01)\tAcc@1  96.88 ( 95.75)\tAcc@5 100.00 ( 99.95)\n",
            "Epoch: [69][180/391]\tTime  0.179 ( 0.230)\tLoss 2.1686e-01 (1.5646e-01)\tAcc@1  93.75 ( 95.61)\tAcc@5 100.00 ( 99.94)\n",
            "Epoch: [69][210/391]\tTime  0.173 ( 0.230)\tLoss 1.4631e-01 (1.5805e-01)\tAcc@1  97.66 ( 95.51)\tAcc@5 100.00 ( 99.93)\n",
            "Epoch: [69][240/391]\tTime  0.163 ( 0.230)\tLoss 2.0089e-01 (1.5930e-01)\tAcc@1  92.97 ( 95.44)\tAcc@5 100.00 ( 99.92)\n",
            "Epoch: [69][270/391]\tTime  0.156 ( 0.230)\tLoss 1.7209e-01 (1.6032e-01)\tAcc@1  92.97 ( 95.41)\tAcc@5 100.00 ( 99.92)\n",
            "Epoch: [69][300/391]\tTime  0.167 ( 0.230)\tLoss 2.0239e-01 (1.6315e-01)\tAcc@1  93.75 ( 95.32)\tAcc@5 100.00 ( 99.91)\n",
            "Epoch: [69][330/391]\tTime  0.264 ( 0.230)\tLoss 1.4512e-01 (1.6592e-01)\tAcc@1  95.31 ( 95.23)\tAcc@5 100.00 ( 99.89)\n",
            "Epoch: [69][360/391]\tTime  0.190 ( 0.230)\tLoss 2.4573e-01 (1.6942e-01)\tAcc@1  93.75 ( 95.11)\tAcc@5  98.44 ( 99.89)\n",
            "Epoch: [69][390/391]\tTime  0.149 ( 0.230)\tLoss 2.1807e-01 (1.7186e-01)\tAcc@1  93.75 ( 95.00)\tAcc@5  98.75 ( 99.88)\n",
            "==> Train Accuracy: Acc@1 95.004 || Acc@5 99.884\n",
            "==> Test Accuracy:  Acc@1 71.710 || Acc@5 92.030\n",
            "==> 94.02 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 70, lr: 0.020000000000000004 -----\n",
            "Epoch: [70][  0/391]\tTime  0.571 ( 0.571)\tLoss 1.2613e-01 (1.2613e-01)\tAcc@1  96.09 ( 96.09)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [70][ 30/391]\tTime  0.266 ( 0.236)\tLoss 2.2344e-01 (1.4801e-01)\tAcc@1  92.19 ( 95.61)\tAcc@5 100.00 ( 99.92)\n",
            "Epoch: [70][ 60/391]\tTime  0.175 ( 0.235)\tLoss 1.4448e-01 (1.4493e-01)\tAcc@1  96.88 ( 95.76)\tAcc@5 100.00 ( 99.92)\n",
            "Epoch: [70][ 90/391]\tTime  0.162 ( 0.233)\tLoss 1.2462e-01 (1.4836e-01)\tAcc@1  96.09 ( 95.69)\tAcc@5 100.00 ( 99.92)\n",
            "Epoch: [70][120/391]\tTime  0.195 ( 0.230)\tLoss 1.7901e-01 (1.5192e-01)\tAcc@1  96.88 ( 95.54)\tAcc@5 100.00 ( 99.91)\n",
            "Epoch: [70][150/391]\tTime  0.286 ( 0.231)\tLoss 1.0687e-01 (1.5514e-01)\tAcc@1  96.88 ( 95.51)\tAcc@5 100.00 ( 99.90)\n",
            "Epoch: [70][180/391]\tTime  0.170 ( 0.230)\tLoss 1.6741e-01 (1.5972e-01)\tAcc@1  93.75 ( 95.33)\tAcc@5 100.00 ( 99.90)\n",
            "Epoch: [70][210/391]\tTime  0.183 ( 0.229)\tLoss 1.6900e-01 (1.6121e-01)\tAcc@1  93.75 ( 95.29)\tAcc@5 100.00 ( 99.90)\n",
            "Epoch: [70][240/391]\tTime  0.166 ( 0.229)\tLoss 2.3182e-01 (1.6455e-01)\tAcc@1  93.75 ( 95.20)\tAcc@5  99.22 ( 99.89)\n",
            "Epoch: [70][270/391]\tTime  0.172 ( 0.230)\tLoss 2.2640e-01 (1.6725e-01)\tAcc@1  93.75 ( 95.10)\tAcc@5 100.00 ( 99.89)\n",
            "Epoch: [70][300/391]\tTime  0.173 ( 0.230)\tLoss 1.3370e-01 (1.6816e-01)\tAcc@1  96.88 ( 95.07)\tAcc@5 100.00 ( 99.89)\n",
            "Epoch: [70][330/391]\tTime  0.164 ( 0.230)\tLoss 1.5701e-01 (1.6952e-01)\tAcc@1  95.31 ( 95.00)\tAcc@5  99.22 ( 99.88)\n",
            "Epoch: [70][360/391]\tTime  0.162 ( 0.230)\tLoss 2.1467e-01 (1.7226e-01)\tAcc@1  92.97 ( 94.89)\tAcc@5 100.00 ( 99.88)\n",
            "Epoch: [70][390/391]\tTime  0.149 ( 0.229)\tLoss 1.8228e-01 (1.7539e-01)\tAcc@1  93.75 ( 94.77)\tAcc@5 100.00 ( 99.87)\n",
            "==> Train Accuracy: Acc@1 94.770 || Acc@5 99.874\n",
            "==> Test Accuracy:  Acc@1 70.800 || Acc@5 91.720\n",
            "==> 93.74 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 71, lr: 0.020000000000000004 -----\n",
            "Epoch: [71][  0/391]\tTime  0.594 ( 0.594)\tLoss 1.6177e-01 (1.6177e-01)\tAcc@1  97.66 ( 97.66)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [71][ 30/391]\tTime  0.269 ( 0.235)\tLoss 1.3494e-01 (1.5765e-01)\tAcc@1  96.88 ( 95.59)\tAcc@5 100.00 ( 99.92)\n",
            "Epoch: [71][ 60/391]\tTime  0.281 ( 0.232)\tLoss 1.2837e-01 (1.5767e-01)\tAcc@1  95.31 ( 95.67)\tAcc@5 100.00 ( 99.92)\n",
            "Epoch: [71][ 90/391]\tTime  0.173 ( 0.229)\tLoss 1.8874e-01 (1.5124e-01)\tAcc@1  93.75 ( 95.91)\tAcc@5 100.00 ( 99.92)\n",
            "Epoch: [71][120/391]\tTime  0.216 ( 0.228)\tLoss 1.8629e-01 (1.5704e-01)\tAcc@1  96.09 ( 95.57)\tAcc@5 100.00 ( 99.92)\n",
            "Epoch: [71][150/391]\tTime  0.178 ( 0.227)\tLoss 1.5188e-01 (1.5867e-01)\tAcc@1  96.09 ( 95.53)\tAcc@5 100.00 ( 99.92)\n",
            "Epoch: [71][180/391]\tTime  0.211 ( 0.226)\tLoss 2.0259e-01 (1.6381e-01)\tAcc@1  96.09 ( 95.37)\tAcc@5  99.22 ( 99.88)\n",
            "Epoch: [71][210/391]\tTime  0.281 ( 0.226)\tLoss 1.1107e-01 (1.6661e-01)\tAcc@1  96.09 ( 95.27)\tAcc@5 100.00 ( 99.88)\n",
            "Epoch: [71][240/391]\tTime  0.277 ( 0.226)\tLoss 1.7627e-01 (1.6920e-01)\tAcc@1  95.31 ( 95.12)\tAcc@5  99.22 ( 99.88)\n",
            "Epoch: [71][270/391]\tTime  0.154 ( 0.227)\tLoss 1.2715e-01 (1.7142e-01)\tAcc@1  94.53 ( 94.97)\tAcc@5 100.00 ( 99.88)\n",
            "Epoch: [71][300/391]\tTime  0.160 ( 0.227)\tLoss 2.2440e-01 (1.7450e-01)\tAcc@1  92.19 ( 94.85)\tAcc@5 100.00 ( 99.87)\n",
            "Epoch: [71][330/391]\tTime  0.176 ( 0.228)\tLoss 1.9648e-01 (1.7762e-01)\tAcc@1  92.97 ( 94.74)\tAcc@5 100.00 ( 99.87)\n",
            "Epoch: [71][360/391]\tTime  0.174 ( 0.228)\tLoss 2.5560e-01 (1.8135e-01)\tAcc@1  92.19 ( 94.59)\tAcc@5  99.22 ( 99.86)\n",
            "Epoch: [71][390/391]\tTime  0.140 ( 0.228)\tLoss 1.9650e-01 (1.8380e-01)\tAcc@1  96.25 ( 94.52)\tAcc@5  98.75 ( 99.86)\n",
            "==> Train Accuracy: Acc@1 94.520 || Acc@5 99.862\n",
            "==> Test Accuracy:  Acc@1 71.370 || Acc@5 92.030\n",
            "==> 93.27 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 72, lr: 0.020000000000000004 -----\n",
            "Epoch: [72][  0/391]\tTime  0.586 ( 0.586)\tLoss 1.6827e-01 (1.6827e-01)\tAcc@1  96.88 ( 96.88)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [72][ 30/391]\tTime  0.320 ( 0.240)\tLoss 1.7547e-01 (1.6135e-01)\tAcc@1  92.97 ( 95.41)\tAcc@5 100.00 ( 99.97)\n",
            "Epoch: [72][ 60/391]\tTime  0.167 ( 0.231)\tLoss 1.3833e-01 (1.5702e-01)\tAcc@1  96.88 ( 95.40)\tAcc@5 100.00 ( 99.95)\n",
            "Epoch: [72][ 90/391]\tTime  0.261 ( 0.229)\tLoss 1.6459e-01 (1.6011e-01)\tAcc@1  96.09 ( 95.35)\tAcc@5 100.00 ( 99.87)\n",
            "Epoch: [72][120/391]\tTime  0.180 ( 0.227)\tLoss 2.2602e-01 (1.6260e-01)\tAcc@1  92.19 ( 95.27)\tAcc@5 100.00 ( 99.86)\n",
            "Epoch: [72][150/391]\tTime  0.285 ( 0.227)\tLoss 1.6096e-01 (1.6556e-01)\tAcc@1  94.53 ( 95.14)\tAcc@5 100.00 ( 99.89)\n",
            "Epoch: [72][180/391]\tTime  0.335 ( 0.229)\tLoss 1.9559e-01 (1.6834e-01)\tAcc@1  95.31 ( 95.05)\tAcc@5 100.00 ( 99.88)\n",
            "Epoch: [72][210/391]\tTime  0.288 ( 0.230)\tLoss 2.5910e-01 (1.7268e-01)\tAcc@1  88.28 ( 94.87)\tAcc@5 100.00 ( 99.87)\n",
            "Epoch: [72][240/391]\tTime  0.269 ( 0.229)\tLoss 2.1208e-01 (1.7844e-01)\tAcc@1  94.53 ( 94.66)\tAcc@5 100.00 ( 99.87)\n",
            "Epoch: [72][270/391]\tTime  0.350 ( 0.230)\tLoss 2.5966e-01 (1.8491e-01)\tAcc@1  91.41 ( 94.40)\tAcc@5 100.00 ( 99.87)\n",
            "Epoch: [72][300/391]\tTime  0.288 ( 0.229)\tLoss 1.6413e-01 (1.8921e-01)\tAcc@1  93.75 ( 94.25)\tAcc@5 100.00 ( 99.86)\n",
            "Epoch: [72][330/391]\tTime  0.198 ( 0.229)\tLoss 2.4070e-01 (1.9102e-01)\tAcc@1  90.62 ( 94.21)\tAcc@5 100.00 ( 99.85)\n",
            "Epoch: [72][360/391]\tTime  0.171 ( 0.229)\tLoss 2.2219e-01 (1.9447e-01)\tAcc@1  93.75 ( 94.10)\tAcc@5  99.22 ( 99.84)\n",
            "Epoch: [72][390/391]\tTime  0.147 ( 0.228)\tLoss 3.0867e-01 (1.9807e-01)\tAcc@1  87.50 ( 93.98)\tAcc@5 100.00 ( 99.83)\n",
            "==> Train Accuracy: Acc@1 93.982 || Acc@5 99.832\n",
            "==> Test Accuracy:  Acc@1 70.440 || Acc@5 91.530\n",
            "==> 93.31 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 73, lr: 0.020000000000000004 -----\n",
            "Epoch: [73][  0/391]\tTime  0.570 ( 0.570)\tLoss 3.3207e-01 (3.3207e-01)\tAcc@1  88.28 ( 88.28)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [73][ 30/391]\tTime  0.335 ( 0.243)\tLoss 2.0721e-01 (1.8029e-01)\tAcc@1  93.75 ( 95.16)\tAcc@5 100.00 ( 99.95)\n",
            "Epoch: [73][ 60/391]\tTime  0.290 ( 0.235)\tLoss 1.5432e-01 (1.6818e-01)\tAcc@1  96.09 ( 95.36)\tAcc@5 100.00 ( 99.94)\n",
            "Epoch: [73][ 90/391]\tTime  0.177 ( 0.236)\tLoss 1.3438e-01 (1.6754e-01)\tAcc@1  93.75 ( 95.19)\tAcc@5 100.00 ( 99.92)\n",
            "Epoch: [73][120/391]\tTime  0.160 ( 0.234)\tLoss 1.8870e-01 (1.7131e-01)\tAcc@1  96.09 ( 95.02)\tAcc@5 100.00 ( 99.91)\n",
            "Epoch: [73][150/391]\tTime  0.247 ( 0.233)\tLoss 1.5321e-01 (1.7709e-01)\tAcc@1  93.75 ( 94.77)\tAcc@5 100.00 ( 99.89)\n",
            "Epoch: [73][180/391]\tTime  0.158 ( 0.232)\tLoss 1.6839e-01 (1.8149e-01)\tAcc@1  94.53 ( 94.60)\tAcc@5 100.00 ( 99.86)\n",
            "Epoch: [73][210/391]\tTime  0.162 ( 0.233)\tLoss 2.8256e-01 (1.8302e-01)\tAcc@1  90.62 ( 94.51)\tAcc@5 100.00 ( 99.85)\n",
            "Epoch: [73][240/391]\tTime  0.172 ( 0.232)\tLoss 1.8741e-01 (1.8504e-01)\tAcc@1  95.31 ( 94.47)\tAcc@5  99.22 ( 99.85)\n",
            "Epoch: [73][270/391]\tTime  0.327 ( 0.232)\tLoss 2.5371e-01 (1.8661e-01)\tAcc@1  90.62 ( 94.39)\tAcc@5 100.00 ( 99.85)\n",
            "Epoch: [73][300/391]\tTime  0.233 ( 0.231)\tLoss 2.1142e-01 (1.9022e-01)\tAcc@1  92.97 ( 94.30)\tAcc@5 100.00 ( 99.85)\n",
            "Epoch: [73][330/391]\tTime  0.177 ( 0.231)\tLoss 1.5502e-01 (1.9269e-01)\tAcc@1  93.75 ( 94.22)\tAcc@5 100.00 ( 99.84)\n",
            "Epoch: [73][360/391]\tTime  0.194 ( 0.231)\tLoss 2.7551e-01 (1.9719e-01)\tAcc@1  89.84 ( 94.07)\tAcc@5  98.44 ( 99.82)\n",
            "Epoch: [73][390/391]\tTime  0.142 ( 0.230)\tLoss 2.3063e-01 (2.0096e-01)\tAcc@1  91.25 ( 93.92)\tAcc@5 100.00 ( 99.82)\n",
            "==> Train Accuracy: Acc@1 93.924 || Acc@5 99.820\n",
            "==> Test Accuracy:  Acc@1 70.090 || Acc@5 91.460\n",
            "==> 94.17 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 74, lr: 0.020000000000000004 -----\n",
            "Epoch: [74][  0/391]\tTime  0.561 ( 0.561)\tLoss 1.2546e-01 (1.2546e-01)\tAcc@1  96.88 ( 96.88)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [74][ 30/391]\tTime  0.184 ( 0.236)\tLoss 2.4359e-01 (2.0165e-01)\tAcc@1  93.75 ( 93.70)\tAcc@5  98.44 ( 99.75)\n",
            "Epoch: [74][ 60/391]\tTime  0.162 ( 0.234)\tLoss 1.3557e-01 (1.9728e-01)\tAcc@1  94.53 ( 93.87)\tAcc@5 100.00 ( 99.77)\n",
            "Epoch: [74][ 90/391]\tTime  0.267 ( 0.230)\tLoss 1.0092e-01 (1.8718e-01)\tAcc@1  97.66 ( 94.38)\tAcc@5 100.00 ( 99.81)\n",
            "Epoch: [74][120/391]\tTime  0.176 ( 0.229)\tLoss 1.3936e-01 (1.8325e-01)\tAcc@1  95.31 ( 94.50)\tAcc@5 100.00 ( 99.81)\n",
            "Epoch: [74][150/391]\tTime  0.173 ( 0.228)\tLoss 1.3759e-01 (1.8190e-01)\tAcc@1  97.66 ( 94.60)\tAcc@5 100.00 ( 99.83)\n",
            "Epoch: [74][180/391]\tTime  0.175 ( 0.228)\tLoss 1.5871e-01 (1.8640e-01)\tAcc@1  96.88 ( 94.42)\tAcc@5 100.00 ( 99.84)\n",
            "Epoch: [74][210/391]\tTime  0.180 ( 0.227)\tLoss 1.4170e-01 (1.8870e-01)\tAcc@1  97.66 ( 94.34)\tAcc@5 100.00 ( 99.85)\n",
            "Epoch: [74][240/391]\tTime  0.187 ( 0.227)\tLoss 2.6639e-01 (1.9381e-01)\tAcc@1  91.41 ( 94.19)\tAcc@5  99.22 ( 99.83)\n",
            "Epoch: [74][270/391]\tTime  0.318 ( 0.226)\tLoss 2.9891e-01 (1.9925e-01)\tAcc@1  86.72 ( 93.97)\tAcc@5 100.00 ( 99.83)\n",
            "Epoch: [74][300/391]\tTime  0.166 ( 0.226)\tLoss 2.6728e-01 (2.0066e-01)\tAcc@1  91.41 ( 93.91)\tAcc@5 100.00 ( 99.83)\n",
            "Epoch: [74][330/391]\tTime  0.261 ( 0.226)\tLoss 2.7695e-01 (2.0394e-01)\tAcc@1  93.75 ( 93.82)\tAcc@5  99.22 ( 99.81)\n",
            "Epoch: [74][360/391]\tTime  0.200 ( 0.225)\tLoss 3.9192e-01 (2.0743e-01)\tAcc@1  83.59 ( 93.69)\tAcc@5  99.22 ( 99.80)\n",
            "Epoch: [74][390/391]\tTime  0.150 ( 0.225)\tLoss 3.3875e-01 (2.1118e-01)\tAcc@1  86.25 ( 93.57)\tAcc@5 100.00 ( 99.79)\n",
            "==> Train Accuracy: Acc@1 93.566 || Acc@5 99.786\n",
            "==> Test Accuracy:  Acc@1 69.150 || Acc@5 90.970\n",
            "==> 92.30 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 75, lr: 0.020000000000000004 -----\n",
            "Epoch: [75][  0/391]\tTime  0.595 ( 0.595)\tLoss 2.1379e-01 (2.1379e-01)\tAcc@1  91.41 ( 91.41)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [75][ 30/391]\tTime  0.175 ( 0.240)\tLoss 1.6317e-01 (1.8513e-01)\tAcc@1  96.09 ( 94.88)\tAcc@5 100.00 ( 99.80)\n",
            "Epoch: [75][ 60/391]\tTime  0.165 ( 0.235)\tLoss 3.0504e-01 (1.9205e-01)\tAcc@1  92.97 ( 94.51)\tAcc@5 100.00 ( 99.85)\n",
            "Epoch: [75][ 90/391]\tTime  0.173 ( 0.233)\tLoss 1.7455e-01 (1.8910e-01)\tAcc@1  94.53 ( 94.55)\tAcc@5 100.00 ( 99.82)\n",
            "Epoch: [75][120/391]\tTime  0.164 ( 0.231)\tLoss 1.6990e-01 (1.8383e-01)\tAcc@1  96.88 ( 94.69)\tAcc@5 100.00 ( 99.84)\n",
            "Epoch: [75][150/391]\tTime  0.173 ( 0.230)\tLoss 1.7568e-01 (1.8527e-01)\tAcc@1  95.31 ( 94.65)\tAcc@5 100.00 ( 99.84)\n",
            "Epoch: [75][180/391]\tTime  0.176 ( 0.229)\tLoss 2.1664e-01 (1.8605e-01)\tAcc@1  92.97 ( 94.57)\tAcc@5 100.00 ( 99.85)\n",
            "Epoch: [75][210/391]\tTime  0.309 ( 0.229)\tLoss 2.0453e-01 (1.8743e-01)\tAcc@1  92.19 ( 94.45)\tAcc@5 100.00 ( 99.84)\n",
            "Epoch: [75][240/391]\tTime  0.163 ( 0.228)\tLoss 2.0599e-01 (1.9058e-01)\tAcc@1  92.19 ( 94.22)\tAcc@5 100.00 ( 99.85)\n",
            "Epoch: [75][270/391]\tTime  0.263 ( 0.228)\tLoss 1.3617e-01 (1.9537e-01)\tAcc@1  98.44 ( 94.04)\tAcc@5 100.00 ( 99.85)\n",
            "Epoch: [75][300/391]\tTime  0.302 ( 0.228)\tLoss 1.9878e-01 (2.0114e-01)\tAcc@1  95.31 ( 93.87)\tAcc@5  99.22 ( 99.83)\n",
            "Epoch: [75][330/391]\tTime  0.260 ( 0.228)\tLoss 2.5329e-01 (2.0581e-01)\tAcc@1  92.19 ( 93.72)\tAcc@5 100.00 ( 99.81)\n",
            "Epoch: [75][360/391]\tTime  0.215 ( 0.227)\tLoss 1.7048e-01 (2.0854e-01)\tAcc@1  96.09 ( 93.65)\tAcc@5 100.00 ( 99.80)\n",
            "Epoch: [75][390/391]\tTime  0.150 ( 0.227)\tLoss 2.9240e-01 (2.1200e-01)\tAcc@1  90.00 ( 93.57)\tAcc@5 100.00 ( 99.80)\n",
            "==> Train Accuracy: Acc@1 93.574 || Acc@5 99.798\n",
            "==> Test Accuracy:  Acc@1 69.650 || Acc@5 90.660\n",
            "==> 92.99 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 76, lr: 0.020000000000000004 -----\n",
            "Epoch: [76][  0/391]\tTime  0.563 ( 0.563)\tLoss 1.3161e-01 (1.3161e-01)\tAcc@1  96.88 ( 96.88)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [76][ 30/391]\tTime  0.166 ( 0.233)\tLoss 1.4361e-01 (1.9220e-01)\tAcc@1  94.53 ( 94.41)\tAcc@5 100.00 ( 99.87)\n",
            "Epoch: [76][ 60/391]\tTime  0.171 ( 0.225)\tLoss 2.1265e-01 (1.8507e-01)\tAcc@1  92.97 ( 94.57)\tAcc@5 100.00 ( 99.90)\n",
            "Epoch: [76][ 90/391]\tTime  0.220 ( 0.224)\tLoss 2.3246e-01 (1.8669e-01)\tAcc@1  92.97 ( 94.51)\tAcc@5  99.22 ( 99.89)\n",
            "Epoch: [76][120/391]\tTime  0.256 ( 0.223)\tLoss 2.4244e-01 (1.8590e-01)\tAcc@1  92.19 ( 94.49)\tAcc@5  99.22 ( 99.88)\n",
            "Epoch: [76][150/391]\tTime  0.348 ( 0.225)\tLoss 2.0216e-01 (1.8771e-01)\tAcc@1  89.84 ( 94.45)\tAcc@5 100.00 ( 99.86)\n",
            "Epoch: [76][180/391]\tTime  0.167 ( 0.224)\tLoss 2.1085e-01 (1.8859e-01)\tAcc@1  94.53 ( 94.37)\tAcc@5 100.00 ( 99.84)\n",
            "Epoch: [76][210/391]\tTime  0.175 ( 0.223)\tLoss 2.0596e-01 (1.8947e-01)\tAcc@1  92.19 ( 94.28)\tAcc@5 100.00 ( 99.84)\n",
            "Epoch: [76][240/391]\tTime  0.169 ( 0.224)\tLoss 1.5758e-01 (1.9422e-01)\tAcc@1  96.09 ( 94.13)\tAcc@5  99.22 ( 99.82)\n",
            "Epoch: [76][270/391]\tTime  0.181 ( 0.224)\tLoss 2.2118e-01 (1.9673e-01)\tAcc@1  92.19 ( 94.07)\tAcc@5 100.00 ( 99.83)\n",
            "Epoch: [76][300/391]\tTime  0.191 ( 0.225)\tLoss 2.2703e-01 (2.0018e-01)\tAcc@1  95.31 ( 93.96)\tAcc@5 100.00 ( 99.82)\n",
            "Epoch: [76][330/391]\tTime  0.163 ( 0.225)\tLoss 3.0975e-01 (2.0407e-01)\tAcc@1  91.41 ( 93.84)\tAcc@5 100.00 ( 99.80)\n",
            "Epoch: [76][360/391]\tTime  0.167 ( 0.225)\tLoss 1.8264e-01 (2.0834e-01)\tAcc@1  96.09 ( 93.71)\tAcc@5 100.00 ( 99.80)\n",
            "Epoch: [76][390/391]\tTime  0.152 ( 0.225)\tLoss 4.2903e-01 (2.1387e-01)\tAcc@1  87.50 ( 93.50)\tAcc@5 100.00 ( 99.79)\n",
            "==> Train Accuracy: Acc@1 93.496 || Acc@5 99.790\n",
            "==> Test Accuracy:  Acc@1 69.860 || Acc@5 91.220\n",
            "==> 92.07 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 77, lr: 0.020000000000000004 -----\n",
            "Epoch: [77][  0/391]\tTime  0.601 ( 0.601)\tLoss 1.2622e-01 (1.2622e-01)\tAcc@1  96.88 ( 96.88)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [77][ 30/391]\tTime  0.181 ( 0.230)\tLoss 1.8045e-01 (2.0189e-01)\tAcc@1  95.31 ( 94.08)\tAcc@5  99.22 ( 99.70)\n",
            "Epoch: [77][ 60/391]\tTime  0.168 ( 0.229)\tLoss 1.6496e-01 (1.9822e-01)\tAcc@1  95.31 ( 94.24)\tAcc@5 100.00 ( 99.76)\n",
            "Epoch: [77][ 90/391]\tTime  0.180 ( 0.230)\tLoss 2.4109e-01 (1.9869e-01)\tAcc@1  92.19 ( 94.17)\tAcc@5  99.22 ( 99.80)\n",
            "Epoch: [77][120/391]\tTime  0.159 ( 0.228)\tLoss 1.8739e-01 (2.0318e-01)\tAcc@1  92.97 ( 94.00)\tAcc@5 100.00 ( 99.80)\n",
            "Epoch: [77][150/391]\tTime  0.155 ( 0.226)\tLoss 2.0284e-01 (2.0536e-01)\tAcc@1  93.75 ( 93.94)\tAcc@5 100.00 ( 99.80)\n",
            "Epoch: [77][180/391]\tTime  0.185 ( 0.227)\tLoss 3.2740e-01 (2.0561e-01)\tAcc@1  89.84 ( 93.93)\tAcc@5  98.44 ( 99.79)\n",
            "Epoch: [77][210/391]\tTime  0.154 ( 0.227)\tLoss 3.3661e-01 (2.0800e-01)\tAcc@1  88.28 ( 93.78)\tAcc@5  99.22 ( 99.78)\n",
            "Epoch: [77][240/391]\tTime  0.178 ( 0.226)\tLoss 2.5679e-01 (2.1201e-01)\tAcc@1  94.53 ( 93.64)\tAcc@5 100.00 ( 99.78)\n",
            "Epoch: [77][270/391]\tTime  0.251 ( 0.226)\tLoss 2.7875e-01 (2.1667e-01)\tAcc@1  90.62 ( 93.42)\tAcc@5  98.44 ( 99.77)\n",
            "Epoch: [77][300/391]\tTime  0.355 ( 0.227)\tLoss 1.4530e-01 (2.1889e-01)\tAcc@1  96.09 ( 93.31)\tAcc@5 100.00 ( 99.78)\n",
            "Epoch: [77][330/391]\tTime  0.187 ( 0.226)\tLoss 2.1046e-01 (2.2283e-01)\tAcc@1  93.75 ( 93.20)\tAcc@5  99.22 ( 99.77)\n",
            "Epoch: [77][360/391]\tTime  0.174 ( 0.226)\tLoss 2.8388e-01 (2.2744e-01)\tAcc@1  89.84 ( 93.04)\tAcc@5 100.00 ( 99.75)\n",
            "Epoch: [77][390/391]\tTime  0.150 ( 0.226)\tLoss 3.6984e-01 (2.2957e-01)\tAcc@1  87.50 ( 93.00)\tAcc@5  98.75 ( 99.76)\n",
            "==> Train Accuracy: Acc@1 93.002 || Acc@5 99.760\n",
            "==> Test Accuracy:  Acc@1 70.480 || Acc@5 90.940\n",
            "==> 92.55 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 78, lr: 0.020000000000000004 -----\n",
            "Epoch: [78][  0/391]\tTime  0.560 ( 0.560)\tLoss 1.4589e-01 (1.4589e-01)\tAcc@1  96.09 ( 96.09)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [78][ 30/391]\tTime  0.168 ( 0.235)\tLoss 2.3229e-01 (2.1153e-01)\tAcc@1  91.41 ( 93.62)\tAcc@5  99.22 ( 99.72)\n",
            "Epoch: [78][ 60/391]\tTime  0.168 ( 0.232)\tLoss 1.8118e-01 (2.0524e-01)\tAcc@1  93.75 ( 93.97)\tAcc@5 100.00 ( 99.71)\n",
            "Epoch: [78][ 90/391]\tTime  0.159 ( 0.229)\tLoss 9.5868e-02 (2.0063e-01)\tAcc@1  97.66 ( 94.05)\tAcc@5 100.00 ( 99.78)\n",
            "Epoch: [78][120/391]\tTime  0.227 ( 0.227)\tLoss 2.6818e-01 (2.0121e-01)\tAcc@1  92.19 ( 94.03)\tAcc@5 100.00 ( 99.79)\n",
            "Epoch: [78][150/391]\tTime  0.185 ( 0.225)\tLoss 2.5370e-01 (1.9843e-01)\tAcc@1  93.75 ( 94.14)\tAcc@5 100.00 ( 99.79)\n",
            "Epoch: [78][180/391]\tTime  0.166 ( 0.224)\tLoss 2.3553e-01 (2.0233e-01)\tAcc@1  92.97 ( 94.03)\tAcc@5 100.00 ( 99.80)\n",
            "Epoch: [78][210/391]\tTime  0.167 ( 0.225)\tLoss 2.9901e-01 (2.0336e-01)\tAcc@1  89.84 ( 93.91)\tAcc@5 100.00 ( 99.81)\n",
            "Epoch: [78][240/391]\tTime  0.177 ( 0.226)\tLoss 2.0889e-01 (2.0537e-01)\tAcc@1  92.97 ( 93.79)\tAcc@5 100.00 ( 99.82)\n",
            "Epoch: [78][270/391]\tTime  0.248 ( 0.225)\tLoss 2.0120e-01 (2.0781e-01)\tAcc@1  96.09 ( 93.73)\tAcc@5 100.00 ( 99.82)\n",
            "Epoch: [78][300/391]\tTime  0.270 ( 0.225)\tLoss 1.8249e-01 (2.1218e-01)\tAcc@1  94.53 ( 93.58)\tAcc@5 100.00 ( 99.80)\n",
            "Epoch: [78][330/391]\tTime  0.180 ( 0.225)\tLoss 2.5546e-01 (2.1525e-01)\tAcc@1  92.97 ( 93.49)\tAcc@5 100.00 ( 99.79)\n",
            "Epoch: [78][360/391]\tTime  0.255 ( 0.225)\tLoss 2.3468e-01 (2.1731e-01)\tAcc@1  94.53 ( 93.43)\tAcc@5 100.00 ( 99.78)\n",
            "Epoch: [78][390/391]\tTime  0.150 ( 0.225)\tLoss 3.9830e-01 (2.2252e-01)\tAcc@1  86.25 ( 93.28)\tAcc@5 100.00 ( 99.77)\n",
            "==> Train Accuracy: Acc@1 93.276 || Acc@5 99.770\n",
            "==> Test Accuracy:  Acc@1 68.900 || Acc@5 90.170\n",
            "==> 92.05 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 79, lr: 0.020000000000000004 -----\n",
            "Epoch: [79][  0/391]\tTime  0.550 ( 0.550)\tLoss 1.4414e-01 (1.4414e-01)\tAcc@1  94.53 ( 94.53)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [79][ 30/391]\tTime  0.268 ( 0.235)\tLoss 1.7920e-01 (2.0287e-01)\tAcc@1  94.53 ( 93.90)\tAcc@5 100.00 ( 99.75)\n",
            "Epoch: [79][ 60/391]\tTime  0.308 ( 0.231)\tLoss 2.8720e-01 (2.0203e-01)\tAcc@1  91.41 ( 93.87)\tAcc@5 100.00 ( 99.81)\n",
            "Epoch: [79][ 90/391]\tTime  0.297 ( 0.230)\tLoss 1.7288e-01 (2.0419e-01)\tAcc@1  94.53 ( 93.79)\tAcc@5 100.00 ( 99.83)\n",
            "Epoch: [79][120/391]\tTime  0.277 ( 0.230)\tLoss 2.1308e-01 (2.0624e-01)\tAcc@1  92.97 ( 93.74)\tAcc@5 100.00 ( 99.82)\n",
            "Epoch: [79][150/391]\tTime  0.295 ( 0.228)\tLoss 2.0575e-01 (2.0752e-01)\tAcc@1  94.53 ( 93.70)\tAcc@5 100.00 ( 99.82)\n",
            "Epoch: [79][180/391]\tTime  0.229 ( 0.229)\tLoss 1.6778e-01 (2.0729e-01)\tAcc@1  94.53 ( 93.66)\tAcc@5 100.00 ( 99.84)\n",
            "Epoch: [79][210/391]\tTime  0.263 ( 0.228)\tLoss 1.4991e-01 (2.0818e-01)\tAcc@1  96.09 ( 93.64)\tAcc@5 100.00 ( 99.83)\n",
            "Epoch: [79][240/391]\tTime  0.321 ( 0.229)\tLoss 1.2267e-01 (2.1002e-01)\tAcc@1  95.31 ( 93.60)\tAcc@5 100.00 ( 99.83)\n",
            "Epoch: [79][270/391]\tTime  0.231 ( 0.228)\tLoss 2.2976e-01 (2.1029e-01)\tAcc@1  92.19 ( 93.59)\tAcc@5 100.00 ( 99.84)\n",
            "Epoch: [79][300/391]\tTime  0.306 ( 0.229)\tLoss 2.7244e-01 (2.1384e-01)\tAcc@1  89.84 ( 93.50)\tAcc@5 100.00 ( 99.83)\n",
            "Epoch: [79][330/391]\tTime  0.296 ( 0.228)\tLoss 4.3094e-01 (2.2024e-01)\tAcc@1  85.94 ( 93.29)\tAcc@5  98.44 ( 99.79)\n",
            "Epoch: [79][360/391]\tTime  0.221 ( 0.227)\tLoss 3.0456e-01 (2.2266e-01)\tAcc@1  89.84 ( 93.23)\tAcc@5  98.44 ( 99.78)\n",
            "Epoch: [79][390/391]\tTime  0.149 ( 0.227)\tLoss 2.7358e-01 (2.2663e-01)\tAcc@1  88.75 ( 93.09)\tAcc@5 100.00 ( 99.78)\n",
            "==> Train Accuracy: Acc@1 93.088 || Acc@5 99.778\n",
            "==> Test Accuracy:  Acc@1 68.950 || Acc@5 90.280\n",
            "==> 92.84 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 80, lr: 0.020000000000000004 -----\n",
            "Epoch: [80][  0/391]\tTime  0.558 ( 0.558)\tLoss 1.9317e-01 (1.9317e-01)\tAcc@1  96.88 ( 96.88)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [80][ 30/391]\tTime  0.280 ( 0.235)\tLoss 1.2515e-01 (1.9876e-01)\tAcc@1  97.66 ( 94.41)\tAcc@5 100.00 ( 99.67)\n",
            "Epoch: [80][ 60/391]\tTime  0.362 ( 0.232)\tLoss 2.1571e-01 (1.8563e-01)\tAcc@1  93.75 ( 94.85)\tAcc@5 100.00 ( 99.77)\n",
            "Epoch: [80][ 90/391]\tTime  0.278 ( 0.229)\tLoss 1.6566e-01 (1.8485e-01)\tAcc@1  93.75 ( 94.79)\tAcc@5 100.00 ( 99.80)\n",
            "Epoch: [80][120/391]\tTime  0.258 ( 0.228)\tLoss 2.5231e-01 (1.9168e-01)\tAcc@1  91.41 ( 94.51)\tAcc@5 100.00 ( 99.82)\n",
            "Epoch: [80][150/391]\tTime  0.275 ( 0.228)\tLoss 2.1660e-01 (1.9084e-01)\tAcc@1  92.97 ( 94.45)\tAcc@5 100.00 ( 99.83)\n",
            "Epoch: [80][180/391]\tTime  0.244 ( 0.227)\tLoss 2.1293e-01 (1.9250e-01)\tAcc@1  94.53 ( 94.42)\tAcc@5 100.00 ( 99.82)\n",
            "Epoch: [80][210/391]\tTime  0.158 ( 0.226)\tLoss 3.4735e-01 (1.9367e-01)\tAcc@1  86.72 ( 94.33)\tAcc@5  99.22 ( 99.83)\n",
            "Epoch: [80][240/391]\tTime  0.182 ( 0.227)\tLoss 2.0253e-01 (1.9673e-01)\tAcc@1  94.53 ( 94.14)\tAcc@5 100.00 ( 99.82)\n",
            "Epoch: [80][270/391]\tTime  0.156 ( 0.226)\tLoss 1.2979e-01 (1.9802e-01)\tAcc@1  96.09 ( 94.06)\tAcc@5 100.00 ( 99.82)\n",
            "Epoch: [80][300/391]\tTime  0.202 ( 0.225)\tLoss 1.4486e-01 (2.0128e-01)\tAcc@1  96.09 ( 93.93)\tAcc@5 100.00 ( 99.81)\n",
            "Epoch: [80][330/391]\tTime  0.291 ( 0.225)\tLoss 2.8513e-01 (2.0585e-01)\tAcc@1  92.19 ( 93.79)\tAcc@5 100.00 ( 99.80)\n",
            "Epoch: [80][360/391]\tTime  0.223 ( 0.225)\tLoss 3.2006e-01 (2.1129e-01)\tAcc@1  91.41 ( 93.61)\tAcc@5 100.00 ( 99.79)\n",
            "Epoch: [80][390/391]\tTime  0.153 ( 0.225)\tLoss 3.2452e-01 (2.1710e-01)\tAcc@1  88.75 ( 93.38)\tAcc@5 100.00 ( 99.76)\n",
            "==> Train Accuracy: Acc@1 93.382 || Acc@5 99.760\n",
            "==> Test Accuracy:  Acc@1 68.920 || Acc@5 90.180\n",
            "==> 92.10 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 81, lr: 0.020000000000000004 -----\n",
            "Epoch: [81][  0/391]\tTime  0.579 ( 0.579)\tLoss 1.6755e-01 (1.6755e-01)\tAcc@1  96.09 ( 96.09)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [81][ 30/391]\tTime  0.269 ( 0.234)\tLoss 2.6701e-01 (2.0323e-01)\tAcc@1  92.97 ( 94.51)\tAcc@5 100.00 ( 99.87)\n",
            "Epoch: [81][ 60/391]\tTime  0.261 ( 0.231)\tLoss 1.7077e-01 (1.9735e-01)\tAcc@1  93.75 ( 94.25)\tAcc@5 100.00 ( 99.87)\n",
            "Epoch: [81][ 90/391]\tTime  0.183 ( 0.227)\tLoss 2.7008e-01 (1.9579e-01)\tAcc@1  92.19 ( 94.24)\tAcc@5 100.00 ( 99.87)\n",
            "Epoch: [81][120/391]\tTime  0.163 ( 0.227)\tLoss 2.4774e-01 (1.9623e-01)\tAcc@1  91.41 ( 94.13)\tAcc@5 100.00 ( 99.87)\n",
            "Epoch: [81][150/391]\tTime  0.172 ( 0.227)\tLoss 2.0576e-01 (1.9654e-01)\tAcc@1  95.31 ( 94.11)\tAcc@5  99.22 ( 99.87)\n",
            "Epoch: [81][180/391]\tTime  0.176 ( 0.227)\tLoss 1.9070e-01 (1.9906e-01)\tAcc@1  93.75 ( 93.97)\tAcc@5 100.00 ( 99.86)\n",
            "Epoch: [81][210/391]\tTime  0.298 ( 0.226)\tLoss 2.3252e-01 (2.0241e-01)\tAcc@1  93.75 ( 93.83)\tAcc@5 100.00 ( 99.84)\n",
            "Epoch: [81][240/391]\tTime  0.221 ( 0.226)\tLoss 3.1847e-01 (2.0561e-01)\tAcc@1  88.28 ( 93.70)\tAcc@5  99.22 ( 99.84)\n",
            "Epoch: [81][270/391]\tTime  0.171 ( 0.226)\tLoss 2.6591e-01 (2.1166e-01)\tAcc@1  92.19 ( 93.52)\tAcc@5  99.22 ( 99.83)\n",
            "Epoch: [81][300/391]\tTime  0.172 ( 0.227)\tLoss 2.2433e-01 (2.1749e-01)\tAcc@1  93.75 ( 93.37)\tAcc@5 100.00 ( 99.82)\n",
            "Epoch: [81][330/391]\tTime  0.170 ( 0.227)\tLoss 2.1009e-01 (2.2033e-01)\tAcc@1  95.31 ( 93.30)\tAcc@5 100.00 ( 99.81)\n",
            "Epoch: [81][360/391]\tTime  0.163 ( 0.227)\tLoss 2.9294e-01 (2.2176e-01)\tAcc@1  87.50 ( 93.20)\tAcc@5  99.22 ( 99.81)\n",
            "Epoch: [81][390/391]\tTime  0.150 ( 0.226)\tLoss 4.6495e-01 (2.2690e-01)\tAcc@1  85.00 ( 93.05)\tAcc@5  98.75 ( 99.78)\n",
            "==> Train Accuracy: Acc@1 93.054 || Acc@5 99.778\n",
            "==> Test Accuracy:  Acc@1 67.750 || Acc@5 90.310\n",
            "==> 92.61 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 82, lr: 0.020000000000000004 -----\n",
            "Epoch: [82][  0/391]\tTime  0.587 ( 0.587)\tLoss 1.7524e-01 (1.7524e-01)\tAcc@1  94.53 ( 94.53)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [82][ 30/391]\tTime  0.170 ( 0.232)\tLoss 2.4179e-01 (2.2480e-01)\tAcc@1  90.62 ( 92.97)\tAcc@5 100.00 ( 99.80)\n",
            "Epoch: [82][ 60/391]\tTime  0.180 ( 0.228)\tLoss 1.8901e-01 (2.1605e-01)\tAcc@1  94.53 ( 93.49)\tAcc@5 100.00 ( 99.80)\n",
            "Epoch: [82][ 90/391]\tTime  0.189 ( 0.230)\tLoss 2.6531e-01 (2.1181e-01)\tAcc@1  89.06 ( 93.70)\tAcc@5 100.00 ( 99.80)\n",
            "Epoch: [82][120/391]\tTime  0.315 ( 0.228)\tLoss 2.1458e-01 (2.0906e-01)\tAcc@1  92.97 ( 93.82)\tAcc@5 100.00 ( 99.82)\n",
            "Epoch: [82][150/391]\tTime  0.277 ( 0.229)\tLoss 2.2726e-01 (2.1180e-01)\tAcc@1  92.19 ( 93.69)\tAcc@5 100.00 ( 99.80)\n",
            "Epoch: [82][180/391]\tTime  0.210 ( 0.228)\tLoss 2.4186e-01 (2.1225e-01)\tAcc@1  92.19 ( 93.68)\tAcc@5 100.00 ( 99.79)\n",
            "Epoch: [82][210/391]\tTime  0.240 ( 0.226)\tLoss 1.5691e-01 (2.1450e-01)\tAcc@1  95.31 ( 93.61)\tAcc@5 100.00 ( 99.79)\n",
            "Epoch: [82][240/391]\tTime  0.180 ( 0.225)\tLoss 1.6322e-01 (2.1598e-01)\tAcc@1  95.31 ( 93.61)\tAcc@5 100.00 ( 99.78)\n",
            "Epoch: [82][270/391]\tTime  0.253 ( 0.225)\tLoss 2.2955e-01 (2.1837e-01)\tAcc@1  92.97 ( 93.50)\tAcc@5 100.00 ( 99.78)\n",
            "Epoch: [82][300/391]\tTime  0.287 ( 0.225)\tLoss 3.0552e-01 (2.2301e-01)\tAcc@1  89.84 ( 93.34)\tAcc@5  98.44 ( 99.78)\n",
            "Epoch: [82][330/391]\tTime  0.277 ( 0.225)\tLoss 2.0875e-01 (2.2585e-01)\tAcc@1  93.75 ( 93.21)\tAcc@5 100.00 ( 99.77)\n",
            "Epoch: [82][360/391]\tTime  0.259 ( 0.226)\tLoss 2.9012e-01 (2.2829e-01)\tAcc@1  89.06 ( 93.07)\tAcc@5  99.22 ( 99.77)\n",
            "Epoch: [82][390/391]\tTime  0.150 ( 0.225)\tLoss 2.1812e-01 (2.3306e-01)\tAcc@1  92.50 ( 92.90)\tAcc@5 100.00 ( 99.75)\n",
            "==> Train Accuracy: Acc@1 92.904 || Acc@5 99.750\n",
            "==> Test Accuracy:  Acc@1 68.400 || Acc@5 89.440\n",
            "==> 92.21 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 83, lr: 0.020000000000000004 -----\n",
            "Epoch: [83][  0/391]\tTime  0.578 ( 0.578)\tLoss 1.6025e-01 (1.6025e-01)\tAcc@1  96.09 ( 96.09)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [83][ 30/391]\tTime  0.170 ( 0.229)\tLoss 2.3319e-01 (2.0533e-01)\tAcc@1  92.19 ( 93.80)\tAcc@5 100.00 ( 99.75)\n",
            "Epoch: [83][ 60/391]\tTime  0.296 ( 0.226)\tLoss 1.9463e-01 (2.0533e-01)\tAcc@1  94.53 ( 93.71)\tAcc@5 100.00 ( 99.82)\n",
            "Epoch: [83][ 90/391]\tTime  0.178 ( 0.224)\tLoss 2.1589e-01 (2.0853e-01)\tAcc@1  93.75 ( 93.78)\tAcc@5 100.00 ( 99.79)\n",
            "Epoch: [83][120/391]\tTime  0.235 ( 0.224)\tLoss 2.0505e-01 (2.0784e-01)\tAcc@1  92.19 ( 93.67)\tAcc@5 100.00 ( 99.79)\n",
            "Epoch: [83][150/391]\tTime  0.175 ( 0.223)\tLoss 2.0510e-01 (2.0699e-01)\tAcc@1  92.97 ( 93.70)\tAcc@5 100.00 ( 99.79)\n",
            "Epoch: [83][180/391]\tTime  0.303 ( 0.223)\tLoss 3.3002e-01 (2.0936e-01)\tAcc@1  91.41 ( 93.69)\tAcc@5 100.00 ( 99.80)\n",
            "Epoch: [83][210/391]\tTime  0.260 ( 0.223)\tLoss 3.1585e-01 (2.1293e-01)\tAcc@1  90.62 ( 93.62)\tAcc@5  99.22 ( 99.78)\n",
            "Epoch: [83][240/391]\tTime  0.221 ( 0.223)\tLoss 2.0930e-01 (2.1337e-01)\tAcc@1  94.53 ( 93.59)\tAcc@5  99.22 ( 99.79)\n",
            "Epoch: [83][270/391]\tTime  0.315 ( 0.223)\tLoss 2.1106e-01 (2.1629e-01)\tAcc@1  93.75 ( 93.48)\tAcc@5 100.00 ( 99.78)\n",
            "Epoch: [83][300/391]\tTime  0.189 ( 0.224)\tLoss 1.8307e-01 (2.1639e-01)\tAcc@1  96.09 ( 93.47)\tAcc@5  98.44 ( 99.76)\n",
            "Epoch: [83][330/391]\tTime  0.266 ( 0.224)\tLoss 2.9854e-01 (2.1807e-01)\tAcc@1  91.41 ( 93.45)\tAcc@5  99.22 ( 99.76)\n",
            "Epoch: [83][360/391]\tTime  0.176 ( 0.225)\tLoss 2.8092e-01 (2.1990e-01)\tAcc@1  89.84 ( 93.37)\tAcc@5  99.22 ( 99.77)\n",
            "Epoch: [83][390/391]\tTime  0.152 ( 0.224)\tLoss 2.0902e-01 (2.2229e-01)\tAcc@1  93.75 ( 93.30)\tAcc@5 100.00 ( 99.77)\n",
            "==> Train Accuracy: Acc@1 93.300 || Acc@5 99.772\n",
            "==> Test Accuracy:  Acc@1 70.290 || Acc@5 91.070\n",
            "==> 91.94 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 84, lr: 0.020000000000000004 -----\n",
            "Epoch: [84][  0/391]\tTime  0.588 ( 0.588)\tLoss 1.5029e-01 (1.5029e-01)\tAcc@1  95.31 ( 95.31)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [84][ 30/391]\tTime  0.251 ( 0.240)\tLoss 1.6782e-01 (1.9683e-01)\tAcc@1  94.53 ( 93.98)\tAcc@5 100.00 ( 99.75)\n",
            "Epoch: [84][ 60/391]\tTime  0.270 ( 0.231)\tLoss 1.4287e-01 (1.8961e-01)\tAcc@1  96.09 ( 94.38)\tAcc@5 100.00 ( 99.83)\n",
            "Epoch: [84][ 90/391]\tTime  0.248 ( 0.230)\tLoss 2.1630e-01 (1.8964e-01)\tAcc@1  92.19 ( 94.41)\tAcc@5 100.00 ( 99.80)\n",
            "Epoch: [84][120/391]\tTime  0.160 ( 0.227)\tLoss 1.3435e-01 (1.8765e-01)\tAcc@1  95.31 ( 94.40)\tAcc@5  99.22 ( 99.80)\n",
            "Epoch: [84][150/391]\tTime  0.275 ( 0.228)\tLoss 1.5153e-01 (1.9045e-01)\tAcc@1  96.09 ( 94.29)\tAcc@5 100.00 ( 99.78)\n",
            "Epoch: [84][180/391]\tTime  0.311 ( 0.227)\tLoss 2.8876e-01 (1.9634e-01)\tAcc@1  89.84 ( 94.12)\tAcc@5 100.00 ( 99.79)\n",
            "Epoch: [84][210/391]\tTime  0.186 ( 0.226)\tLoss 2.5804e-01 (1.9954e-01)\tAcc@1  93.75 ( 94.00)\tAcc@5 100.00 ( 99.80)\n",
            "Epoch: [84][240/391]\tTime  0.170 ( 0.226)\tLoss 1.1125e-01 (2.0506e-01)\tAcc@1  96.09 ( 93.84)\tAcc@5 100.00 ( 99.79)\n",
            "Epoch: [84][270/391]\tTime  0.267 ( 0.226)\tLoss 2.3960e-01 (2.0939e-01)\tAcc@1  92.19 ( 93.69)\tAcc@5 100.00 ( 99.80)\n",
            "Epoch: [84][300/391]\tTime  0.258 ( 0.226)\tLoss 2.3889e-01 (2.1081e-01)\tAcc@1  92.19 ( 93.67)\tAcc@5 100.00 ( 99.80)\n",
            "Epoch: [84][330/391]\tTime  0.191 ( 0.225)\tLoss 4.1138e-01 (2.1389e-01)\tAcc@1  86.72 ( 93.57)\tAcc@5  99.22 ( 99.80)\n",
            "Epoch: [84][360/391]\tTime  0.218 ( 0.225)\tLoss 2.7807e-01 (2.1538e-01)\tAcc@1  90.62 ( 93.51)\tAcc@5  99.22 ( 99.79)\n",
            "Epoch: [84][390/391]\tTime  0.149 ( 0.225)\tLoss 3.9325e-01 (2.1671e-01)\tAcc@1  88.75 ( 93.47)\tAcc@5 100.00 ( 99.78)\n",
            "==> Train Accuracy: Acc@1 93.474 || Acc@5 99.784\n",
            "==> Test Accuracy:  Acc@1 68.290 || Acc@5 90.020\n",
            "==> 92.04 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 85, lr: 0.020000000000000004 -----\n",
            "Epoch: [85][  0/391]\tTime  0.560 ( 0.560)\tLoss 1.3864e-01 (1.3864e-01)\tAcc@1  96.88 ( 96.88)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [85][ 30/391]\tTime  0.234 ( 0.235)\tLoss 2.2787e-01 (1.9706e-01)\tAcc@1  92.97 ( 93.85)\tAcc@5 100.00 ( 99.85)\n",
            "Epoch: [85][ 60/391]\tTime  0.257 ( 0.228)\tLoss 2.6264e-01 (1.9448e-01)\tAcc@1  92.19 ( 93.98)\tAcc@5 100.00 ( 99.88)\n",
            "Epoch: [85][ 90/391]\tTime  0.318 ( 0.228)\tLoss 2.2141e-01 (1.9016e-01)\tAcc@1  95.31 ( 94.12)\tAcc@5 100.00 ( 99.90)\n",
            "Epoch: [85][120/391]\tTime  0.282 ( 0.229)\tLoss 1.5497e-01 (1.8669e-01)\tAcc@1  94.53 ( 94.27)\tAcc@5 100.00 ( 99.88)\n",
            "Epoch: [85][150/391]\tTime  0.298 ( 0.228)\tLoss 2.1272e-01 (1.9166e-01)\tAcc@1  93.75 ( 94.08)\tAcc@5 100.00 ( 99.85)\n",
            "Epoch: [85][180/391]\tTime  0.326 ( 0.228)\tLoss 2.3817e-01 (1.9503e-01)\tAcc@1  93.75 ( 93.99)\tAcc@5 100.00 ( 99.86)\n",
            "Epoch: [85][210/391]\tTime  0.263 ( 0.229)\tLoss 2.0464e-01 (1.9552e-01)\tAcc@1  96.09 ( 93.99)\tAcc@5 100.00 ( 99.86)\n",
            "Epoch: [85][240/391]\tTime  0.292 ( 0.228)\tLoss 1.9620e-01 (1.9668e-01)\tAcc@1  94.53 ( 93.97)\tAcc@5 100.00 ( 99.84)\n",
            "Epoch: [85][270/391]\tTime  0.339 ( 0.229)\tLoss 2.4866e-01 (1.9868e-01)\tAcc@1  94.53 ( 93.93)\tAcc@5  99.22 ( 99.83)\n",
            "Epoch: [85][300/391]\tTime  0.259 ( 0.228)\tLoss 2.5961e-01 (2.0121e-01)\tAcc@1  91.41 ( 93.85)\tAcc@5  99.22 ( 99.82)\n",
            "Epoch: [85][330/391]\tTime  0.302 ( 0.227)\tLoss 2.2791e-01 (2.0313e-01)\tAcc@1  94.53 ( 93.82)\tAcc@5 100.00 ( 99.83)\n",
            "Epoch: [85][360/391]\tTime  0.172 ( 0.227)\tLoss 2.3381e-01 (2.0729e-01)\tAcc@1  90.62 ( 93.68)\tAcc@5 100.00 ( 99.82)\n",
            "Epoch: [85][390/391]\tTime  0.151 ( 0.226)\tLoss 2.5663e-01 (2.1087e-01)\tAcc@1  92.50 ( 93.58)\tAcc@5  98.75 ( 99.79)\n",
            "==> Train Accuracy: Acc@1 93.582 || Acc@5 99.794\n",
            "==> Test Accuracy:  Acc@1 69.940 || Acc@5 91.330\n",
            "==> 92.52 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 86, lr: 0.020000000000000004 -----\n",
            "Epoch: [86][  0/391]\tTime  0.541 ( 0.541)\tLoss 1.2070e-01 (1.2070e-01)\tAcc@1  96.88 ( 96.88)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [86][ 30/391]\tTime  0.300 ( 0.239)\tLoss 2.3898e-01 (2.0498e-01)\tAcc@1  92.97 ( 94.41)\tAcc@5  99.22 ( 99.75)\n",
            "Epoch: [86][ 60/391]\tTime  0.254 ( 0.230)\tLoss 1.8596e-01 (2.0390e-01)\tAcc@1  93.75 ( 94.25)\tAcc@5 100.00 ( 99.80)\n",
            "Epoch: [86][ 90/391]\tTime  0.235 ( 0.227)\tLoss 1.9449e-01 (2.0110e-01)\tAcc@1  96.88 ( 94.33)\tAcc@5 100.00 ( 99.79)\n",
            "Epoch: [86][120/391]\tTime  0.163 ( 0.225)\tLoss 2.6474e-01 (1.9564e-01)\tAcc@1  90.62 ( 94.43)\tAcc@5 100.00 ( 99.82)\n",
            "Epoch: [86][150/391]\tTime  0.173 ( 0.225)\tLoss 2.1937e-01 (1.9512e-01)\tAcc@1  92.19 ( 94.36)\tAcc@5 100.00 ( 99.83)\n",
            "Epoch: [86][180/391]\tTime  0.175 ( 0.225)\tLoss 1.1979e-01 (1.9979e-01)\tAcc@1  97.66 ( 94.19)\tAcc@5 100.00 ( 99.83)\n",
            "Epoch: [86][210/391]\tTime  0.173 ( 0.224)\tLoss 2.6157e-01 (1.9950e-01)\tAcc@1  89.06 ( 94.16)\tAcc@5  99.22 ( 99.82)\n",
            "Epoch: [86][240/391]\tTime  0.173 ( 0.224)\tLoss 1.5541e-01 (2.0098e-01)\tAcc@1  96.09 ( 94.07)\tAcc@5 100.00 ( 99.81)\n",
            "Epoch: [86][270/391]\tTime  0.174 ( 0.224)\tLoss 2.5045e-01 (2.0283e-01)\tAcc@1  91.41 ( 93.96)\tAcc@5  99.22 ( 99.80)\n",
            "Epoch: [86][300/391]\tTime  0.172 ( 0.223)\tLoss 1.9207e-01 (2.0443e-01)\tAcc@1  92.97 ( 93.88)\tAcc@5 100.00 ( 99.80)\n",
            "Epoch: [86][330/391]\tTime  0.168 ( 0.223)\tLoss 2.4924e-01 (2.0787e-01)\tAcc@1  92.19 ( 93.77)\tAcc@5  99.22 ( 99.80)\n",
            "Epoch: [86][360/391]\tTime  0.176 ( 0.223)\tLoss 3.0206e-01 (2.1186e-01)\tAcc@1  92.19 ( 93.64)\tAcc@5  99.22 ( 99.78)\n",
            "Epoch: [86][390/391]\tTime  0.150 ( 0.223)\tLoss 3.9304e-01 (2.1637e-01)\tAcc@1  86.25 ( 93.51)\tAcc@5 100.00 ( 99.77)\n",
            "==> Train Accuracy: Acc@1 93.508 || Acc@5 99.768\n",
            "==> Test Accuracy:  Acc@1 69.890 || Acc@5 91.000\n",
            "==> 91.34 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 87, lr: 0.020000000000000004 -----\n",
            "Epoch: [87][  0/391]\tTime  0.556 ( 0.556)\tLoss 2.9804e-01 (2.9804e-01)\tAcc@1  92.19 ( 92.19)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [87][ 30/391]\tTime  0.328 ( 0.243)\tLoss 2.2171e-01 (2.0919e-01)\tAcc@1  91.41 ( 93.32)\tAcc@5 100.00 ( 99.80)\n",
            "Epoch: [87][ 60/391]\tTime  0.194 ( 0.229)\tLoss 1.7203e-01 (2.0407e-01)\tAcc@1  96.09 ( 93.85)\tAcc@5 100.00 ( 99.80)\n",
            "Epoch: [87][ 90/391]\tTime  0.305 ( 0.228)\tLoss 2.3053e-01 (1.9752e-01)\tAcc@1  89.84 ( 94.08)\tAcc@5 100.00 ( 99.82)\n",
            "Epoch: [87][120/391]\tTime  0.260 ( 0.227)\tLoss 2.2752e-01 (2.0239e-01)\tAcc@1  91.41 ( 93.87)\tAcc@5 100.00 ( 99.82)\n",
            "Epoch: [87][150/391]\tTime  0.182 ( 0.227)\tLoss 2.3505e-01 (2.0428e-01)\tAcc@1  92.19 ( 93.80)\tAcc@5 100.00 ( 99.82)\n",
            "Epoch: [87][180/391]\tTime  0.165 ( 0.227)\tLoss 1.7566e-01 (2.0462e-01)\tAcc@1  96.88 ( 93.82)\tAcc@5 100.00 ( 99.81)\n",
            "Epoch: [87][210/391]\tTime  0.171 ( 0.227)\tLoss 1.7883e-01 (2.0636e-01)\tAcc@1  94.53 ( 93.74)\tAcc@5 100.00 ( 99.81)\n",
            "Epoch: [87][240/391]\tTime  0.179 ( 0.226)\tLoss 1.9380e-01 (2.0999e-01)\tAcc@1  94.53 ( 93.66)\tAcc@5 100.00 ( 99.78)\n",
            "Epoch: [87][270/391]\tTime  0.206 ( 0.226)\tLoss 2.7713e-01 (2.1300e-01)\tAcc@1  92.97 ( 93.55)\tAcc@5  99.22 ( 99.75)\n",
            "Epoch: [87][300/391]\tTime  0.233 ( 0.226)\tLoss 2.1982e-01 (2.1578e-01)\tAcc@1  92.19 ( 93.41)\tAcc@5 100.00 ( 99.76)\n",
            "Epoch: [87][330/391]\tTime  0.249 ( 0.226)\tLoss 2.6124e-01 (2.2120e-01)\tAcc@1  90.62 ( 93.25)\tAcc@5  99.22 ( 99.74)\n",
            "Epoch: [87][360/391]\tTime  0.307 ( 0.226)\tLoss 1.8179e-01 (2.2240e-01)\tAcc@1  94.53 ( 93.21)\tAcc@5 100.00 ( 99.74)\n",
            "Epoch: [87][390/391]\tTime  0.152 ( 0.225)\tLoss 1.3819e-01 (2.2408e-01)\tAcc@1  96.25 ( 93.16)\tAcc@5 100.00 ( 99.75)\n",
            "==> Train Accuracy: Acc@1 93.162 || Acc@5 99.748\n",
            "==> Test Accuracy:  Acc@1 68.740 || Acc@5 90.680\n",
            "==> 92.12 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 88, lr: 0.020000000000000004 -----\n",
            "Epoch: [88][  0/391]\tTime  0.545 ( 0.545)\tLoss 1.3723e-01 (1.3723e-01)\tAcc@1  95.31 ( 95.31)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [88][ 30/391]\tTime  0.287 ( 0.229)\tLoss 1.8578e-01 (1.7206e-01)\tAcc@1  94.53 ( 94.86)\tAcc@5 100.00 ( 99.85)\n",
            "Epoch: [88][ 60/391]\tTime  0.280 ( 0.227)\tLoss 1.7245e-01 (1.7451e-01)\tAcc@1  94.53 ( 94.77)\tAcc@5 100.00 ( 99.86)\n",
            "Epoch: [88][ 90/391]\tTime  0.177 ( 0.224)\tLoss 1.7600e-01 (1.8067e-01)\tAcc@1  93.75 ( 94.65)\tAcc@5 100.00 ( 99.83)\n",
            "Epoch: [88][120/391]\tTime  0.166 ( 0.225)\tLoss 2.2038e-01 (1.8199e-01)\tAcc@1  91.41 ( 94.63)\tAcc@5  99.22 ( 99.81)\n",
            "Epoch: [88][150/391]\tTime  0.173 ( 0.226)\tLoss 2.2137e-01 (1.8327e-01)\tAcc@1  96.09 ( 94.66)\tAcc@5 100.00 ( 99.83)\n",
            "Epoch: [88][180/391]\tTime  0.172 ( 0.226)\tLoss 1.7759e-01 (1.8655e-01)\tAcc@1  95.31 ( 94.56)\tAcc@5 100.00 ( 99.83)\n",
            "Epoch: [88][210/391]\tTime  0.172 ( 0.226)\tLoss 2.6261e-01 (1.9017e-01)\tAcc@1  92.19 ( 94.41)\tAcc@5 100.00 ( 99.82)\n",
            "Epoch: [88][240/391]\tTime  0.166 ( 0.226)\tLoss 2.2858e-01 (1.9283e-01)\tAcc@1  92.19 ( 94.28)\tAcc@5 100.00 ( 99.82)\n",
            "Epoch: [88][270/391]\tTime  0.180 ( 0.226)\tLoss 1.1091e-01 (1.9453e-01)\tAcc@1  97.66 ( 94.25)\tAcc@5 100.00 ( 99.82)\n",
            "Epoch: [88][300/391]\tTime  0.175 ( 0.226)\tLoss 2.2962e-01 (1.9470e-01)\tAcc@1  95.31 ( 94.26)\tAcc@5 100.00 ( 99.83)\n",
            "Epoch: [88][330/391]\tTime  0.170 ( 0.226)\tLoss 2.3622e-01 (1.9739e-01)\tAcc@1  93.75 ( 94.18)\tAcc@5  98.44 ( 99.82)\n",
            "Epoch: [88][360/391]\tTime  0.167 ( 0.226)\tLoss 3.0377e-01 (2.0071e-01)\tAcc@1  88.28 ( 94.06)\tAcc@5 100.00 ( 99.81)\n",
            "Epoch: [88][390/391]\tTime  0.151 ( 0.226)\tLoss 2.4400e-01 (2.0477e-01)\tAcc@1  90.00 ( 93.93)\tAcc@5 100.00 ( 99.81)\n",
            "==> Train Accuracy: Acc@1 93.926 || Acc@5 99.810\n",
            "==> Test Accuracy:  Acc@1 68.790 || Acc@5 90.520\n",
            "==> 92.48 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 89, lr: 0.020000000000000004 -----\n",
            "Epoch: [89][  0/391]\tTime  0.579 ( 0.579)\tLoss 2.9222e-01 (2.9222e-01)\tAcc@1  92.19 ( 92.19)\tAcc@5  99.22 ( 99.22)\n",
            "Epoch: [89][ 30/391]\tTime  0.290 ( 0.235)\tLoss 2.0266e-01 (1.8950e-01)\tAcc@1  94.53 ( 94.13)\tAcc@5 100.00 ( 99.77)\n",
            "Epoch: [89][ 60/391]\tTime  0.173 ( 0.229)\tLoss 1.6444e-01 (1.8110e-01)\tAcc@1  95.31 ( 94.61)\tAcc@5 100.00 ( 99.83)\n",
            "Epoch: [89][ 90/391]\tTime  0.205 ( 0.226)\tLoss 2.2992e-01 (1.8260e-01)\tAcc@1  93.75 ( 94.64)\tAcc@5 100.00 ( 99.84)\n",
            "Epoch: [89][120/391]\tTime  0.324 ( 0.227)\tLoss 1.9535e-01 (1.8317e-01)\tAcc@1  94.53 ( 94.60)\tAcc@5 100.00 ( 99.83)\n",
            "Epoch: [89][150/391]\tTime  0.170 ( 0.226)\tLoss 3.2139e-01 (1.8483e-01)\tAcc@1  92.19 ( 94.56)\tAcc@5  99.22 ( 99.84)\n",
            "Epoch: [89][180/391]\tTime  0.321 ( 0.226)\tLoss 2.6814e-01 (1.8639e-01)\tAcc@1  91.41 ( 94.50)\tAcc@5 100.00 ( 99.85)\n",
            "Epoch: [89][210/391]\tTime  0.292 ( 0.226)\tLoss 3.1049e-01 (1.8940e-01)\tAcc@1  90.62 ( 94.33)\tAcc@5  99.22 ( 99.83)\n",
            "Epoch: [89][240/391]\tTime  0.291 ( 0.226)\tLoss 3.3198e-01 (1.9421e-01)\tAcc@1  90.62 ( 94.19)\tAcc@5  98.44 ( 99.82)\n",
            "Epoch: [89][270/391]\tTime  0.185 ( 0.225)\tLoss 2.8226e-01 (1.9774e-01)\tAcc@1  90.62 ( 94.06)\tAcc@5 100.00 ( 99.82)\n",
            "Epoch: [89][300/391]\tTime  0.169 ( 0.225)\tLoss 2.8623e-01 (2.0063e-01)\tAcc@1  89.84 ( 94.00)\tAcc@5 100.00 ( 99.81)\n",
            "Epoch: [89][330/391]\tTime  0.176 ( 0.225)\tLoss 2.6646e-01 (2.0242e-01)\tAcc@1  90.62 ( 93.95)\tAcc@5  99.22 ( 99.81)\n",
            "Epoch: [89][360/391]\tTime  0.164 ( 0.225)\tLoss 2.2285e-01 (2.0521e-01)\tAcc@1  94.53 ( 93.86)\tAcc@5 100.00 ( 99.81)\n",
            "Epoch: [89][390/391]\tTime  0.150 ( 0.224)\tLoss 2.8537e-01 (2.1073e-01)\tAcc@1  90.00 ( 93.65)\tAcc@5 100.00 ( 99.80)\n",
            "==> Train Accuracy: Acc@1 93.654 || Acc@5 99.796\n",
            "==> Test Accuracy:  Acc@1 68.670 || Acc@5 90.580\n",
            "==> 91.73 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 90, lr: 0.004000000000000001 -----\n",
            "Epoch: [90][  0/391]\tTime  0.566 ( 0.566)\tLoss 3.4446e-01 (3.4446e-01)\tAcc@1  91.41 ( 91.41)\tAcc@5  98.44 ( 98.44)\n",
            "Epoch: [90][ 30/391]\tTime  0.303 ( 0.241)\tLoss 1.9661e-01 (1.6609e-01)\tAcc@1  92.97 ( 95.26)\tAcc@5 100.00 ( 99.82)\n",
            "Epoch: [90][ 60/391]\tTime  0.237 ( 0.233)\tLoss 1.0842e-01 (1.4354e-01)\tAcc@1  97.66 ( 95.91)\tAcc@5 100.00 ( 99.88)\n",
            "Epoch: [90][ 90/391]\tTime  0.173 ( 0.229)\tLoss 8.9302e-02 (1.2851e-01)\tAcc@1  96.88 ( 96.42)\tAcc@5 100.00 ( 99.92)\n",
            "Epoch: [90][120/391]\tTime  0.183 ( 0.227)\tLoss 9.7665e-02 (1.1640e-01)\tAcc@1  97.66 ( 96.82)\tAcc@5 100.00 ( 99.94)\n",
            "Epoch: [90][150/391]\tTime  0.220 ( 0.227)\tLoss 4.7237e-02 (1.0930e-01)\tAcc@1 100.00 ( 97.06)\tAcc@5 100.00 ( 99.95)\n",
            "Epoch: [90][180/391]\tTime  0.221 ( 0.225)\tLoss 8.4920e-02 (1.0362e-01)\tAcc@1  97.66 ( 97.22)\tAcc@5 100.00 ( 99.96)\n",
            "Epoch: [90][210/391]\tTime  0.331 ( 0.225)\tLoss 8.3827e-02 (9.8240e-02)\tAcc@1  97.66 ( 97.41)\tAcc@5 100.00 ( 99.97)\n",
            "Epoch: [90][240/391]\tTime  0.261 ( 0.225)\tLoss 5.3324e-02 (9.4284e-02)\tAcc@1  99.22 ( 97.55)\tAcc@5 100.00 ( 99.97)\n",
            "Epoch: [90][270/391]\tTime  0.259 ( 0.225)\tLoss 5.2141e-02 (9.1489e-02)\tAcc@1  98.44 ( 97.63)\tAcc@5 100.00 ( 99.97)\n",
            "Epoch: [90][300/391]\tTime  0.235 ( 0.224)\tLoss 5.9183e-02 (8.7912e-02)\tAcc@1  98.44 ( 97.74)\tAcc@5 100.00 ( 99.97)\n",
            "Epoch: [90][330/391]\tTime  0.178 ( 0.224)\tLoss 4.2151e-02 (8.5235e-02)\tAcc@1  99.22 ( 97.82)\tAcc@5 100.00 ( 99.98)\n",
            "Epoch: [90][360/391]\tTime  0.177 ( 0.224)\tLoss 5.3443e-02 (8.2735e-02)\tAcc@1  99.22 ( 97.91)\tAcc@5 100.00 ( 99.98)\n",
            "Epoch: [90][390/391]\tTime  0.148 ( 0.224)\tLoss 4.2754e-02 (8.0966e-02)\tAcc@1  98.75 ( 97.97)\tAcc@5 100.00 ( 99.98)\n",
            "==> Train Accuracy: Acc@1 97.966 || Acc@5 99.980\n",
            "==> Test Accuracy:  Acc@1 75.320 || Acc@5 93.370\n",
            "==> 91.68 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 91, lr: 0.004000000000000001 -----\n",
            "Epoch: [91][  0/391]\tTime  0.557 ( 0.557)\tLoss 6.9472e-02 (6.9472e-02)\tAcc@1  98.44 ( 98.44)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [91][ 30/391]\tTime  0.179 ( 0.234)\tLoss 9.0299e-02 (4.7848e-02)\tAcc@1  96.88 ( 98.89)\tAcc@5  99.22 ( 99.97)\n",
            "Epoch: [91][ 60/391]\tTime  0.154 ( 0.225)\tLoss 6.4896e-02 (4.7604e-02)\tAcc@1  98.44 ( 99.00)\tAcc@5 100.00 ( 99.99)\n",
            "Epoch: [91][ 90/391]\tTime  0.173 ( 0.223)\tLoss 9.6355e-02 (4.8184e-02)\tAcc@1  96.88 ( 98.94)\tAcc@5 100.00 ( 99.99)\n",
            "Epoch: [91][120/391]\tTime  0.161 ( 0.222)\tLoss 3.4978e-02 (4.6708e-02)\tAcc@1 100.00 ( 99.01)\tAcc@5 100.00 ( 99.99)\n",
            "Epoch: [91][150/391]\tTime  0.278 ( 0.221)\tLoss 4.3225e-02 (4.5088e-02)\tAcc@1  99.22 ( 99.07)\tAcc@5 100.00 ( 99.99)\n",
            "Epoch: [91][180/391]\tTime  0.224 ( 0.221)\tLoss 1.9688e-02 (4.4593e-02)\tAcc@1  99.22 ( 99.09)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [91][210/391]\tTime  0.167 ( 0.221)\tLoss 3.8802e-02 (4.3174e-02)\tAcc@1  99.22 ( 99.16)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [91][240/391]\tTime  0.173 ( 0.222)\tLoss 5.2721e-02 (4.3123e-02)\tAcc@1  99.22 ( 99.16)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [91][270/391]\tTime  0.170 ( 0.222)\tLoss 3.5523e-02 (4.3155e-02)\tAcc@1  99.22 ( 99.13)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [91][300/391]\tTime  0.161 ( 0.223)\tLoss 2.7184e-02 (4.2954e-02)\tAcc@1 100.00 ( 99.13)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [91][330/391]\tTime  0.318 ( 0.223)\tLoss 2.3302e-02 (4.2514e-02)\tAcc@1 100.00 ( 99.16)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [91][360/391]\tTime  0.166 ( 0.223)\tLoss 4.1428e-02 (4.2180e-02)\tAcc@1  98.44 ( 99.17)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [91][390/391]\tTime  0.144 ( 0.222)\tLoss 4.2445e-02 (4.1970e-02)\tAcc@1  98.75 ( 99.17)\tAcc@5 100.00 (100.00)\n",
            "==> Train Accuracy: Acc@1 99.172 || Acc@5 99.996\n",
            "==> Test Accuracy:  Acc@1 75.780 || Acc@5 93.470\n",
            "==> 90.91 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 92, lr: 0.004000000000000001 -----\n",
            "Epoch: [92][  0/391]\tTime  0.638 ( 0.638)\tLoss 4.1237e-02 (4.1237e-02)\tAcc@1  99.22 ( 99.22)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [92][ 30/391]\tTime  0.300 ( 0.241)\tLoss 2.3657e-02 (2.9148e-02)\tAcc@1 100.00 ( 99.52)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [92][ 60/391]\tTime  0.335 ( 0.239)\tLoss 2.3069e-02 (3.1928e-02)\tAcc@1 100.00 ( 99.46)\tAcc@5 100.00 ( 99.99)\n",
            "Epoch: [92][ 90/391]\tTime  0.246 ( 0.234)\tLoss 2.1569e-02 (3.1349e-02)\tAcc@1 100.00 ( 99.45)\tAcc@5 100.00 ( 99.99)\n",
            "Epoch: [92][120/391]\tTime  0.307 ( 0.233)\tLoss 3.2562e-02 (3.0787e-02)\tAcc@1 100.00 ( 99.46)\tAcc@5 100.00 ( 99.99)\n",
            "Epoch: [92][150/391]\tTime  0.316 ( 0.232)\tLoss 3.2250e-02 (3.0766e-02)\tAcc@1  99.22 ( 99.48)\tAcc@5 100.00 ( 99.99)\n",
            "Epoch: [92][180/391]\tTime  0.167 ( 0.230)\tLoss 5.8645e-02 (3.0981e-02)\tAcc@1  98.44 ( 99.47)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [92][210/391]\tTime  0.167 ( 0.229)\tLoss 2.6459e-02 (3.1380e-02)\tAcc@1  99.22 ( 99.46)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [92][240/391]\tTime  0.176 ( 0.229)\tLoss 5.4825e-02 (3.1869e-02)\tAcc@1  98.44 ( 99.45)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [92][270/391]\tTime  0.347 ( 0.228)\tLoss 2.5193e-02 (3.1505e-02)\tAcc@1 100.00 ( 99.48)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [92][300/391]\tTime  0.165 ( 0.227)\tLoss 3.5904e-02 (3.1428e-02)\tAcc@1  98.44 ( 99.49)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [92][330/391]\tTime  0.177 ( 0.227)\tLoss 2.9767e-02 (3.1305e-02)\tAcc@1 100.00 ( 99.49)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [92][360/391]\tTime  0.247 ( 0.227)\tLoss 3.5020e-02 (3.1458e-02)\tAcc@1 100.00 ( 99.47)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [92][390/391]\tTime  0.142 ( 0.226)\tLoss 2.9683e-02 (3.1479e-02)\tAcc@1 100.00 ( 99.48)\tAcc@5 100.00 (100.00)\n",
            "==> Train Accuracy: Acc@1 99.478 || Acc@5 99.998\n",
            "==> Test Accuracy:  Acc@1 75.910 || Acc@5 93.630\n",
            "==> 92.60 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 93, lr: 0.004000000000000001 -----\n",
            "Epoch: [93][  0/391]\tTime  0.573 ( 0.573)\tLoss 3.7120e-02 (3.7120e-02)\tAcc@1  99.22 ( 99.22)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [93][ 30/391]\tTime  0.295 ( 0.228)\tLoss 2.6151e-02 (2.7359e-02)\tAcc@1 100.00 ( 99.70)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [93][ 60/391]\tTime  0.330 ( 0.224)\tLoss 1.8389e-02 (2.4789e-02)\tAcc@1 100.00 ( 99.68)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [93][ 90/391]\tTime  0.165 ( 0.222)\tLoss 3.0380e-02 (2.4063e-02)\tAcc@1 100.00 ( 99.72)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [93][120/391]\tTime  0.287 ( 0.223)\tLoss 3.3243e-02 (2.4969e-02)\tAcc@1 100.00 ( 99.67)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [93][150/391]\tTime  0.197 ( 0.222)\tLoss 2.4664e-02 (2.4999e-02)\tAcc@1  99.22 ( 99.66)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [93][180/391]\tTime  0.321 ( 0.222)\tLoss 2.0953e-02 (2.5258e-02)\tAcc@1 100.00 ( 99.66)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [93][210/391]\tTime  0.184 ( 0.222)\tLoss 4.0401e-02 (2.5620e-02)\tAcc@1  98.44 ( 99.64)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [93][240/391]\tTime  0.159 ( 0.223)\tLoss 3.0914e-02 (2.5921e-02)\tAcc@1  99.22 ( 99.63)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [93][270/391]\tTime  0.165 ( 0.224)\tLoss 1.8622e-02 (2.5718e-02)\tAcc@1 100.00 ( 99.64)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [93][300/391]\tTime  0.174 ( 0.224)\tLoss 3.3600e-02 (2.5816e-02)\tAcc@1 100.00 ( 99.64)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [93][330/391]\tTime  0.164 ( 0.223)\tLoss 1.6901e-02 (2.5799e-02)\tAcc@1 100.00 ( 99.63)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [93][360/391]\tTime  0.201 ( 0.223)\tLoss 1.5125e-02 (2.5713e-02)\tAcc@1 100.00 ( 99.63)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [93][390/391]\tTime  0.151 ( 0.223)\tLoss 3.3381e-02 (2.5987e-02)\tAcc@1 100.00 ( 99.62)\tAcc@5 100.00 (100.00)\n",
            "==> Train Accuracy: Acc@1 99.622 || Acc@5 100.000\n",
            "==> Test Accuracy:  Acc@1 76.100 || Acc@5 93.580\n",
            "==> 91.35 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 94, lr: 0.004000000000000001 -----\n",
            "Epoch: [94][  0/391]\tTime  0.567 ( 0.567)\tLoss 2.9208e-02 (2.9208e-02)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [94][ 30/391]\tTime  0.309 ( 0.245)\tLoss 1.8963e-02 (2.3253e-02)\tAcc@1 100.00 ( 99.60)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [94][ 60/391]\tTime  0.176 ( 0.228)\tLoss 1.5716e-02 (2.1235e-02)\tAcc@1 100.00 ( 99.72)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [94][ 90/391]\tTime  0.180 ( 0.224)\tLoss 9.9315e-03 (2.1653e-02)\tAcc@1 100.00 ( 99.70)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [94][120/391]\tTime  0.186 ( 0.223)\tLoss 1.4234e-02 (2.1104e-02)\tAcc@1 100.00 ( 99.75)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [94][150/391]\tTime  0.200 ( 0.223)\tLoss 1.9839e-02 (2.1765e-02)\tAcc@1 100.00 ( 99.72)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [94][180/391]\tTime  0.307 ( 0.224)\tLoss 2.3757e-02 (2.2407e-02)\tAcc@1 100.00 ( 99.71)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [94][210/391]\tTime  0.242 ( 0.224)\tLoss 1.2443e-02 (2.2141e-02)\tAcc@1 100.00 ( 99.72)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [94][240/391]\tTime  0.294 ( 0.224)\tLoss 1.9736e-02 (2.2754e-02)\tAcc@1 100.00 ( 99.68)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [94][270/391]\tTime  0.299 ( 0.225)\tLoss 2.8962e-02 (2.2837e-02)\tAcc@1  99.22 ( 99.67)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [94][300/391]\tTime  0.174 ( 0.224)\tLoss 1.5498e-02 (2.2847e-02)\tAcc@1 100.00 ( 99.67)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [94][330/391]\tTime  0.255 ( 0.223)\tLoss 1.1008e-02 (2.2739e-02)\tAcc@1 100.00 ( 99.67)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [94][360/391]\tTime  0.157 ( 0.223)\tLoss 1.5301e-02 (2.2830e-02)\tAcc@1 100.00 ( 99.66)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [94][390/391]\tTime  0.149 ( 0.223)\tLoss 2.2520e-02 (2.2635e-02)\tAcc@1 100.00 ( 99.68)\tAcc@5 100.00 (100.00)\n",
            "==> Train Accuracy: Acc@1 99.682 || Acc@5 100.000\n",
            "==> Test Accuracy:  Acc@1 76.300 || Acc@5 93.670\n",
            "==> 91.39 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 95, lr: 0.004000000000000001 -----\n",
            "Epoch: [95][  0/391]\tTime  0.601 ( 0.601)\tLoss 2.1486e-02 (2.1486e-02)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [95][ 30/391]\tTime  0.205 ( 0.235)\tLoss 4.7248e-02 (1.9540e-02)\tAcc@1  99.22 ( 99.82)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [95][ 60/391]\tTime  0.177 ( 0.231)\tLoss 4.0425e-02 (2.0237e-02)\tAcc@1  98.44 ( 99.78)\tAcc@5 100.00 ( 99.99)\n",
            "Epoch: [95][ 90/391]\tTime  0.159 ( 0.229)\tLoss 3.2294e-02 (2.0657e-02)\tAcc@1  99.22 ( 99.76)\tAcc@5 100.00 ( 99.99)\n",
            "Epoch: [95][120/391]\tTime  0.180 ( 0.229)\tLoss 1.8144e-02 (2.0657e-02)\tAcc@1 100.00 ( 99.73)\tAcc@5 100.00 ( 99.99)\n",
            "Epoch: [95][150/391]\tTime  0.174 ( 0.227)\tLoss 1.8112e-02 (2.0344e-02)\tAcc@1 100.00 ( 99.74)\tAcc@5 100.00 ( 99.99)\n",
            "Epoch: [95][180/391]\tTime  0.182 ( 0.227)\tLoss 1.2568e-02 (2.0310e-02)\tAcc@1 100.00 ( 99.73)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [95][210/391]\tTime  0.155 ( 0.227)\tLoss 2.3407e-02 (2.0381e-02)\tAcc@1  99.22 ( 99.74)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [95][240/391]\tTime  0.263 ( 0.226)\tLoss 4.5556e-02 (2.0779e-02)\tAcc@1  97.66 ( 99.73)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [95][270/391]\tTime  0.171 ( 0.225)\tLoss 1.2450e-02 (2.1005e-02)\tAcc@1 100.00 ( 99.72)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [95][300/391]\tTime  0.182 ( 0.225)\tLoss 1.0048e-02 (2.0897e-02)\tAcc@1 100.00 ( 99.72)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [95][330/391]\tTime  0.166 ( 0.225)\tLoss 1.9471e-02 (2.0681e-02)\tAcc@1  99.22 ( 99.72)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [95][360/391]\tTime  0.300 ( 0.225)\tLoss 4.1626e-02 (2.0718e-02)\tAcc@1  98.44 ( 99.72)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [95][390/391]\tTime  0.148 ( 0.224)\tLoss 3.6371e-02 (2.0635e-02)\tAcc@1 100.00 ( 99.72)\tAcc@5 100.00 (100.00)\n",
            "==> Train Accuracy: Acc@1 99.724 || Acc@5 99.998\n",
            "==> Test Accuracy:  Acc@1 76.450 || Acc@5 93.690\n",
            "==> 91.67 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 96, lr: 0.004000000000000001 -----\n",
            "Epoch: [96][  0/391]\tTime  0.568 ( 0.568)\tLoss 9.4593e-03 (9.4593e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [96][ 30/391]\tTime  0.172 ( 0.229)\tLoss 1.1827e-02 (1.7670e-02)\tAcc@1 100.00 ( 99.77)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [96][ 60/391]\tTime  0.265 ( 0.226)\tLoss 1.0147e-02 (1.7460e-02)\tAcc@1 100.00 ( 99.80)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [96][ 90/391]\tTime  0.291 ( 0.227)\tLoss 2.1989e-02 (1.8325e-02)\tAcc@1 100.00 ( 99.79)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [96][120/391]\tTime  0.256 ( 0.227)\tLoss 1.6674e-02 (1.7860e-02)\tAcc@1 100.00 ( 99.81)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [96][150/391]\tTime  0.228 ( 0.224)\tLoss 1.6602e-02 (1.7757e-02)\tAcc@1 100.00 ( 99.81)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [96][180/391]\tTime  0.168 ( 0.224)\tLoss 1.1316e-02 (1.7805e-02)\tAcc@1 100.00 ( 99.78)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [96][210/391]\tTime  0.197 ( 0.223)\tLoss 1.8711e-02 (1.7627e-02)\tAcc@1 100.00 ( 99.80)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [96][240/391]\tTime  0.166 ( 0.223)\tLoss 1.8627e-02 (1.7745e-02)\tAcc@1 100.00 ( 99.79)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [96][270/391]\tTime  0.173 ( 0.223)\tLoss 1.8039e-02 (1.7848e-02)\tAcc@1 100.00 ( 99.80)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [96][300/391]\tTime  0.248 ( 0.224)\tLoss 9.6663e-03 (1.8097e-02)\tAcc@1 100.00 ( 99.79)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [96][330/391]\tTime  0.176 ( 0.223)\tLoss 2.2336e-02 (1.7912e-02)\tAcc@1 100.00 ( 99.79)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [96][360/391]\tTime  0.213 ( 0.223)\tLoss 5.8684e-02 (1.8221e-02)\tAcc@1  99.22 ( 99.79)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [96][390/391]\tTime  0.150 ( 0.223)\tLoss 2.1343e-02 (1.8222e-02)\tAcc@1 100.00 ( 99.79)\tAcc@5 100.00 (100.00)\n",
            "==> Train Accuracy: Acc@1 99.794 || Acc@5 100.000\n",
            "==> Test Accuracy:  Acc@1 76.180 || Acc@5 93.660\n",
            "==> 91.21 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 97, lr: 0.004000000000000001 -----\n",
            "Epoch: [97][  0/391]\tTime  0.569 ( 0.569)\tLoss 3.8262e-02 (3.8262e-02)\tAcc@1  99.22 ( 99.22)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [97][ 30/391]\tTime  0.317 ( 0.236)\tLoss 2.3294e-02 (1.8928e-02)\tAcc@1 100.00 ( 99.77)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [97][ 60/391]\tTime  0.261 ( 0.232)\tLoss 3.0294e-02 (1.8700e-02)\tAcc@1  99.22 ( 99.81)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [97][ 90/391]\tTime  0.174 ( 0.228)\tLoss 8.6167e-03 (1.8028e-02)\tAcc@1 100.00 ( 99.81)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [97][120/391]\tTime  0.176 ( 0.227)\tLoss 1.1410e-02 (1.7650e-02)\tAcc@1 100.00 ( 99.84)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [97][150/391]\tTime  0.177 ( 0.225)\tLoss 9.9803e-03 (1.7632e-02)\tAcc@1 100.00 ( 99.82)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [97][180/391]\tTime  0.241 ( 0.225)\tLoss 1.3212e-02 (1.7495e-02)\tAcc@1 100.00 ( 99.83)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [97][210/391]\tTime  0.167 ( 0.225)\tLoss 1.6508e-02 (1.7515e-02)\tAcc@1 100.00 ( 99.83)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [97][240/391]\tTime  0.170 ( 0.224)\tLoss 2.3148e-02 (1.7408e-02)\tAcc@1  99.22 ( 99.83)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [97][270/391]\tTime  0.232 ( 0.223)\tLoss 1.9038e-02 (1.7416e-02)\tAcc@1 100.00 ( 99.83)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [97][300/391]\tTime  0.250 ( 0.223)\tLoss 2.2126e-02 (1.7652e-02)\tAcc@1 100.00 ( 99.83)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [97][330/391]\tTime  0.254 ( 0.223)\tLoss 1.1473e-02 (1.7542e-02)\tAcc@1 100.00 ( 99.83)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [97][360/391]\tTime  0.171 ( 0.223)\tLoss 1.4496e-02 (1.7374e-02)\tAcc@1 100.00 ( 99.84)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [97][390/391]\tTime  0.152 ( 0.222)\tLoss 3.5803e-02 (1.7501e-02)\tAcc@1  98.75 ( 99.83)\tAcc@5 100.00 (100.00)\n",
            "==> Train Accuracy: Acc@1 99.834 || Acc@5 100.000\n",
            "==> Test Accuracy:  Acc@1 76.670 || Acc@5 93.640\n",
            "==> 91.14 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 98, lr: 0.004000000000000001 -----\n",
            "Epoch: [98][  0/391]\tTime  0.559 ( 0.559)\tLoss 1.6169e-02 (1.6169e-02)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [98][ 30/391]\tTime  0.268 ( 0.242)\tLoss 1.8916e-02 (1.5363e-02)\tAcc@1  99.22 ( 99.80)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [98][ 60/391]\tTime  0.227 ( 0.231)\tLoss 1.1592e-02 (1.6239e-02)\tAcc@1 100.00 ( 99.80)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [98][ 90/391]\tTime  0.227 ( 0.229)\tLoss 9.6306e-03 (1.6080e-02)\tAcc@1 100.00 ( 99.82)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [98][120/391]\tTime  0.178 ( 0.227)\tLoss 2.5643e-02 (1.6316e-02)\tAcc@1 100.00 ( 99.81)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [98][150/391]\tTime  0.175 ( 0.226)\tLoss 5.5002e-02 (1.6599e-02)\tAcc@1  98.44 ( 99.82)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [98][180/391]\tTime  0.172 ( 0.226)\tLoss 3.0887e-02 (1.6496e-02)\tAcc@1  99.22 ( 99.82)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [98][210/391]\tTime  0.188 ( 0.225)\tLoss 2.1635e-02 (1.6974e-02)\tAcc@1  99.22 ( 99.82)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [98][240/391]\tTime  0.167 ( 0.225)\tLoss 1.0041e-02 (1.6721e-02)\tAcc@1 100.00 ( 99.82)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [98][270/391]\tTime  0.174 ( 0.225)\tLoss 1.3485e-02 (1.6777e-02)\tAcc@1 100.00 ( 99.82)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [98][300/391]\tTime  0.172 ( 0.225)\tLoss 9.4607e-03 (1.6757e-02)\tAcc@1 100.00 ( 99.82)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [98][330/391]\tTime  0.246 ( 0.225)\tLoss 1.5690e-02 (1.6716e-02)\tAcc@1  99.22 ( 99.81)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [98][360/391]\tTime  0.172 ( 0.225)\tLoss 1.0319e-02 (1.6559e-02)\tAcc@1 100.00 ( 99.82)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [98][390/391]\tTime  0.151 ( 0.224)\tLoss 1.9798e-02 (1.6503e-02)\tAcc@1 100.00 ( 99.82)\tAcc@5 100.00 (100.00)\n",
            "==> Train Accuracy: Acc@1 99.822 || Acc@5 100.000\n",
            "==> Test Accuracy:  Acc@1 76.770 || Acc@5 93.730\n",
            "==> 91.79 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 99, lr: 0.004000000000000001 -----\n",
            "Epoch: [99][  0/391]\tTime  0.580 ( 0.580)\tLoss 2.2075e-02 (2.2075e-02)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [99][ 30/391]\tTime  0.277 ( 0.240)\tLoss 1.5328e-02 (1.6219e-02)\tAcc@1 100.00 ( 99.80)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [99][ 60/391]\tTime  0.256 ( 0.227)\tLoss 1.2411e-02 (1.6494e-02)\tAcc@1 100.00 ( 99.80)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [99][ 90/391]\tTime  0.164 ( 0.227)\tLoss 1.7236e-02 (1.5815e-02)\tAcc@1 100.00 ( 99.84)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [99][120/391]\tTime  0.201 ( 0.225)\tLoss 1.2793e-02 (1.5836e-02)\tAcc@1 100.00 ( 99.85)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [99][150/391]\tTime  0.326 ( 0.226)\tLoss 2.3762e-02 (1.5516e-02)\tAcc@1  99.22 ( 99.86)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [99][180/391]\tTime  0.313 ( 0.225)\tLoss 4.1902e-02 (1.5405e-02)\tAcc@1  98.44 ( 99.87)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [99][210/391]\tTime  0.281 ( 0.225)\tLoss 1.1771e-02 (1.5306e-02)\tAcc@1 100.00 ( 99.86)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [99][240/391]\tTime  0.230 ( 0.225)\tLoss 1.2387e-02 (1.5192e-02)\tAcc@1 100.00 ( 99.87)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [99][270/391]\tTime  0.286 ( 0.224)\tLoss 2.0794e-02 (1.5046e-02)\tAcc@1  99.22 ( 99.87)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [99][300/391]\tTime  0.170 ( 0.224)\tLoss 4.9363e-02 (1.5201e-02)\tAcc@1  99.22 ( 99.87)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [99][330/391]\tTime  0.185 ( 0.224)\tLoss 1.8429e-02 (1.5093e-02)\tAcc@1  99.22 ( 99.87)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [99][360/391]\tTime  0.224 ( 0.223)\tLoss 1.7581e-02 (1.5213e-02)\tAcc@1 100.00 ( 99.87)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [99][390/391]\tTime  0.151 ( 0.223)\tLoss 1.3477e-02 (1.5406e-02)\tAcc@1 100.00 ( 99.86)\tAcc@5 100.00 (100.00)\n",
            "==> Train Accuracy: Acc@1 99.864 || Acc@5 100.000\n",
            "==> Test Accuracy:  Acc@1 76.730 || Acc@5 93.490\n",
            "==> 91.35 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 100, lr: 0.004000000000000001 -----\n",
            "Epoch: [100][  0/391]\tTime  0.572 ( 0.572)\tLoss 1.0747e-02 (1.0747e-02)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [100][ 30/391]\tTime  0.288 ( 0.239)\tLoss 1.4392e-02 (1.5339e-02)\tAcc@1 100.00 ( 99.82)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [100][ 60/391]\tTime  0.279 ( 0.231)\tLoss 1.0099e-02 (1.4394e-02)\tAcc@1 100.00 ( 99.86)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [100][ 90/391]\tTime  0.295 ( 0.229)\tLoss 9.0493e-03 (1.4691e-02)\tAcc@1 100.00 ( 99.85)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [100][120/391]\tTime  0.269 ( 0.228)\tLoss 1.6868e-02 (1.4776e-02)\tAcc@1  99.22 ( 99.86)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [100][150/391]\tTime  0.235 ( 0.225)\tLoss 4.0286e-02 (1.5156e-02)\tAcc@1  98.44 ( 99.84)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [100][180/391]\tTime  0.181 ( 0.225)\tLoss 1.3136e-02 (1.5337e-02)\tAcc@1 100.00 ( 99.84)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [100][210/391]\tTime  0.305 ( 0.225)\tLoss 1.3326e-02 (1.5164e-02)\tAcc@1 100.00 ( 99.84)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [100][240/391]\tTime  0.257 ( 0.225)\tLoss 1.2178e-02 (1.5015e-02)\tAcc@1 100.00 ( 99.85)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [100][270/391]\tTime  0.168 ( 0.224)\tLoss 1.1011e-02 (1.4848e-02)\tAcc@1 100.00 ( 99.86)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [100][300/391]\tTime  0.164 ( 0.224)\tLoss 1.6682e-02 (1.4897e-02)\tAcc@1 100.00 ( 99.86)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [100][330/391]\tTime  0.164 ( 0.224)\tLoss 1.0047e-02 (1.4773e-02)\tAcc@1 100.00 ( 99.87)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [100][360/391]\tTime  0.163 ( 0.224)\tLoss 1.5388e-02 (1.4808e-02)\tAcc@1 100.00 ( 99.86)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [100][390/391]\tTime  0.150 ( 0.224)\tLoss 9.0613e-03 (1.4852e-02)\tAcc@1 100.00 ( 99.86)\tAcc@5 100.00 (100.00)\n",
            "==> Train Accuracy: Acc@1 99.858 || Acc@5 100.000\n",
            "==> Test Accuracy:  Acc@1 76.830 || Acc@5 93.690\n",
            "==> 91.68 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 101, lr: 0.004000000000000001 -----\n",
            "Epoch: [101][  0/391]\tTime  0.555 ( 0.555)\tLoss 8.6340e-03 (8.6340e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [101][ 30/391]\tTime  0.243 ( 0.239)\tLoss 1.8432e-02 (1.3412e-02)\tAcc@1  99.22 ( 99.90)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [101][ 60/391]\tTime  0.242 ( 0.231)\tLoss 1.3431e-02 (1.3654e-02)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [101][ 90/391]\tTime  0.189 ( 0.230)\tLoss 1.1118e-02 (1.3318e-02)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [101][120/391]\tTime  0.307 ( 0.227)\tLoss 9.8875e-03 (1.3047e-02)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [101][150/391]\tTime  0.302 ( 0.227)\tLoss 1.2423e-02 (1.2920e-02)\tAcc@1 100.00 ( 99.93)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [101][180/391]\tTime  0.288 ( 0.226)\tLoss 1.0937e-02 (1.2996e-02)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [101][210/391]\tTime  0.246 ( 0.225)\tLoss 1.2555e-02 (1.2886e-02)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [101][240/391]\tTime  0.169 ( 0.225)\tLoss 1.5139e-02 (1.3075e-02)\tAcc@1 100.00 ( 99.93)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [101][270/391]\tTime  0.227 ( 0.224)\tLoss 1.8680e-02 (1.2887e-02)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [101][300/391]\tTime  0.183 ( 0.224)\tLoss 1.1113e-02 (1.2944e-02)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [101][330/391]\tTime  0.160 ( 0.223)\tLoss 1.2943e-02 (1.2947e-02)\tAcc@1 100.00 ( 99.93)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [101][360/391]\tTime  0.233 ( 0.224)\tLoss 1.0989e-02 (1.3101e-02)\tAcc@1 100.00 ( 99.93)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [101][390/391]\tTime  0.150 ( 0.223)\tLoss 1.0017e-02 (1.3117e-02)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n",
            "==> Train Accuracy: Acc@1 99.924 || Acc@5 100.000\n",
            "==> Test Accuracy:  Acc@1 76.830 || Acc@5 93.600\n",
            "==> 91.40 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 102, lr: 0.004000000000000001 -----\n",
            "Epoch: [102][  0/391]\tTime  0.567 ( 0.567)\tLoss 6.7692e-03 (6.7692e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [102][ 30/391]\tTime  0.272 ( 0.232)\tLoss 1.7108e-02 (1.2274e-02)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [102][ 60/391]\tTime  0.266 ( 0.225)\tLoss 2.1341e-02 (1.2839e-02)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [102][ 90/391]\tTime  0.218 ( 0.226)\tLoss 9.0247e-03 (1.3032e-02)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [102][120/391]\tTime  0.312 ( 0.228)\tLoss 9.2684e-03 (1.2997e-02)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [102][150/391]\tTime  0.252 ( 0.227)\tLoss 2.7696e-02 (1.3421e-02)\tAcc@1  99.22 ( 99.90)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [102][180/391]\tTime  0.244 ( 0.228)\tLoss 1.1175e-02 (1.3392e-02)\tAcc@1 100.00 ( 99.90)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [102][210/391]\tTime  0.356 ( 0.227)\tLoss 9.4976e-03 (1.3765e-02)\tAcc@1 100.00 ( 99.90)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [102][240/391]\tTime  0.289 ( 0.227)\tLoss 1.0621e-02 (1.3737e-02)\tAcc@1 100.00 ( 99.90)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [102][270/391]\tTime  0.287 ( 0.227)\tLoss 1.1438e-02 (1.3841e-02)\tAcc@1 100.00 ( 99.90)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [102][300/391]\tTime  0.306 ( 0.228)\tLoss 1.2205e-02 (1.3754e-02)\tAcc@1 100.00 ( 99.90)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [102][330/391]\tTime  0.261 ( 0.227)\tLoss 1.4369e-02 (1.3759e-02)\tAcc@1 100.00 ( 99.90)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [102][360/391]\tTime  0.220 ( 0.227)\tLoss 1.4909e-02 (1.3941e-02)\tAcc@1 100.00 ( 99.89)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [102][390/391]\tTime  0.145 ( 0.226)\tLoss 3.1105e-02 (1.3970e-02)\tAcc@1 100.00 ( 99.89)\tAcc@5 100.00 (100.00)\n",
            "==> Train Accuracy: Acc@1 99.894 || Acc@5 100.000\n",
            "==> Test Accuracy:  Acc@1 76.900 || Acc@5 93.700\n",
            "==> 92.47 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 103, lr: 0.004000000000000001 -----\n",
            "Epoch: [103][  0/391]\tTime  0.574 ( 0.574)\tLoss 1.0976e-02 (1.0976e-02)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [103][ 30/391]\tTime  0.283 ( 0.239)\tLoss 9.7013e-03 (1.2640e-02)\tAcc@1 100.00 ( 99.87)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [103][ 60/391]\tTime  0.300 ( 0.234)\tLoss 1.5407e-02 (1.2824e-02)\tAcc@1  99.22 ( 99.88)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [103][ 90/391]\tTime  0.247 ( 0.230)\tLoss 2.6692e-02 (1.3287e-02)\tAcc@1  99.22 ( 99.85)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [103][120/391]\tTime  0.222 ( 0.228)\tLoss 8.6489e-03 (1.3595e-02)\tAcc@1 100.00 ( 99.86)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [103][150/391]\tTime  0.176 ( 0.227)\tLoss 1.4736e-02 (1.3368e-02)\tAcc@1 100.00 ( 99.86)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [103][180/391]\tTime  0.227 ( 0.227)\tLoss 1.4075e-02 (1.3257e-02)\tAcc@1 100.00 ( 99.86)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [103][210/391]\tTime  0.179 ( 0.225)\tLoss 1.9178e-02 (1.3300e-02)\tAcc@1 100.00 ( 99.86)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [103][240/391]\tTime  0.280 ( 0.225)\tLoss 7.0795e-03 (1.3124e-02)\tAcc@1 100.00 ( 99.87)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [103][270/391]\tTime  0.272 ( 0.225)\tLoss 1.2001e-02 (1.3110e-02)\tAcc@1 100.00 ( 99.87)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [103][300/391]\tTime  0.256 ( 0.224)\tLoss 1.8684e-02 (1.3214e-02)\tAcc@1  99.22 ( 99.87)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [103][330/391]\tTime  0.181 ( 0.223)\tLoss 1.0177e-02 (1.3129e-02)\tAcc@1 100.00 ( 99.88)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [103][360/391]\tTime  0.174 ( 0.224)\tLoss 1.7436e-02 (1.3119e-02)\tAcc@1  99.22 ( 99.88)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [103][390/391]\tTime  0.150 ( 0.224)\tLoss 8.4495e-03 (1.3178e-02)\tAcc@1 100.00 ( 99.88)\tAcc@5 100.00 (100.00)\n",
            "==> Train Accuracy: Acc@1 99.880 || Acc@5 100.000\n",
            "==> Test Accuracy:  Acc@1 76.670 || Acc@5 93.700\n",
            "==> 91.68 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 104, lr: 0.004000000000000001 -----\n",
            "Epoch: [104][  0/391]\tTime  0.551 ( 0.551)\tLoss 1.3279e-02 (1.3279e-02)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [104][ 30/391]\tTime  0.206 ( 0.232)\tLoss 7.4918e-03 (1.2494e-02)\tAcc@1 100.00 ( 99.85)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [104][ 60/391]\tTime  0.220 ( 0.226)\tLoss 8.9476e-03 (1.2758e-02)\tAcc@1 100.00 ( 99.87)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [104][ 90/391]\tTime  0.227 ( 0.224)\tLoss 7.1105e-03 (1.2357e-02)\tAcc@1 100.00 ( 99.89)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [104][120/391]\tTime  0.172 ( 0.221)\tLoss 1.7114e-02 (1.2859e-02)\tAcc@1 100.00 ( 99.88)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [104][150/391]\tTime  0.167 ( 0.221)\tLoss 5.5160e-03 (1.3049e-02)\tAcc@1 100.00 ( 99.87)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [104][180/391]\tTime  0.192 ( 0.222)\tLoss 6.4259e-03 (1.2787e-02)\tAcc@1 100.00 ( 99.88)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [104][210/391]\tTime  0.177 ( 0.223)\tLoss 8.1450e-03 (1.2541e-02)\tAcc@1 100.00 ( 99.89)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [104][240/391]\tTime  0.176 ( 0.222)\tLoss 1.4844e-02 (1.2656e-02)\tAcc@1 100.00 ( 99.90)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [104][270/391]\tTime  0.287 ( 0.222)\tLoss 2.7117e-02 (1.2968e-02)\tAcc@1  98.44 ( 99.88)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [104][300/391]\tTime  0.330 ( 0.222)\tLoss 1.5754e-02 (1.2951e-02)\tAcc@1  99.22 ( 99.88)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [104][330/391]\tTime  0.166 ( 0.222)\tLoss 8.8058e-03 (1.3034e-02)\tAcc@1 100.00 ( 99.88)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [104][360/391]\tTime  0.169 ( 0.222)\tLoss 1.4885e-02 (1.3230e-02)\tAcc@1 100.00 ( 99.88)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [104][390/391]\tTime  0.151 ( 0.222)\tLoss 9.4955e-03 (1.3185e-02)\tAcc@1 100.00 ( 99.88)\tAcc@5 100.00 (100.00)\n",
            "==> Train Accuracy: Acc@1 99.882 || Acc@5 100.000\n",
            "==> Test Accuracy:  Acc@1 76.870 || Acc@5 93.710\n",
            "==> 90.80 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 105, lr: 0.004000000000000001 -----\n",
            "Epoch: [105][  0/391]\tTime  0.564 ( 0.564)\tLoss 6.0574e-03 (6.0574e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [105][ 30/391]\tTime  0.250 ( 0.242)\tLoss 1.4433e-02 (1.2083e-02)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [105][ 60/391]\tTime  0.257 ( 0.233)\tLoss 7.0129e-03 (1.2599e-02)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [105][ 90/391]\tTime  0.297 ( 0.231)\tLoss 9.9068e-03 (1.2080e-02)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [105][120/391]\tTime  0.241 ( 0.228)\tLoss 7.5275e-03 (1.2032e-02)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [105][150/391]\tTime  0.166 ( 0.228)\tLoss 8.9168e-03 (1.2586e-02)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [105][180/391]\tTime  0.183 ( 0.228)\tLoss 8.6372e-03 (1.2674e-02)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [105][210/391]\tTime  0.181 ( 0.227)\tLoss 1.5968e-02 (1.2618e-02)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [105][240/391]\tTime  0.173 ( 0.227)\tLoss 8.8713e-03 (1.2512e-02)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [105][270/391]\tTime  0.184 ( 0.226)\tLoss 3.9025e-02 (1.2625e-02)\tAcc@1  99.22 ( 99.92)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [105][300/391]\tTime  0.174 ( 0.226)\tLoss 1.0342e-02 (1.2671e-02)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [105][330/391]\tTime  0.172 ( 0.226)\tLoss 8.8023e-03 (1.2820e-02)\tAcc@1 100.00 ( 99.90)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [105][360/391]\tTime  0.328 ( 0.226)\tLoss 8.8452e-03 (1.2731e-02)\tAcc@1 100.00 ( 99.90)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [105][390/391]\tTime  0.151 ( 0.225)\tLoss 2.2189e-02 (1.2734e-02)\tAcc@1 100.00 ( 99.90)\tAcc@5 100.00 (100.00)\n",
            "==> Train Accuracy: Acc@1 99.902 || Acc@5 100.000\n",
            "==> Test Accuracy:  Acc@1 76.910 || Acc@5 93.750\n",
            "==> 92.34 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 106, lr: 0.004000000000000001 -----\n",
            "Epoch: [106][  0/391]\tTime  0.572 ( 0.572)\tLoss 2.5208e-02 (2.5208e-02)\tAcc@1  99.22 ( 99.22)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [106][ 30/391]\tTime  0.284 ( 0.236)\tLoss 1.3112e-02 (1.2266e-02)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [106][ 60/391]\tTime  0.160 ( 0.231)\tLoss 9.3591e-03 (1.2514e-02)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [106][ 90/391]\tTime  0.174 ( 0.231)\tLoss 8.8579e-03 (1.2724e-02)\tAcc@1 100.00 ( 99.88)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [106][120/391]\tTime  0.172 ( 0.228)\tLoss 1.4681e-02 (1.2373e-02)\tAcc@1 100.00 ( 99.90)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [106][150/391]\tTime  0.180 ( 0.227)\tLoss 1.1720e-02 (1.2506e-02)\tAcc@1 100.00 ( 99.90)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [106][180/391]\tTime  0.173 ( 0.226)\tLoss 1.5173e-02 (1.2481e-02)\tAcc@1 100.00 ( 99.90)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [106][210/391]\tTime  0.232 ( 0.225)\tLoss 1.4665e-02 (1.2396e-02)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [106][240/391]\tTime  0.275 ( 0.225)\tLoss 8.2567e-03 (1.2478e-02)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [106][270/391]\tTime  0.170 ( 0.224)\tLoss 5.8677e-03 (1.2316e-02)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [106][300/391]\tTime  0.175 ( 0.223)\tLoss 1.4349e-02 (1.2319e-02)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [106][330/391]\tTime  0.329 ( 0.223)\tLoss 1.0024e-02 (1.2334e-02)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [106][360/391]\tTime  0.207 ( 0.223)\tLoss 1.0698e-02 (1.2380e-02)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [106][390/391]\tTime  0.150 ( 0.222)\tLoss 1.7165e-02 (1.2407e-02)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n",
            "==> Train Accuracy: Acc@1 99.908 || Acc@5 100.000\n",
            "==> Test Accuracy:  Acc@1 76.900 || Acc@5 93.680\n",
            "==> 91.08 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 107, lr: 0.004000000000000001 -----\n",
            "Epoch: [107][  0/391]\tTime  0.569 ( 0.569)\tLoss 6.5542e-03 (6.5542e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [107][ 30/391]\tTime  0.178 ( 0.236)\tLoss 1.0090e-02 (1.1373e-02)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [107][ 60/391]\tTime  0.177 ( 0.230)\tLoss 1.3080e-02 (1.1634e-02)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [107][ 90/391]\tTime  0.290 ( 0.230)\tLoss 1.4218e-02 (1.1118e-02)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [107][120/391]\tTime  0.289 ( 0.228)\tLoss 1.6169e-02 (1.1388e-02)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [107][150/391]\tTime  0.315 ( 0.227)\tLoss 1.2965e-02 (1.1530e-02)\tAcc@1 100.00 ( 99.93)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [107][180/391]\tTime  0.359 ( 0.228)\tLoss 7.1912e-03 (1.1524e-02)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [107][210/391]\tTime  0.319 ( 0.228)\tLoss 7.0313e-03 (1.1640e-02)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [107][240/391]\tTime  0.171 ( 0.227)\tLoss 1.9029e-02 (1.1726e-02)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [107][270/391]\tTime  0.180 ( 0.227)\tLoss 2.8692e-02 (1.1831e-02)\tAcc@1  99.22 ( 99.91)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [107][300/391]\tTime  0.176 ( 0.228)\tLoss 1.5021e-02 (1.2076e-02)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [107][330/391]\tTime  0.262 ( 0.227)\tLoss 1.2161e-02 (1.2331e-02)\tAcc@1 100.00 ( 99.90)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [107][360/391]\tTime  0.162 ( 0.227)\tLoss 3.3098e-02 (1.2494e-02)\tAcc@1  99.22 ( 99.90)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [107][390/391]\tTime  0.140 ( 0.227)\tLoss 1.8408e-02 (1.2564e-02)\tAcc@1 100.00 ( 99.90)\tAcc@5 100.00 (100.00)\n",
            "==> Train Accuracy: Acc@1 99.898 || Acc@5 100.000\n",
            "==> Test Accuracy:  Acc@1 76.990 || Acc@5 93.900\n",
            "==> 92.74 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 108, lr: 0.004000000000000001 -----\n",
            "Epoch: [108][  0/391]\tTime  0.582 ( 0.582)\tLoss 8.6553e-03 (8.6553e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [108][ 30/391]\tTime  0.240 ( 0.239)\tLoss 5.5398e-03 (1.1771e-02)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [108][ 60/391]\tTime  0.159 ( 0.232)\tLoss 9.7328e-03 (1.1486e-02)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [108][ 90/391]\tTime  0.282 ( 0.231)\tLoss 1.3102e-02 (1.1807e-02)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [108][120/391]\tTime  0.295 ( 0.232)\tLoss 1.0562e-02 (1.1519e-02)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [108][150/391]\tTime  0.241 ( 0.232)\tLoss 9.4737e-03 (1.1564e-02)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [108][180/391]\tTime  0.169 ( 0.230)\tLoss 1.3799e-02 (1.1503e-02)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [108][210/391]\tTime  0.163 ( 0.229)\tLoss 9.1794e-03 (1.1435e-02)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [108][240/391]\tTime  0.162 ( 0.229)\tLoss 1.1074e-02 (1.1639e-02)\tAcc@1 100.00 ( 99.93)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [108][270/391]\tTime  0.263 ( 0.229)\tLoss 1.0978e-02 (1.1906e-02)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [108][300/391]\tTime  0.267 ( 0.228)\tLoss 9.2058e-03 (1.2021e-02)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [108][330/391]\tTime  0.171 ( 0.227)\tLoss 1.2048e-02 (1.1937e-02)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [108][360/391]\tTime  0.252 ( 0.227)\tLoss 2.1416e-02 (1.1970e-02)\tAcc@1  99.22 ( 99.90)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [108][390/391]\tTime  0.147 ( 0.226)\tLoss 8.6049e-03 (1.1984e-02)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n",
            "==> Train Accuracy: Acc@1 99.906 || Acc@5 100.000\n",
            "==> Test Accuracy:  Acc@1 77.370 || Acc@5 93.820\n",
            "==> 92.43 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 109, lr: 0.004000000000000001 -----\n",
            "Epoch: [109][  0/391]\tTime  0.598 ( 0.598)\tLoss 7.8291e-03 (7.8291e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [109][ 30/391]\tTime  0.301 ( 0.250)\tLoss 5.2232e-03 (9.9838e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [109][ 60/391]\tTime  0.244 ( 0.237)\tLoss 8.9046e-03 (1.0779e-02)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [109][ 90/391]\tTime  0.232 ( 0.232)\tLoss 2.2614e-02 (1.0964e-02)\tAcc@1  99.22 ( 99.93)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [109][120/391]\tTime  0.307 ( 0.230)\tLoss 7.4896e-03 (1.0918e-02)\tAcc@1 100.00 ( 99.93)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [109][150/391]\tTime  0.221 ( 0.228)\tLoss 1.1011e-02 (1.1117e-02)\tAcc@1 100.00 ( 99.93)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [109][180/391]\tTime  0.171 ( 0.228)\tLoss 1.4518e-02 (1.1202e-02)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [109][210/391]\tTime  0.169 ( 0.228)\tLoss 5.9897e-03 (1.1193e-02)\tAcc@1 100.00 ( 99.93)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [109][240/391]\tTime  0.173 ( 0.229)\tLoss 1.3170e-02 (1.1402e-02)\tAcc@1  99.22 ( 99.92)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [109][270/391]\tTime  0.164 ( 0.228)\tLoss 1.5027e-02 (1.1355e-02)\tAcc@1 100.00 ( 99.93)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [109][300/391]\tTime  0.307 ( 0.228)\tLoss 1.1025e-02 (1.1450e-02)\tAcc@1 100.00 ( 99.93)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [109][330/391]\tTime  0.240 ( 0.228)\tLoss 1.1352e-02 (1.1502e-02)\tAcc@1 100.00 ( 99.93)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [109][360/391]\tTime  0.161 ( 0.228)\tLoss 8.8823e-03 (1.1506e-02)\tAcc@1 100.00 ( 99.93)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [109][390/391]\tTime  0.141 ( 0.227)\tLoss 2.0813e-02 (1.1524e-02)\tAcc@1  98.75 ( 99.93)\tAcc@5 100.00 (100.00)\n",
            "==> Train Accuracy: Acc@1 99.928 || Acc@5 100.000\n",
            "==> Test Accuracy:  Acc@1 77.200 || Acc@5 93.850\n",
            "==> 92.84 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 110, lr: 0.004000000000000001 -----\n",
            "Epoch: [110][  0/391]\tTime  0.580 ( 0.580)\tLoss 1.4478e-02 (1.4478e-02)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [110][ 30/391]\tTime  0.291 ( 0.238)\tLoss 1.0793e-02 (1.1116e-02)\tAcc@1 100.00 ( 99.90)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [110][ 60/391]\tTime  0.155 ( 0.229)\tLoss 9.4593e-03 (1.0919e-02)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [110][ 90/391]\tTime  0.313 ( 0.229)\tLoss 1.4583e-02 (1.1048e-02)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [110][120/391]\tTime  0.238 ( 0.228)\tLoss 7.5403e-03 (1.0660e-02)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [110][150/391]\tTime  0.168 ( 0.229)\tLoss 6.0321e-03 (1.0711e-02)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [110][180/391]\tTime  0.224 ( 0.228)\tLoss 6.9573e-03 (1.0821e-02)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [110][210/391]\tTime  0.262 ( 0.228)\tLoss 5.5774e-03 (1.0735e-02)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [110][240/391]\tTime  0.311 ( 0.229)\tLoss 9.4398e-03 (1.0691e-02)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [110][270/391]\tTime  0.287 ( 0.228)\tLoss 3.5799e-02 (1.0883e-02)\tAcc@1  99.22 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [110][300/391]\tTime  0.220 ( 0.229)\tLoss 9.5453e-03 (1.1048e-02)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [110][330/391]\tTime  0.270 ( 0.228)\tLoss 8.0637e-03 (1.1060e-02)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [110][360/391]\tTime  0.242 ( 0.229)\tLoss 6.5805e-03 (1.1192e-02)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [110][390/391]\tTime  0.148 ( 0.228)\tLoss 1.2544e-02 (1.1138e-02)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "==> Train Accuracy: Acc@1 99.946 || Acc@5 100.000\n",
            "==> Test Accuracy:  Acc@1 77.140 || Acc@5 93.830\n",
            "==> 93.17 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 111, lr: 0.004000000000000001 -----\n",
            "Epoch: [111][  0/391]\tTime  0.592 ( 0.592)\tLoss 2.2507e-02 (2.2507e-02)\tAcc@1  99.22 ( 99.22)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [111][ 30/391]\tTime  0.328 ( 0.240)\tLoss 1.1692e-02 (1.0834e-02)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [111][ 60/391]\tTime  0.341 ( 0.233)\tLoss 8.0388e-03 (1.1098e-02)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [111][ 90/391]\tTime  0.188 ( 0.229)\tLoss 8.1939e-03 (1.0674e-02)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [111][120/391]\tTime  0.282 ( 0.227)\tLoss 7.4751e-03 (1.0421e-02)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [111][150/391]\tTime  0.215 ( 0.228)\tLoss 1.1274e-02 (1.0456e-02)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [111][180/391]\tTime  0.179 ( 0.228)\tLoss 1.0336e-02 (1.0552e-02)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [111][210/391]\tTime  0.272 ( 0.227)\tLoss 7.1431e-03 (1.0548e-02)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [111][240/391]\tTime  0.248 ( 0.227)\tLoss 5.8725e-03 (1.0670e-02)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [111][270/391]\tTime  0.155 ( 0.228)\tLoss 8.0142e-03 (1.0699e-02)\tAcc@1 100.00 ( 99.93)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [111][300/391]\tTime  0.315 ( 0.227)\tLoss 5.9447e-03 (1.0928e-02)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [111][330/391]\tTime  0.188 ( 0.226)\tLoss 8.3715e-03 (1.0981e-02)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [111][360/391]\tTime  0.311 ( 0.226)\tLoss 9.1936e-03 (1.1128e-02)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [111][390/391]\tTime  0.147 ( 0.226)\tLoss 2.3969e-02 (1.1096e-02)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n",
            "==> Train Accuracy: Acc@1 99.914 || Acc@5 100.000\n",
            "==> Test Accuracy:  Acc@1 77.300 || Acc@5 93.820\n",
            "==> 92.38 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 112, lr: 0.004000000000000001 -----\n",
            "Epoch: [112][  0/391]\tTime  0.553 ( 0.553)\tLoss 1.7340e-02 (1.7340e-02)\tAcc@1  99.22 ( 99.22)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [112][ 30/391]\tTime  0.303 ( 0.235)\tLoss 1.0087e-02 (1.1528e-02)\tAcc@1 100.00 ( 99.85)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [112][ 60/391]\tTime  0.234 ( 0.229)\tLoss 6.5626e-03 (1.1727e-02)\tAcc@1 100.00 ( 99.88)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [112][ 90/391]\tTime  0.243 ( 0.228)\tLoss 1.8278e-02 (1.1318e-02)\tAcc@1  99.22 ( 99.91)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [112][120/391]\tTime  0.333 ( 0.227)\tLoss 1.2543e-02 (1.1342e-02)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [112][150/391]\tTime  0.166 ( 0.227)\tLoss 8.9506e-03 (1.1212e-02)\tAcc@1 100.00 ( 99.93)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [112][180/391]\tTime  0.173 ( 0.227)\tLoss 8.4077e-03 (1.1515e-02)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [112][210/391]\tTime  0.320 ( 0.227)\tLoss 1.2521e-02 (1.1399e-02)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [112][240/391]\tTime  0.172 ( 0.227)\tLoss 7.6230e-03 (1.1590e-02)\tAcc@1 100.00 ( 99.93)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [112][270/391]\tTime  0.166 ( 0.227)\tLoss 7.5925e-03 (1.1674e-02)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [112][300/391]\tTime  0.220 ( 0.227)\tLoss 1.9945e-02 (1.1745e-02)\tAcc@1  99.22 ( 99.92)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [112][330/391]\tTime  0.216 ( 0.226)\tLoss 1.7006e-02 (1.1741e-02)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [112][360/391]\tTime  0.317 ( 0.226)\tLoss 1.1568e-02 (1.1750e-02)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [112][390/391]\tTime  0.141 ( 0.226)\tLoss 6.0285e-03 (1.1651e-02)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n",
            "==> Train Accuracy: Acc@1 99.924 || Acc@5 100.000\n",
            "==> Test Accuracy:  Acc@1 77.180 || Acc@5 93.890\n",
            "==> 92.32 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 113, lr: 0.004000000000000001 -----\n",
            "Epoch: [113][  0/391]\tTime  0.587 ( 0.587)\tLoss 7.0970e-03 (7.0970e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [113][ 30/391]\tTime  0.322 ( 0.241)\tLoss 8.0250e-03 (1.1521e-02)\tAcc@1 100.00 ( 99.90)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [113][ 60/391]\tTime  0.240 ( 0.233)\tLoss 1.0318e-02 (1.1652e-02)\tAcc@1 100.00 ( 99.88)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [113][ 90/391]\tTime  0.321 ( 0.231)\tLoss 1.1849e-02 (1.1096e-02)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [113][120/391]\tTime  0.275 ( 0.231)\tLoss 6.6195e-03 (1.1245e-02)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [113][150/391]\tTime  0.258 ( 0.231)\tLoss 1.2294e-02 (1.1336e-02)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [113][180/391]\tTime  0.267 ( 0.230)\tLoss 6.6023e-03 (1.1166e-02)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [113][210/391]\tTime  0.300 ( 0.231)\tLoss 6.4088e-03 (1.1219e-02)\tAcc@1 100.00 ( 99.90)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [113][240/391]\tTime  0.354 ( 0.230)\tLoss 1.3412e-02 (1.1127e-02)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [113][270/391]\tTime  0.259 ( 0.230)\tLoss 1.5796e-02 (1.1123e-02)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [113][300/391]\tTime  0.175 ( 0.230)\tLoss 8.1449e-03 (1.1059e-02)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [113][330/391]\tTime  0.179 ( 0.230)\tLoss 7.6816e-03 (1.1071e-02)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [113][360/391]\tTime  0.272 ( 0.230)\tLoss 7.4794e-03 (1.1052e-02)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [113][390/391]\tTime  0.147 ( 0.229)\tLoss 1.4316e-02 (1.1086e-02)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n",
            "==> Train Accuracy: Acc@1 99.924 || Acc@5 100.000\n",
            "==> Test Accuracy:  Acc@1 77.440 || Acc@5 93.850\n",
            "==> 93.52 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 114, lr: 0.004000000000000001 -----\n",
            "Epoch: [114][  0/391]\tTime  0.591 ( 0.591)\tLoss 9.5727e-03 (9.5727e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [114][ 30/391]\tTime  0.266 ( 0.244)\tLoss 1.3950e-02 (1.1705e-02)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [114][ 60/391]\tTime  0.247 ( 0.233)\tLoss 1.0095e-02 (1.1873e-02)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [114][ 90/391]\tTime  0.163 ( 0.229)\tLoss 7.6147e-03 (1.1335e-02)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [114][120/391]\tTime  0.181 ( 0.231)\tLoss 5.6717e-03 (1.1153e-02)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [114][150/391]\tTime  0.348 ( 0.230)\tLoss 8.0626e-03 (1.1138e-02)\tAcc@1 100.00 ( 99.93)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [114][180/391]\tTime  0.286 ( 0.229)\tLoss 1.5036e-02 (1.1062e-02)\tAcc@1  99.22 ( 99.93)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [114][210/391]\tTime  0.280 ( 0.229)\tLoss 9.0636e-03 (1.1164e-02)\tAcc@1 100.00 ( 99.93)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [114][240/391]\tTime  0.275 ( 0.229)\tLoss 9.3421e-03 (1.1343e-02)\tAcc@1 100.00 ( 99.93)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [114][270/391]\tTime  0.282 ( 0.228)\tLoss 6.3883e-03 (1.1444e-02)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [114][300/391]\tTime  0.201 ( 0.228)\tLoss 7.8244e-03 (1.1476e-02)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [114][330/391]\tTime  0.171 ( 0.228)\tLoss 1.4134e-02 (1.1821e-02)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [114][360/391]\tTime  0.165 ( 0.228)\tLoss 1.7553e-02 (1.1803e-02)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [114][390/391]\tTime  0.146 ( 0.227)\tLoss 9.0793e-03 (1.1949e-02)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n",
            "==> Train Accuracy: Acc@1 99.908 || Acc@5 100.000\n",
            "==> Test Accuracy:  Acc@1 77.390 || Acc@5 94.000\n",
            "==> 92.81 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 115, lr: 0.004000000000000001 -----\n",
            "Epoch: [115][  0/391]\tTime  0.600 ( 0.600)\tLoss 8.4457e-03 (8.4457e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [115][ 30/391]\tTime  0.300 ( 0.247)\tLoss 5.3876e-02 (1.1267e-02)\tAcc@1  98.44 ( 99.90)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [115][ 60/391]\tTime  0.319 ( 0.239)\tLoss 7.8943e-03 (1.0980e-02)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [115][ 90/391]\tTime  0.284 ( 0.233)\tLoss 1.2225e-02 (1.1130e-02)\tAcc@1 100.00 ( 99.93)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [115][120/391]\tTime  0.271 ( 0.233)\tLoss 9.7289e-03 (1.0973e-02)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [115][150/391]\tTime  0.238 ( 0.230)\tLoss 8.2013e-03 (1.0739e-02)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [115][180/391]\tTime  0.160 ( 0.230)\tLoss 9.5758e-03 (1.1007e-02)\tAcc@1 100.00 ( 99.93)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [115][210/391]\tTime  0.167 ( 0.230)\tLoss 6.0638e-03 (1.0813e-02)\tAcc@1 100.00 ( 99.93)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [115][240/391]\tTime  0.180 ( 0.231)\tLoss 1.0662e-02 (1.0877e-02)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [115][270/391]\tTime  0.164 ( 0.231)\tLoss 1.0132e-02 (1.0761e-02)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [115][300/391]\tTime  0.256 ( 0.230)\tLoss 1.4368e-02 (1.0984e-02)\tAcc@1 100.00 ( 99.93)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [115][330/391]\tTime  0.277 ( 0.230)\tLoss 9.8767e-03 (1.0955e-02)\tAcc@1 100.00 ( 99.93)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [115][360/391]\tTime  0.230 ( 0.229)\tLoss 8.0030e-03 (1.1161e-02)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [115][390/391]\tTime  0.147 ( 0.229)\tLoss 9.7676e-03 (1.1168e-02)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n",
            "==> Train Accuracy: Acc@1 99.924 || Acc@5 100.000\n",
            "==> Test Accuracy:  Acc@1 77.290 || Acc@5 93.850\n",
            "==> 93.57 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 116, lr: 0.004000000000000001 -----\n",
            "Epoch: [116][  0/391]\tTime  0.591 ( 0.591)\tLoss 7.6755e-03 (7.6755e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [116][ 30/391]\tTime  0.167 ( 0.239)\tLoss 5.9900e-03 (8.9642e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [116][ 60/391]\tTime  0.204 ( 0.233)\tLoss 9.6169e-03 (9.8173e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [116][ 90/391]\tTime  0.175 ( 0.231)\tLoss 5.3231e-03 (1.0006e-02)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [116][120/391]\tTime  0.193 ( 0.230)\tLoss 1.1541e-02 (1.0067e-02)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [116][150/391]\tTime  0.260 ( 0.229)\tLoss 8.6023e-03 (1.0119e-02)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [116][180/391]\tTime  0.199 ( 0.228)\tLoss 1.4148e-02 (1.0574e-02)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [116][210/391]\tTime  0.243 ( 0.227)\tLoss 5.3972e-03 (1.0546e-02)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [116][240/391]\tTime  0.291 ( 0.227)\tLoss 9.2956e-03 (1.0595e-02)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [116][270/391]\tTime  0.192 ( 0.226)\tLoss 8.8292e-03 (1.0441e-02)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [116][300/391]\tTime  0.245 ( 0.226)\tLoss 7.9935e-03 (1.0624e-02)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [116][330/391]\tTime  0.218 ( 0.225)\tLoss 1.0577e-02 (1.0639e-02)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [116][360/391]\tTime  0.334 ( 0.225)\tLoss 9.5539e-03 (1.0586e-02)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [116][390/391]\tTime  0.142 ( 0.225)\tLoss 1.3339e-02 (1.0597e-02)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "==> Train Accuracy: Acc@1 99.954 || Acc@5 100.000\n",
            "==> Test Accuracy:  Acc@1 77.190 || Acc@5 93.840\n",
            "==> 91.95 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 117, lr: 0.004000000000000001 -----\n",
            "Epoch: [117][  0/391]\tTime  0.586 ( 0.586)\tLoss 1.1334e-02 (1.1334e-02)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [117][ 30/391]\tTime  0.288 ( 0.239)\tLoss 7.9192e-03 (9.5220e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [117][ 60/391]\tTime  0.184 ( 0.227)\tLoss 9.2867e-03 (9.7050e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [117][ 90/391]\tTime  0.313 ( 0.225)\tLoss 1.6952e-02 (1.0066e-02)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [117][120/391]\tTime  0.212 ( 0.226)\tLoss 7.5468e-03 (1.0068e-02)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [117][150/391]\tTime  0.173 ( 0.223)\tLoss 8.1207e-03 (1.0205e-02)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [117][180/391]\tTime  0.172 ( 0.224)\tLoss 9.8786e-03 (1.0256e-02)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [117][210/391]\tTime  0.182 ( 0.225)\tLoss 2.9118e-02 (1.0741e-02)\tAcc@1  99.22 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [117][240/391]\tTime  0.171 ( 0.226)\tLoss 7.2334e-03 (1.0819e-02)\tAcc@1 100.00 ( 99.93)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [117][270/391]\tTime  0.163 ( 0.225)\tLoss 1.5878e-02 (1.0827e-02)\tAcc@1 100.00 ( 99.93)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [117][300/391]\tTime  0.178 ( 0.225)\tLoss 1.3143e-02 (1.0963e-02)\tAcc@1 100.00 ( 99.93)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [117][330/391]\tTime  0.185 ( 0.225)\tLoss 6.8495e-03 (1.0963e-02)\tAcc@1 100.00 ( 99.93)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [117][360/391]\tTime  0.172 ( 0.225)\tLoss 1.2174e-02 (1.0958e-02)\tAcc@1 100.00 ( 99.93)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [117][390/391]\tTime  0.150 ( 0.225)\tLoss 9.7688e-03 (1.0850e-02)\tAcc@1 100.00 ( 99.93)\tAcc@5 100.00 (100.00)\n",
            "==> Train Accuracy: Acc@1 99.934 || Acc@5 100.000\n",
            "==> Test Accuracy:  Acc@1 77.190 || Acc@5 93.850\n",
            "==> 92.08 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 118, lr: 0.004000000000000001 -----\n",
            "Epoch: [118][  0/391]\tTime  0.577 ( 0.577)\tLoss 1.6696e-02 (1.6696e-02)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [118][ 30/391]\tTime  0.170 ( 0.235)\tLoss 9.3126e-03 (1.0370e-02)\tAcc@1 100.00 ( 99.90)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [118][ 60/391]\tTime  0.173 ( 0.229)\tLoss 8.8000e-03 (1.0969e-02)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [118][ 90/391]\tTime  0.256 ( 0.228)\tLoss 2.0368e-02 (1.0731e-02)\tAcc@1  99.22 ( 99.91)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [118][120/391]\tTime  0.158 ( 0.228)\tLoss 7.3485e-03 (1.0508e-02)\tAcc@1 100.00 ( 99.93)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [118][150/391]\tTime  0.172 ( 0.228)\tLoss 2.0839e-02 (1.0353e-02)\tAcc@1  99.22 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [118][180/391]\tTime  0.171 ( 0.228)\tLoss 5.7311e-03 (1.0395e-02)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [118][210/391]\tTime  0.178 ( 0.229)\tLoss 1.3656e-02 (1.0279e-02)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [118][240/391]\tTime  0.171 ( 0.228)\tLoss 1.0354e-02 (1.0299e-02)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [118][270/391]\tTime  0.167 ( 0.227)\tLoss 1.1571e-02 (1.0323e-02)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [118][300/391]\tTime  0.205 ( 0.227)\tLoss 9.2235e-03 (1.0362e-02)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [118][330/391]\tTime  0.176 ( 0.226)\tLoss 1.3790e-02 (1.0350e-02)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [118][360/391]\tTime  0.170 ( 0.227)\tLoss 1.3669e-02 (1.0485e-02)\tAcc@1  99.22 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [118][390/391]\tTime  0.150 ( 0.226)\tLoss 9.5851e-03 (1.0581e-02)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "==> Train Accuracy: Acc@1 99.938 || Acc@5 100.000\n",
            "==> Test Accuracy:  Acc@1 77.220 || Acc@5 93.890\n",
            "==> 92.58 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 119, lr: 0.004000000000000001 -----\n",
            "Epoch: [119][  0/391]\tTime  0.587 ( 0.587)\tLoss 1.8083e-02 (1.8083e-02)\tAcc@1  99.22 ( 99.22)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [119][ 30/391]\tTime  0.320 ( 0.243)\tLoss 1.0851e-02 (9.9323e-03)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [119][ 60/391]\tTime  0.284 ( 0.231)\tLoss 1.6387e-02 (1.0107e-02)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [119][ 90/391]\tTime  0.293 ( 0.230)\tLoss 8.0617e-03 (1.0215e-02)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [119][120/391]\tTime  0.235 ( 0.225)\tLoss 1.4218e-02 (1.0228e-02)\tAcc@1  99.22 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [119][150/391]\tTime  0.191 ( 0.224)\tLoss 1.1461e-02 (1.0300e-02)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [119][180/391]\tTime  0.160 ( 0.224)\tLoss 1.1711e-02 (1.0299e-02)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [119][210/391]\tTime  0.186 ( 0.225)\tLoss 1.5506e-02 (1.0253e-02)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [119][240/391]\tTime  0.325 ( 0.225)\tLoss 1.2510e-02 (1.0349e-02)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [119][270/391]\tTime  0.279 ( 0.225)\tLoss 8.3893e-03 (1.0456e-02)\tAcc@1 100.00 ( 99.93)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [119][300/391]\tTime  0.215 ( 0.224)\tLoss 6.5595e-03 (1.0416e-02)\tAcc@1 100.00 ( 99.93)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [119][330/391]\tTime  0.195 ( 0.225)\tLoss 2.0172e-02 (1.0398e-02)\tAcc@1  99.22 ( 99.93)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [119][360/391]\tTime  0.311 ( 0.225)\tLoss 5.9298e-03 (1.0515e-02)\tAcc@1 100.00 ( 99.93)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [119][390/391]\tTime  0.150 ( 0.224)\tLoss 6.8665e-03 (1.0631e-02)\tAcc@1 100.00 ( 99.93)\tAcc@5 100.00 (100.00)\n",
            "==> Train Accuracy: Acc@1 99.926 || Acc@5 100.000\n",
            "==> Test Accuracy:  Acc@1 77.560 || Acc@5 93.940\n",
            "==> 91.68 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 120, lr: 0.0008000000000000003 -----\n",
            "Epoch: [120][  0/391]\tTime  0.565 ( 0.565)\tLoss 1.0778e-02 (1.0778e-02)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [120][ 30/391]\tTime  0.175 ( 0.233)\tLoss 7.3736e-03 (1.1547e-02)\tAcc@1 100.00 ( 99.87)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [120][ 60/391]\tTime  0.158 ( 0.230)\tLoss 8.6992e-03 (1.0525e-02)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [120][ 90/391]\tTime  0.160 ( 0.228)\tLoss 9.8585e-03 (1.0048e-02)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [120][120/391]\tTime  0.170 ( 0.227)\tLoss 1.2399e-02 (1.0057e-02)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [120][150/391]\tTime  0.242 ( 0.226)\tLoss 1.0277e-02 (9.9782e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [120][180/391]\tTime  0.302 ( 0.225)\tLoss 1.9853e-02 (9.9286e-03)\tAcc@1  99.22 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [120][210/391]\tTime  0.173 ( 0.224)\tLoss 9.5118e-03 (9.9547e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [120][240/391]\tTime  0.332 ( 0.225)\tLoss 9.6716e-03 (1.0001e-02)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [120][270/391]\tTime  0.264 ( 0.224)\tLoss 7.8153e-03 (1.0140e-02)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [120][300/391]\tTime  0.168 ( 0.224)\tLoss 9.2363e-03 (1.0124e-02)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [120][330/391]\tTime  0.272 ( 0.225)\tLoss 1.0485e-02 (1.0018e-02)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [120][360/391]\tTime  0.210 ( 0.225)\tLoss 8.0602e-03 (1.0008e-02)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [120][390/391]\tTime  0.150 ( 0.225)\tLoss 2.2206e-02 (9.9765e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "==> Train Accuracy: Acc@1 99.948 || Acc@5 100.000\n",
            "==> Test Accuracy:  Acc@1 77.230 || Acc@5 93.930\n",
            "==> 91.97 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 121, lr: 0.0008000000000000003 -----\n",
            "Epoch: [121][  0/391]\tTime  0.572 ( 0.572)\tLoss 6.2200e-03 (6.2200e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [121][ 30/391]\tTime  0.310 ( 0.234)\tLoss 8.8909e-03 (9.3316e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [121][ 60/391]\tTime  0.282 ( 0.235)\tLoss 1.6641e-02 (9.0895e-03)\tAcc@1 100.00 ( 99.99)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [121][ 90/391]\tTime  0.169 ( 0.229)\tLoss 1.7261e-02 (8.9524e-03)\tAcc@1  99.22 ( 99.98)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [121][120/391]\tTime  0.167 ( 0.227)\tLoss 2.2757e-02 (9.1280e-03)\tAcc@1  99.22 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [121][150/391]\tTime  0.290 ( 0.227)\tLoss 8.6723e-03 (9.3630e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [121][180/391]\tTime  0.335 ( 0.227)\tLoss 8.5103e-03 (9.2021e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [121][210/391]\tTime  0.172 ( 0.226)\tLoss 7.3294e-03 (9.2930e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [121][240/391]\tTime  0.165 ( 0.227)\tLoss 7.8097e-03 (9.3804e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [121][270/391]\tTime  0.220 ( 0.227)\tLoss 1.3810e-02 (9.3829e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [121][300/391]\tTime  0.226 ( 0.226)\tLoss 7.1781e-03 (9.3498e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [121][330/391]\tTime  0.174 ( 0.226)\tLoss 1.1322e-02 (9.3659e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [121][360/391]\tTime  0.172 ( 0.227)\tLoss 7.6830e-03 (9.3792e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [121][390/391]\tTime  0.151 ( 0.226)\tLoss 7.6476e-03 (9.4610e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "==> Train Accuracy: Acc@1 99.960 || Acc@5 100.000\n",
            "==> Test Accuracy:  Acc@1 77.510 || Acc@5 94.060\n",
            "==> 92.63 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 122, lr: 0.0008000000000000003 -----\n",
            "Epoch: [122][  0/391]\tTime  0.556 ( 0.556)\tLoss 9.1402e-03 (9.1402e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [122][ 30/391]\tTime  0.319 ( 0.234)\tLoss 8.1091e-03 (9.5695e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [122][ 60/391]\tTime  0.341 ( 0.232)\tLoss 1.0522e-02 (9.3842e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [122][ 90/391]\tTime  0.347 ( 0.234)\tLoss 7.3170e-03 (9.0320e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [122][120/391]\tTime  0.177 ( 0.233)\tLoss 1.7653e-02 (9.4166e-03)\tAcc@1  99.22 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [122][150/391]\tTime  0.176 ( 0.230)\tLoss 6.3970e-03 (9.1911e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [122][180/391]\tTime  0.157 ( 0.231)\tLoss 8.3694e-03 (9.1407e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [122][210/391]\tTime  0.179 ( 0.230)\tLoss 7.2644e-03 (9.1011e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [122][240/391]\tTime  0.166 ( 0.231)\tLoss 6.4638e-03 (9.1476e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [122][270/391]\tTime  0.161 ( 0.231)\tLoss 6.9489e-03 (9.1884e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [122][300/391]\tTime  0.165 ( 0.231)\tLoss 1.1654e-02 (9.2784e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [122][330/391]\tTime  0.279 ( 0.230)\tLoss 7.0409e-03 (9.2298e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [122][360/391]\tTime  0.252 ( 0.230)\tLoss 6.5684e-03 (9.3082e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [122][390/391]\tTime  0.141 ( 0.229)\tLoss 1.4343e-02 (9.4308e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "==> Train Accuracy: Acc@1 99.960 || Acc@5 100.000\n",
            "==> Test Accuracy:  Acc@1 77.500 || Acc@5 94.040\n",
            "==> 93.64 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 123, lr: 0.0008000000000000003 -----\n",
            "Epoch: [123][  0/391]\tTime  0.562 ( 0.562)\tLoss 8.0465e-03 (8.0465e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [123][ 30/391]\tTime  0.249 ( 0.228)\tLoss 8.9529e-03 (8.7474e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [123][ 60/391]\tTime  0.176 ( 0.225)\tLoss 5.5429e-03 (9.6301e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [123][ 90/391]\tTime  0.174 ( 0.225)\tLoss 1.9365e-02 (9.7322e-03)\tAcc@1  99.22 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [123][120/391]\tTime  0.220 ( 0.223)\tLoss 1.1185e-02 (9.6523e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [123][150/391]\tTime  0.239 ( 0.224)\tLoss 8.0599e-03 (9.7173e-03)\tAcc@1 100.00 ( 99.93)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [123][180/391]\tTime  0.230 ( 0.224)\tLoss 7.0134e-03 (9.6139e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [123][210/391]\tTime  0.210 ( 0.226)\tLoss 8.0944e-03 (9.4891e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [123][240/391]\tTime  0.317 ( 0.226)\tLoss 8.5440e-03 (9.5604e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [123][270/391]\tTime  0.362 ( 0.226)\tLoss 1.2924e-02 (9.5091e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [123][300/391]\tTime  0.230 ( 0.225)\tLoss 8.6289e-03 (9.5298e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [123][330/391]\tTime  0.169 ( 0.226)\tLoss 7.4069e-03 (9.5566e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [123][360/391]\tTime  0.171 ( 0.225)\tLoss 8.0520e-03 (9.5623e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [123][390/391]\tTime  0.145 ( 0.224)\tLoss 1.3329e-02 (9.5845e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "==> Train Accuracy: Acc@1 99.950 || Acc@5 100.000\n",
            "==> Test Accuracy:  Acc@1 77.350 || Acc@5 94.060\n",
            "==> 91.95 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 124, lr: 0.0008000000000000003 -----\n",
            "Epoch: [124][  0/391]\tTime  0.563 ( 0.563)\tLoss 8.4610e-03 (8.4610e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [124][ 30/391]\tTime  0.171 ( 0.227)\tLoss 5.3775e-03 (8.7569e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [124][ 60/391]\tTime  0.169 ( 0.228)\tLoss 1.0244e-02 (9.3415e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [124][ 90/391]\tTime  0.171 ( 0.225)\tLoss 7.5708e-03 (9.1357e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [124][120/391]\tTime  0.176 ( 0.224)\tLoss 7.6313e-03 (9.0653e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [124][150/391]\tTime  0.178 ( 0.225)\tLoss 1.4288e-02 (9.0888e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [124][180/391]\tTime  0.203 ( 0.225)\tLoss 1.3407e-02 (9.1598e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [124][210/391]\tTime  0.312 ( 0.226)\tLoss 7.3131e-03 (9.2954e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [124][240/391]\tTime  0.188 ( 0.225)\tLoss 1.1353e-02 (9.3522e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [124][270/391]\tTime  0.177 ( 0.225)\tLoss 5.3353e-03 (9.4780e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [124][300/391]\tTime  0.190 ( 0.225)\tLoss 1.0329e-02 (9.5321e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [124][330/391]\tTime  0.173 ( 0.224)\tLoss 1.1307e-02 (9.6163e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [124][360/391]\tTime  0.169 ( 0.225)\tLoss 1.0549e-02 (9.5349e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [124][390/391]\tTime  0.150 ( 0.224)\tLoss 1.1504e-02 (9.5671e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "==> Train Accuracy: Acc@1 99.950 || Acc@5 100.000\n",
            "==> Test Accuracy:  Acc@1 77.320 || Acc@5 94.030\n",
            "==> 91.81 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 125, lr: 0.0008000000000000003 -----\n",
            "Epoch: [125][  0/391]\tTime  0.568 ( 0.568)\tLoss 1.2005e-02 (1.2005e-02)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [125][ 30/391]\tTime  0.184 ( 0.229)\tLoss 1.2331e-02 (9.3309e-03)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [125][ 60/391]\tTime  0.185 ( 0.230)\tLoss 1.0393e-02 (9.7909e-03)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [125][ 90/391]\tTime  0.176 ( 0.226)\tLoss 1.1398e-02 (9.4926e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [125][120/391]\tTime  0.176 ( 0.223)\tLoss 1.0332e-02 (9.2450e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [125][150/391]\tTime  0.173 ( 0.224)\tLoss 2.2452e-02 (9.3353e-03)\tAcc@1  99.22 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [125][180/391]\tTime  0.180 ( 0.224)\tLoss 6.0635e-03 (9.3505e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [125][210/391]\tTime  0.165 ( 0.224)\tLoss 1.6065e-02 (9.3372e-03)\tAcc@1  99.22 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [125][240/391]\tTime  0.187 ( 0.224)\tLoss 7.3576e-03 (9.4049e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [125][270/391]\tTime  0.177 ( 0.223)\tLoss 6.6432e-03 (9.3949e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [125][300/391]\tTime  0.261 ( 0.222)\tLoss 7.3568e-03 (9.4353e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [125][330/391]\tTime  0.212 ( 0.222)\tLoss 9.0676e-03 (9.3780e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [125][360/391]\tTime  0.171 ( 0.223)\tLoss 1.6616e-02 (9.3277e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [125][390/391]\tTime  0.151 ( 0.222)\tLoss 1.0520e-02 (9.3643e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "==> Train Accuracy: Acc@1 99.968 || Acc@5 100.000\n",
            "==> Test Accuracy:  Acc@1 77.400 || Acc@5 93.960\n",
            "==> 91.06 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 126, lr: 0.0008000000000000003 -----\n",
            "Epoch: [126][  0/391]\tTime  0.606 ( 0.606)\tLoss 3.8362e-03 (3.8362e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [126][ 30/391]\tTime  0.286 ( 0.239)\tLoss 7.3769e-03 (9.7084e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [126][ 60/391]\tTime  0.271 ( 0.232)\tLoss 1.2886e-02 (9.6830e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [126][ 90/391]\tTime  0.159 ( 0.226)\tLoss 1.5237e-02 (9.4659e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [126][120/391]\tTime  0.175 ( 0.227)\tLoss 8.0861e-03 (9.3295e-03)\tAcc@1 100.00 ( 99.98)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [126][150/391]\tTime  0.167 ( 0.225)\tLoss 1.2259e-02 (9.3498e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [126][180/391]\tTime  0.169 ( 0.226)\tLoss 6.9270e-03 (9.2645e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [126][210/391]\tTime  0.168 ( 0.225)\tLoss 8.2827e-03 (9.3456e-03)\tAcc@1 100.00 ( 99.98)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [126][240/391]\tTime  0.169 ( 0.225)\tLoss 6.1060e-03 (9.2854e-03)\tAcc@1 100.00 ( 99.98)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [126][270/391]\tTime  0.184 ( 0.224)\tLoss 9.6472e-03 (9.2814e-03)\tAcc@1 100.00 ( 99.98)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [126][300/391]\tTime  0.167 ( 0.224)\tLoss 8.6077e-03 (9.3267e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [126][330/391]\tTime  0.175 ( 0.224)\tLoss 1.1209e-02 (9.3063e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [126][360/391]\tTime  0.278 ( 0.223)\tLoss 7.9475e-03 (9.3645e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [126][390/391]\tTime  0.151 ( 0.223)\tLoss 2.0432e-02 (9.3578e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "==> Train Accuracy: Acc@1 99.974 || Acc@5 100.000\n",
            "==> Test Accuracy:  Acc@1 77.480 || Acc@5 94.030\n",
            "==> 91.44 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 127, lr: 0.0008000000000000003 -----\n",
            "Epoch: [127][  0/391]\tTime  0.556 ( 0.556)\tLoss 7.4211e-03 (7.4211e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [127][ 30/391]\tTime  0.234 ( 0.226)\tLoss 1.0198e-02 (9.1770e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [127][ 60/391]\tTime  0.334 ( 0.222)\tLoss 8.8231e-03 (9.3677e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [127][ 90/391]\tTime  0.184 ( 0.219)\tLoss 2.5026e-02 (9.1669e-03)\tAcc@1  99.22 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [127][120/391]\tTime  0.169 ( 0.221)\tLoss 1.0564e-02 (9.2341e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [127][150/391]\tTime  0.156 ( 0.221)\tLoss 1.3431e-02 (9.3830e-03)\tAcc@1  99.22 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [127][180/391]\tTime  0.309 ( 0.221)\tLoss 7.3034e-03 (9.3847e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [127][210/391]\tTime  0.216 ( 0.221)\tLoss 6.5884e-03 (9.3516e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [127][240/391]\tTime  0.163 ( 0.220)\tLoss 7.4787e-03 (9.4164e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [127][270/391]\tTime  0.153 ( 0.220)\tLoss 8.3231e-03 (9.6092e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [127][300/391]\tTime  0.336 ( 0.221)\tLoss 6.7082e-03 (9.6364e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [127][330/391]\tTime  0.273 ( 0.221)\tLoss 1.3538e-02 (9.5839e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [127][360/391]\tTime  0.255 ( 0.222)\tLoss 4.7203e-03 (9.5009e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [127][390/391]\tTime  0.152 ( 0.221)\tLoss 1.0884e-02 (9.5002e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "==> Train Accuracy: Acc@1 99.956 || Acc@5 100.000\n",
            "==> Test Accuracy:  Acc@1 77.530 || Acc@5 93.960\n",
            "==> 90.57 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 128, lr: 0.0008000000000000003 -----\n",
            "Epoch: [128][  0/391]\tTime  0.556 ( 0.556)\tLoss 8.3558e-03 (8.3558e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [128][ 30/391]\tTime  0.255 ( 0.234)\tLoss 8.9213e-03 (8.2130e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [128][ 60/391]\tTime  0.303 ( 0.230)\tLoss 6.0945e-03 (8.5368e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [128][ 90/391]\tTime  0.312 ( 0.228)\tLoss 8.6071e-03 (8.3932e-03)\tAcc@1 100.00 ( 99.98)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [128][120/391]\tTime  0.296 ( 0.226)\tLoss 6.6691e-03 (8.6821e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [128][150/391]\tTime  0.260 ( 0.224)\tLoss 5.2586e-03 (8.8045e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [128][180/391]\tTime  0.342 ( 0.223)\tLoss 2.8054e-02 (9.0950e-03)\tAcc@1  99.22 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [128][210/391]\tTime  0.224 ( 0.223)\tLoss 5.8066e-03 (9.0286e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [128][240/391]\tTime  0.242 ( 0.222)\tLoss 6.8831e-03 (9.1075e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [128][270/391]\tTime  0.180 ( 0.222)\tLoss 8.1017e-03 (9.1206e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [128][300/391]\tTime  0.162 ( 0.222)\tLoss 7.5249e-03 (9.1145e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [128][330/391]\tTime  0.178 ( 0.222)\tLoss 4.8276e-03 (9.0525e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [128][360/391]\tTime  0.169 ( 0.222)\tLoss 7.6624e-03 (9.1115e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [128][390/391]\tTime  0.150 ( 0.221)\tLoss 2.7411e-02 (9.2123e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "==> Train Accuracy: Acc@1 99.948 || Acc@5 100.000\n",
            "==> Test Accuracy:  Acc@1 77.550 || Acc@5 94.090\n",
            "==> 90.77 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 129, lr: 0.0008000000000000003 -----\n",
            "Epoch: [129][  0/391]\tTime  0.554 ( 0.554)\tLoss 1.2694e-02 (1.2694e-02)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [129][ 30/391]\tTime  0.267 ( 0.233)\tLoss 7.0317e-03 (8.2642e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [129][ 60/391]\tTime  0.229 ( 0.228)\tLoss 3.4635e-02 (9.4772e-03)\tAcc@1  99.22 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [129][ 90/391]\tTime  0.309 ( 0.228)\tLoss 6.2990e-03 (9.4199e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [129][120/391]\tTime  0.250 ( 0.226)\tLoss 8.1605e-03 (9.2339e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [129][150/391]\tTime  0.282 ( 0.227)\tLoss 9.3630e-03 (9.2079e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [129][180/391]\tTime  0.323 ( 0.227)\tLoss 5.2788e-03 (9.2068e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [129][210/391]\tTime  0.276 ( 0.226)\tLoss 1.0247e-02 (9.1854e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [129][240/391]\tTime  0.216 ( 0.225)\tLoss 6.0087e-03 (9.2979e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [129][270/391]\tTime  0.311 ( 0.225)\tLoss 6.4111e-03 (9.2017e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [129][300/391]\tTime  0.266 ( 0.225)\tLoss 5.2676e-03 (9.2668e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [129][330/391]\tTime  0.323 ( 0.224)\tLoss 1.0720e-02 (9.3280e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [129][360/391]\tTime  0.172 ( 0.224)\tLoss 1.2563e-02 (9.2209e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [129][390/391]\tTime  0.152 ( 0.224)\tLoss 2.1827e-02 (9.1888e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "==> Train Accuracy: Acc@1 99.962 || Acc@5 100.000\n",
            "==> Test Accuracy:  Acc@1 77.460 || Acc@5 94.010\n",
            "==> 91.84 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 130, lr: 0.0008000000000000003 -----\n",
            "Epoch: [130][  0/391]\tTime  0.555 ( 0.555)\tLoss 1.4020e-02 (1.4020e-02)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [130][ 30/391]\tTime  0.307 ( 0.227)\tLoss 5.5421e-03 (9.8222e-03)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [130][ 60/391]\tTime  0.314 ( 0.228)\tLoss 7.2843e-03 (9.9024e-03)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [130][ 90/391]\tTime  0.216 ( 0.226)\tLoss 5.3937e-03 (9.6133e-03)\tAcc@1 100.00 ( 99.93)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [130][120/391]\tTime  0.222 ( 0.224)\tLoss 1.3441e-02 (9.4811e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [130][150/391]\tTime  0.286 ( 0.223)\tLoss 7.5434e-03 (9.2237e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [130][180/391]\tTime  0.160 ( 0.222)\tLoss 6.6840e-03 (9.2927e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [130][210/391]\tTime  0.252 ( 0.222)\tLoss 1.1964e-02 (9.2529e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [130][240/391]\tTime  0.243 ( 0.222)\tLoss 8.0319e-03 (9.2777e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [130][270/391]\tTime  0.170 ( 0.221)\tLoss 7.3743e-03 (9.3209e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [130][300/391]\tTime  0.248 ( 0.221)\tLoss 1.1508e-02 (9.4093e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [130][330/391]\tTime  0.262 ( 0.221)\tLoss 6.5786e-03 (9.3312e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [130][360/391]\tTime  0.203 ( 0.221)\tLoss 6.5434e-03 (9.2939e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [130][390/391]\tTime  0.150 ( 0.221)\tLoss 1.0405e-02 (9.2577e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "==> Train Accuracy: Acc@1 99.948 || Acc@5 100.000\n",
            "==> Test Accuracy:  Acc@1 77.560 || Acc@5 94.080\n",
            "==> 90.45 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 131, lr: 0.0008000000000000003 -----\n",
            "Epoch: [131][  0/391]\tTime  0.573 ( 0.573)\tLoss 9.5022e-03 (9.5022e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [131][ 30/391]\tTime  0.174 ( 0.229)\tLoss 6.8000e-03 (8.6470e-03)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [131][ 60/391]\tTime  0.283 ( 0.228)\tLoss 6.1249e-03 (8.9480e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [131][ 90/391]\tTime  0.299 ( 0.228)\tLoss 5.3313e-03 (9.5536e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [131][120/391]\tTime  0.293 ( 0.226)\tLoss 6.4087e-03 (9.2303e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [131][150/391]\tTime  0.182 ( 0.224)\tLoss 7.7133e-03 (9.1922e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [131][180/391]\tTime  0.241 ( 0.224)\tLoss 7.4015e-03 (9.3365e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [131][210/391]\tTime  0.178 ( 0.223)\tLoss 8.6461e-03 (9.1987e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [131][240/391]\tTime  0.161 ( 0.223)\tLoss 5.4461e-03 (9.1783e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [131][270/391]\tTime  0.176 ( 0.223)\tLoss 1.0074e-02 (9.2818e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [131][300/391]\tTime  0.171 ( 0.222)\tLoss 8.0506e-03 (9.3108e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [131][330/391]\tTime  0.168 ( 0.222)\tLoss 4.8572e-03 (9.2537e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [131][360/391]\tTime  0.273 ( 0.222)\tLoss 1.7581e-02 (9.2034e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [131][390/391]\tTime  0.151 ( 0.222)\tLoss 1.1098e-02 (9.1542e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "==> Train Accuracy: Acc@1 99.960 || Acc@5 100.000\n",
            "==> Test Accuracy:  Acc@1 77.570 || Acc@5 94.100\n",
            "==> 90.82 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 132, lr: 0.0008000000000000003 -----\n",
            "Epoch: [132][  0/391]\tTime  0.573 ( 0.573)\tLoss 8.9205e-03 (8.9205e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [132][ 30/391]\tTime  0.169 ( 0.226)\tLoss 6.8068e-03 (9.2804e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [132][ 60/391]\tTime  0.176 ( 0.224)\tLoss 1.0686e-02 (9.5369e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [132][ 90/391]\tTime  0.278 ( 0.223)\tLoss 7.3198e-03 (9.3790e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [132][120/391]\tTime  0.263 ( 0.222)\tLoss 5.4574e-03 (9.2339e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [132][150/391]\tTime  0.177 ( 0.222)\tLoss 5.8974e-03 (9.3774e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [132][180/391]\tTime  0.253 ( 0.220)\tLoss 7.4794e-03 (9.2431e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [132][210/391]\tTime  0.299 ( 0.221)\tLoss 7.6421e-03 (9.1171e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [132][240/391]\tTime  0.258 ( 0.221)\tLoss 7.0933e-03 (9.0476e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [132][270/391]\tTime  0.158 ( 0.220)\tLoss 8.8822e-03 (9.0849e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [132][300/391]\tTime  0.180 ( 0.221)\tLoss 7.9682e-03 (9.0869e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [132][330/391]\tTime  0.165 ( 0.221)\tLoss 3.0163e-02 (9.2047e-03)\tAcc@1  99.22 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [132][360/391]\tTime  0.175 ( 0.221)\tLoss 1.0488e-02 (9.2445e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [132][390/391]\tTime  0.152 ( 0.221)\tLoss 7.7377e-03 (9.1677e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "==> Train Accuracy: Acc@1 99.952 || Acc@5 100.000\n",
            "==> Test Accuracy:  Acc@1 77.510 || Acc@5 94.130\n",
            "==> 90.44 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 133, lr: 0.0008000000000000003 -----\n",
            "Epoch: [133][  0/391]\tTime  0.579 ( 0.579)\tLoss 9.6198e-03 (9.6198e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [133][ 30/391]\tTime  0.276 ( 0.239)\tLoss 4.9829e-03 (9.0065e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [133][ 60/391]\tTime  0.209 ( 0.226)\tLoss 6.5919e-03 (8.4728e-03)\tAcc@1 100.00 ( 99.99)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [133][ 90/391]\tTime  0.182 ( 0.226)\tLoss 1.0635e-02 (8.7679e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [133][120/391]\tTime  0.151 ( 0.226)\tLoss 8.9613e-03 (8.7363e-03)\tAcc@1 100.00 ( 99.98)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [133][150/391]\tTime  0.161 ( 0.226)\tLoss 7.1298e-03 (8.6450e-03)\tAcc@1 100.00 ( 99.98)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [133][180/391]\tTime  0.169 ( 0.227)\tLoss 1.1668e-02 (8.7481e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [133][210/391]\tTime  0.242 ( 0.226)\tLoss 6.1156e-03 (8.9141e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [133][240/391]\tTime  0.242 ( 0.225)\tLoss 9.3422e-03 (9.1110e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [133][270/391]\tTime  0.267 ( 0.224)\tLoss 1.1225e-02 (9.1239e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [133][300/391]\tTime  0.217 ( 0.224)\tLoss 6.2333e-03 (9.1198e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [133][330/391]\tTime  0.165 ( 0.223)\tLoss 7.7847e-03 (9.0650e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [133][360/391]\tTime  0.266 ( 0.224)\tLoss 7.8177e-03 (9.0436e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [133][390/391]\tTime  0.145 ( 0.223)\tLoss 1.2535e-02 (9.0877e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "==> Train Accuracy: Acc@1 99.962 || Acc@5 100.000\n",
            "==> Test Accuracy:  Acc@1 77.480 || Acc@5 94.010\n",
            "==> 91.30 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 134, lr: 0.0008000000000000003 -----\n",
            "Epoch: [134][  0/391]\tTime  0.538 ( 0.538)\tLoss 7.9459e-03 (7.9459e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [134][ 30/391]\tTime  0.270 ( 0.231)\tLoss 2.0941e-02 (8.8640e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [134][ 60/391]\tTime  0.169 ( 0.225)\tLoss 1.0169e-02 (8.5307e-03)\tAcc@1 100.00 ( 99.99)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [134][ 90/391]\tTime  0.175 ( 0.224)\tLoss 6.9693e-03 (8.4224e-03)\tAcc@1 100.00 ( 99.99)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [134][120/391]\tTime  0.168 ( 0.225)\tLoss 7.1588e-03 (8.5975e-03)\tAcc@1 100.00 ( 99.99)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [134][150/391]\tTime  0.184 ( 0.224)\tLoss 7.7590e-03 (8.6873e-03)\tAcc@1 100.00 ( 99.98)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [134][180/391]\tTime  0.174 ( 0.225)\tLoss 8.6978e-03 (8.7710e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [134][210/391]\tTime  0.266 ( 0.224)\tLoss 6.0266e-03 (8.7875e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [134][240/391]\tTime  0.196 ( 0.223)\tLoss 9.4366e-03 (8.7624e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [134][270/391]\tTime  0.285 ( 0.223)\tLoss 7.3369e-03 (8.8305e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [134][300/391]\tTime  0.183 ( 0.222)\tLoss 5.6908e-03 (8.8651e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [134][330/391]\tTime  0.168 ( 0.223)\tLoss 7.8370e-03 (8.8777e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [134][360/391]\tTime  0.161 ( 0.222)\tLoss 1.0741e-02 (8.8949e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [134][390/391]\tTime  0.150 ( 0.223)\tLoss 8.2392e-03 (8.9564e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "==> Train Accuracy: Acc@1 99.968 || Acc@5 100.000\n",
            "==> Test Accuracy:  Acc@1 77.530 || Acc@5 93.950\n",
            "==> 91.19 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 135, lr: 0.0008000000000000003 -----\n",
            "Epoch: [135][  0/391]\tTime  0.554 ( 0.554)\tLoss 1.4720e-02 (1.4720e-02)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [135][ 30/391]\tTime  0.165 ( 0.228)\tLoss 1.1708e-02 (9.5010e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [135][ 60/391]\tTime  0.172 ( 0.223)\tLoss 9.3940e-03 (9.1297e-03)\tAcc@1 100.00 ( 99.99)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [135][ 90/391]\tTime  0.171 ( 0.222)\tLoss 7.7133e-03 (9.1996e-03)\tAcc@1 100.00 ( 99.98)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [135][120/391]\tTime  0.254 ( 0.224)\tLoss 6.2852e-03 (9.2248e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [135][150/391]\tTime  0.344 ( 0.224)\tLoss 1.0168e-02 (9.0891e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [135][180/391]\tTime  0.300 ( 0.223)\tLoss 1.9554e-02 (9.1108e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [135][210/391]\tTime  0.262 ( 0.223)\tLoss 7.9893e-03 (9.0260e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [135][240/391]\tTime  0.233 ( 0.224)\tLoss 6.4185e-03 (9.1637e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [135][270/391]\tTime  0.287 ( 0.224)\tLoss 8.1949e-03 (9.0831e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [135][300/391]\tTime  0.310 ( 0.224)\tLoss 8.2178e-03 (9.1877e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [135][330/391]\tTime  0.241 ( 0.224)\tLoss 1.1847e-02 (9.2108e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [135][360/391]\tTime  0.156 ( 0.223)\tLoss 7.4158e-03 (9.2074e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [135][390/391]\tTime  0.154 ( 0.222)\tLoss 9.6026e-03 (9.2115e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "==> Train Accuracy: Acc@1 99.956 || Acc@5 100.000\n",
            "==> Test Accuracy:  Acc@1 77.640 || Acc@5 94.020\n",
            "==> 91.09 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 136, lr: 0.0008000000000000003 -----\n",
            "Epoch: [136][  0/391]\tTime  0.600 ( 0.600)\tLoss 9.2267e-03 (9.2267e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [136][ 30/391]\tTime  0.299 ( 0.238)\tLoss 7.8635e-03 (8.5489e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [136][ 60/391]\tTime  0.190 ( 0.229)\tLoss 6.3291e-03 (8.8266e-03)\tAcc@1 100.00 ( 99.99)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [136][ 90/391]\tTime  0.172 ( 0.226)\tLoss 1.0520e-02 (9.0937e-03)\tAcc@1 100.00 ( 99.98)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [136][120/391]\tTime  0.217 ( 0.223)\tLoss 1.1838e-02 (9.1453e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [136][150/391]\tTime  0.234 ( 0.223)\tLoss 8.2975e-03 (9.1721e-03)\tAcc@1 100.00 ( 99.98)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [136][180/391]\tTime  0.320 ( 0.224)\tLoss 6.5707e-03 (9.1893e-03)\tAcc@1 100.00 ( 99.98)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [136][210/391]\tTime  0.254 ( 0.223)\tLoss 9.6487e-03 (9.2011e-03)\tAcc@1 100.00 ( 99.98)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [136][240/391]\tTime  0.205 ( 0.223)\tLoss 1.3685e-02 (9.2346e-03)\tAcc@1 100.00 ( 99.98)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [136][270/391]\tTime  0.282 ( 0.223)\tLoss 7.3297e-03 (9.1739e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [136][300/391]\tTime  0.275 ( 0.222)\tLoss 1.2955e-02 (9.1767e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [136][330/391]\tTime  0.180 ( 0.223)\tLoss 5.8851e-03 (9.1696e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [136][360/391]\tTime  0.163 ( 0.222)\tLoss 1.1630e-02 (9.2269e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [136][390/391]\tTime  0.151 ( 0.221)\tLoss 1.1583e-02 (9.1372e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "==> Train Accuracy: Acc@1 99.972 || Acc@5 100.000\n",
            "==> Test Accuracy:  Acc@1 77.610 || Acc@5 94.000\n",
            "==> 90.72 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 137, lr: 0.0008000000000000003 -----\n",
            "Epoch: [137][  0/391]\tTime  0.552 ( 0.552)\tLoss 5.9806e-03 (5.9806e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [137][ 30/391]\tTime  0.238 ( 0.235)\tLoss 6.8191e-03 (8.2206e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [137][ 60/391]\tTime  0.198 ( 0.227)\tLoss 4.8744e-03 (8.5619e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [137][ 90/391]\tTime  0.171 ( 0.226)\tLoss 1.0310e-02 (8.6396e-03)\tAcc@1 100.00 ( 99.98)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [137][120/391]\tTime  0.218 ( 0.223)\tLoss 8.6892e-03 (8.7815e-03)\tAcc@1 100.00 ( 99.98)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [137][150/391]\tTime  0.223 ( 0.224)\tLoss 1.0775e-02 (8.8633e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [137][180/391]\tTime  0.181 ( 0.223)\tLoss 8.1924e-03 (8.9126e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [137][210/391]\tTime  0.272 ( 0.223)\tLoss 7.1762e-03 (8.9513e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [137][240/391]\tTime  0.316 ( 0.223)\tLoss 8.1431e-03 (9.0856e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [137][270/391]\tTime  0.313 ( 0.223)\tLoss 6.5216e-03 (9.0189e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [137][300/391]\tTime  0.335 ( 0.223)\tLoss 5.6283e-03 (9.0102e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [137][330/391]\tTime  0.267 ( 0.223)\tLoss 1.3575e-02 (9.0502e-03)\tAcc@1  99.22 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [137][360/391]\tTime  0.160 ( 0.222)\tLoss 1.3099e-02 (9.1272e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [137][390/391]\tTime  0.151 ( 0.222)\tLoss 1.6789e-02 (9.0956e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "==> Train Accuracy: Acc@1 99.962 || Acc@5 100.000\n",
            "==> Test Accuracy:  Acc@1 77.670 || Acc@5 94.030\n",
            "==> 90.97 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 138, lr: 0.0008000000000000003 -----\n",
            "Epoch: [138][  0/391]\tTime  0.561 ( 0.561)\tLoss 8.4437e-03 (8.4437e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [138][ 30/391]\tTime  0.329 ( 0.240)\tLoss 8.9456e-03 (8.7589e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [138][ 60/391]\tTime  0.190 ( 0.225)\tLoss 6.0778e-03 (8.8873e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [138][ 90/391]\tTime  0.168 ( 0.224)\tLoss 9.2334e-03 (9.0995e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [138][120/391]\tTime  0.170 ( 0.223)\tLoss 5.1506e-03 (9.1351e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [138][150/391]\tTime  0.162 ( 0.221)\tLoss 8.3021e-03 (9.0217e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [138][180/391]\tTime  0.173 ( 0.221)\tLoss 8.0648e-03 (8.9102e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [138][210/391]\tTime  0.303 ( 0.222)\tLoss 1.0326e-02 (8.8963e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [138][240/391]\tTime  0.330 ( 0.221)\tLoss 7.9718e-03 (8.9536e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [138][270/391]\tTime  0.266 ( 0.221)\tLoss 7.0441e-03 (8.9042e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [138][300/391]\tTime  0.297 ( 0.222)\tLoss 6.7615e-03 (8.8744e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [138][330/391]\tTime  0.225 ( 0.221)\tLoss 1.7517e-02 (8.9311e-03)\tAcc@1  99.22 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [138][360/391]\tTime  0.166 ( 0.221)\tLoss 7.3877e-03 (8.9712e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [138][390/391]\tTime  0.150 ( 0.221)\tLoss 1.5862e-02 (8.9997e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "==> Train Accuracy: Acc@1 99.962 || Acc@5 100.000\n",
            "==> Test Accuracy:  Acc@1 77.750 || Acc@5 94.080\n",
            "==> 90.49 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 139, lr: 0.0008000000000000003 -----\n",
            "Epoch: [139][  0/391]\tTime  0.581 ( 0.581)\tLoss 8.1633e-03 (8.1633e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [139][ 30/391]\tTime  0.288 ( 0.238)\tLoss 4.8024e-03 (8.3849e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [139][ 60/391]\tTime  0.263 ( 0.229)\tLoss 1.0424e-02 (8.9388e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [139][ 90/391]\tTime  0.334 ( 0.225)\tLoss 6.1808e-03 (8.9724e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [139][120/391]\tTime  0.275 ( 0.225)\tLoss 5.6511e-03 (9.0108e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [139][150/391]\tTime  0.229 ( 0.225)\tLoss 1.5891e-02 (8.9949e-03)\tAcc@1  99.22 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [139][180/391]\tTime  0.174 ( 0.223)\tLoss 8.5787e-03 (9.0912e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [139][210/391]\tTime  0.166 ( 0.223)\tLoss 6.0694e-03 (9.0619e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [139][240/391]\tTime  0.172 ( 0.223)\tLoss 1.3512e-02 (9.1331e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [139][270/391]\tTime  0.167 ( 0.223)\tLoss 7.3305e-03 (9.1241e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [139][300/391]\tTime  0.222 ( 0.222)\tLoss 1.0768e-02 (9.2305e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [139][330/391]\tTime  0.286 ( 0.222)\tLoss 7.0527e-03 (9.1219e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [139][360/391]\tTime  0.297 ( 0.222)\tLoss 1.5475e-02 (9.1695e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [139][390/391]\tTime  0.143 ( 0.221)\tLoss 9.1867e-03 (9.2302e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "==> Train Accuracy: Acc@1 99.956 || Acc@5 100.000\n",
            "==> Test Accuracy:  Acc@1 77.790 || Acc@5 94.090\n",
            "==> 90.61 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 140, lr: 0.0008000000000000003 -----\n",
            "Epoch: [140][  0/391]\tTime  0.551 ( 0.551)\tLoss 8.3470e-03 (8.3470e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [140][ 30/391]\tTime  0.159 ( 0.228)\tLoss 6.0462e-03 (8.1880e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [140][ 60/391]\tTime  0.273 ( 0.227)\tLoss 1.1402e-02 (8.0090e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [140][ 90/391]\tTime  0.185 ( 0.222)\tLoss 5.2276e-03 (8.2838e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [140][120/391]\tTime  0.164 ( 0.221)\tLoss 1.0581e-02 (8.7479e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [140][150/391]\tTime  0.173 ( 0.223)\tLoss 1.2088e-02 (8.8365e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [140][180/391]\tTime  0.176 ( 0.222)\tLoss 1.3008e-02 (8.8023e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [140][210/391]\tTime  0.211 ( 0.221)\tLoss 6.5732e-03 (8.8261e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [140][240/391]\tTime  0.196 ( 0.222)\tLoss 8.2290e-03 (8.8846e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [140][270/391]\tTime  0.181 ( 0.220)\tLoss 9.5873e-03 (8.8850e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [140][300/391]\tTime  0.180 ( 0.221)\tLoss 9.2467e-03 (8.8223e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [140][330/391]\tTime  0.180 ( 0.221)\tLoss 6.0458e-03 (8.7900e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [140][360/391]\tTime  0.194 ( 0.220)\tLoss 1.8587e-02 (8.9516e-03)\tAcc@1  99.22 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [140][390/391]\tTime  0.151 ( 0.220)\tLoss 1.0580e-02 (9.0639e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "==> Train Accuracy: Acc@1 99.954 || Acc@5 100.000\n",
            "==> Test Accuracy:  Acc@1 77.600 || Acc@5 94.040\n",
            "==> 90.22 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 141, lr: 0.0008000000000000003 -----\n",
            "Epoch: [141][  0/391]\tTime  0.549 ( 0.549)\tLoss 6.2857e-03 (6.2857e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [141][ 30/391]\tTime  0.264 ( 0.232)\tLoss 6.5855e-03 (9.0588e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [141][ 60/391]\tTime  0.189 ( 0.222)\tLoss 6.8858e-03 (9.1665e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [141][ 90/391]\tTime  0.328 ( 0.222)\tLoss 7.1883e-03 (9.0109e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [141][120/391]\tTime  0.291 ( 0.221)\tLoss 7.1757e-03 (8.8834e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [141][150/391]\tTime  0.185 ( 0.221)\tLoss 1.4006e-02 (8.9563e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [141][180/391]\tTime  0.165 ( 0.222)\tLoss 8.5554e-03 (8.9180e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [141][210/391]\tTime  0.170 ( 0.222)\tLoss 9.1802e-03 (9.0015e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [141][240/391]\tTime  0.215 ( 0.222)\tLoss 8.2870e-03 (9.0007e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [141][270/391]\tTime  0.174 ( 0.223)\tLoss 8.3098e-03 (8.9797e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [141][300/391]\tTime  0.170 ( 0.223)\tLoss 7.2017e-03 (8.9443e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [141][330/391]\tTime  0.283 ( 0.224)\tLoss 5.0927e-03 (8.8940e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [141][360/391]\tTime  0.170 ( 0.224)\tLoss 1.6827e-02 (8.8631e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [141][390/391]\tTime  0.145 ( 0.224)\tLoss 9.3355e-03 (8.8796e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "==> Train Accuracy: Acc@1 99.968 || Acc@5 100.000\n",
            "==> Test Accuracy:  Acc@1 77.510 || Acc@5 93.970\n",
            "==> 91.80 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 142, lr: 0.0008000000000000003 -----\n",
            "Epoch: [142][  0/391]\tTime  0.575 ( 0.575)\tLoss 6.1390e-03 (6.1390e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [142][ 30/391]\tTime  0.216 ( 0.231)\tLoss 6.7262e-03 (8.7924e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [142][ 60/391]\tTime  0.172 ( 0.228)\tLoss 8.2436e-03 (9.3188e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [142][ 90/391]\tTime  0.191 ( 0.224)\tLoss 5.8954e-03 (9.1725e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [142][120/391]\tTime  0.270 ( 0.224)\tLoss 7.1711e-03 (9.0027e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [142][150/391]\tTime  0.162 ( 0.222)\tLoss 5.6126e-03 (9.0330e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [142][180/391]\tTime  0.176 ( 0.223)\tLoss 8.6460e-03 (8.9041e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [142][210/391]\tTime  0.167 ( 0.223)\tLoss 9.0441e-03 (8.9584e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [142][240/391]\tTime  0.181 ( 0.222)\tLoss 8.2628e-03 (9.1443e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [142][270/391]\tTime  0.190 ( 0.221)\tLoss 8.0112e-03 (9.1238e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [142][300/391]\tTime  0.158 ( 0.222)\tLoss 8.5698e-03 (9.0778e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [142][330/391]\tTime  0.165 ( 0.222)\tLoss 7.9826e-03 (9.1391e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [142][360/391]\tTime  0.174 ( 0.221)\tLoss 1.7461e-02 (9.1126e-03)\tAcc@1  99.22 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [142][390/391]\tTime  0.150 ( 0.221)\tLoss 1.8537e-02 (9.1052e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "==> Train Accuracy: Acc@1 99.952 || Acc@5 100.000\n",
            "==> Test Accuracy:  Acc@1 77.460 || Acc@5 93.950\n",
            "==> 90.70 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 143, lr: 0.0008000000000000003 -----\n",
            "Epoch: [143][  0/391]\tTime  0.610 ( 0.610)\tLoss 1.4540e-02 (1.4540e-02)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [143][ 30/391]\tTime  0.318 ( 0.245)\tLoss 5.7036e-03 (9.0131e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [143][ 60/391]\tTime  0.208 ( 0.228)\tLoss 5.6374e-03 (8.6627e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [143][ 90/391]\tTime  0.172 ( 0.225)\tLoss 9.8225e-03 (8.9857e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [143][120/391]\tTime  0.270 ( 0.227)\tLoss 4.6188e-03 (8.9467e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [143][150/391]\tTime  0.253 ( 0.225)\tLoss 6.6744e-03 (8.9226e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [143][180/391]\tTime  0.328 ( 0.224)\tLoss 9.4371e-03 (8.8396e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [143][210/391]\tTime  0.275 ( 0.224)\tLoss 7.0140e-03 (8.8461e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [143][240/391]\tTime  0.311 ( 0.223)\tLoss 5.1537e-03 (8.9147e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [143][270/391]\tTime  0.274 ( 0.224)\tLoss 6.5837e-03 (8.9148e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [143][300/391]\tTime  0.284 ( 0.224)\tLoss 6.7360e-03 (8.8838e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [143][330/391]\tTime  0.287 ( 0.225)\tLoss 1.3626e-02 (8.9036e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [143][360/391]\tTime  0.255 ( 0.224)\tLoss 1.7582e-02 (8.9059e-03)\tAcc@1  99.22 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [143][390/391]\tTime  0.151 ( 0.223)\tLoss 8.4344e-03 (8.9655e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "==> Train Accuracy: Acc@1 99.958 || Acc@5 100.000\n",
            "==> Test Accuracy:  Acc@1 77.640 || Acc@5 93.960\n",
            "==> 91.53 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 144, lr: 0.0008000000000000003 -----\n",
            "Epoch: [144][  0/391]\tTime  0.566 ( 0.566)\tLoss 9.2474e-03 (9.2474e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [144][ 30/391]\tTime  0.262 ( 0.227)\tLoss 6.9417e-03 (9.0615e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [144][ 60/391]\tTime  0.331 ( 0.223)\tLoss 8.1984e-03 (8.9803e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [144][ 90/391]\tTime  0.243 ( 0.223)\tLoss 9.1342e-03 (9.3159e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [144][120/391]\tTime  0.307 ( 0.223)\tLoss 1.0601e-02 (9.3227e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [144][150/391]\tTime  0.176 ( 0.223)\tLoss 8.3029e-03 (9.2900e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [144][180/391]\tTime  0.257 ( 0.222)\tLoss 1.1372e-02 (9.1982e-03)\tAcc@1  99.22 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [144][210/391]\tTime  0.322 ( 0.223)\tLoss 8.8611e-03 (9.1457e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [144][240/391]\tTime  0.300 ( 0.223)\tLoss 7.8694e-03 (9.0832e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [144][270/391]\tTime  0.172 ( 0.223)\tLoss 6.4726e-03 (9.1286e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [144][300/391]\tTime  0.217 ( 0.223)\tLoss 6.0119e-03 (9.0644e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [144][330/391]\tTime  0.164 ( 0.223)\tLoss 8.4051e-03 (9.0204e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [144][360/391]\tTime  0.181 ( 0.223)\tLoss 6.3056e-03 (8.9846e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [144][390/391]\tTime  0.151 ( 0.223)\tLoss 9.0584e-03 (8.9636e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "==> Train Accuracy: Acc@1 99.972 || Acc@5 100.000\n",
            "==> Test Accuracy:  Acc@1 77.540 || Acc@5 94.030\n",
            "==> 91.34 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 145, lr: 0.0008000000000000003 -----\n",
            "Epoch: [145][  0/391]\tTime  0.550 ( 0.550)\tLoss 5.0575e-03 (5.0575e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [145][ 30/391]\tTime  0.173 ( 0.225)\tLoss 7.2609e-03 (8.0675e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [145][ 60/391]\tTime  0.273 ( 0.223)\tLoss 7.1232e-03 (8.7559e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [145][ 90/391]\tTime  0.275 ( 0.225)\tLoss 9.4305e-03 (8.9995e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [145][120/391]\tTime  0.176 ( 0.222)\tLoss 6.6569e-03 (9.0536e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [145][150/391]\tTime  0.172 ( 0.222)\tLoss 5.4870e-03 (9.0988e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [145][180/391]\tTime  0.337 ( 0.223)\tLoss 8.3524e-03 (8.9325e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [145][210/391]\tTime  0.288 ( 0.223)\tLoss 1.1687e-02 (8.9946e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [145][240/391]\tTime  0.248 ( 0.223)\tLoss 1.0443e-02 (9.0075e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [145][270/391]\tTime  0.191 ( 0.223)\tLoss 8.6374e-03 (8.9020e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [145][300/391]\tTime  0.177 ( 0.223)\tLoss 5.3806e-03 (8.8935e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [145][330/391]\tTime  0.166 ( 0.223)\tLoss 6.0613e-03 (8.8957e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [145][360/391]\tTime  0.166 ( 0.223)\tLoss 9.3420e-03 (8.8851e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [145][390/391]\tTime  0.150 ( 0.223)\tLoss 1.0451e-02 (8.9055e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "==> Train Accuracy: Acc@1 99.958 || Acc@5 100.000\n",
            "==> Test Accuracy:  Acc@1 77.640 || Acc@5 93.960\n",
            "==> 91.30 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 146, lr: 0.0008000000000000003 -----\n",
            "Epoch: [146][  0/391]\tTime  0.591 ( 0.591)\tLoss 7.0390e-03 (7.0390e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [146][ 30/391]\tTime  0.290 ( 0.238)\tLoss 6.7441e-03 (8.9158e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [146][ 60/391]\tTime  0.154 ( 0.226)\tLoss 6.7590e-03 (8.8749e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [146][ 90/391]\tTime  0.284 ( 0.226)\tLoss 7.2397e-03 (8.8181e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [146][120/391]\tTime  0.276 ( 0.226)\tLoss 1.2117e-02 (8.8754e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [146][150/391]\tTime  0.206 ( 0.224)\tLoss 7.4048e-03 (8.9510e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [146][180/391]\tTime  0.292 ( 0.223)\tLoss 1.0361e-02 (8.8215e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [146][210/391]\tTime  0.289 ( 0.223)\tLoss 7.0448e-03 (8.8712e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [146][240/391]\tTime  0.306 ( 0.223)\tLoss 6.1894e-03 (8.9100e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [146][270/391]\tTime  0.221 ( 0.222)\tLoss 8.0777e-03 (8.9315e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [146][300/391]\tTime  0.167 ( 0.221)\tLoss 8.3589e-03 (8.8596e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [146][330/391]\tTime  0.179 ( 0.222)\tLoss 8.6344e-03 (8.8854e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [146][360/391]\tTime  0.179 ( 0.223)\tLoss 6.5827e-03 (8.9198e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [146][390/391]\tTime  0.150 ( 0.222)\tLoss 1.0673e-02 (8.9223e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "==> Train Accuracy: Acc@1 99.966 || Acc@5 100.000\n",
            "==> Test Accuracy:  Acc@1 77.510 || Acc@5 94.030\n",
            "==> 91.11 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 147, lr: 0.0008000000000000003 -----\n",
            "Epoch: [147][  0/391]\tTime  0.552 ( 0.552)\tLoss 8.7346e-03 (8.7346e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [147][ 30/391]\tTime  0.166 ( 0.229)\tLoss 7.7021e-03 (8.9924e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [147][ 60/391]\tTime  0.195 ( 0.224)\tLoss 1.8079e-02 (8.9875e-03)\tAcc@1  99.22 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [147][ 90/391]\tTime  0.201 ( 0.224)\tLoss 1.0149e-02 (8.9880e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [147][120/391]\tTime  0.199 ( 0.222)\tLoss 1.4589e-02 (8.9766e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [147][150/391]\tTime  0.298 ( 0.224)\tLoss 6.7120e-03 (8.9431e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [147][180/391]\tTime  0.262 ( 0.224)\tLoss 1.0803e-02 (8.9414e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [147][210/391]\tTime  0.164 ( 0.224)\tLoss 8.4658e-03 (9.0669e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [147][240/391]\tTime  0.342 ( 0.225)\tLoss 7.5553e-03 (9.2270e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [147][270/391]\tTime  0.240 ( 0.225)\tLoss 7.5414e-03 (9.1342e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [147][300/391]\tTime  0.162 ( 0.224)\tLoss 7.5826e-03 (9.1468e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [147][330/391]\tTime  0.200 ( 0.224)\tLoss 7.1603e-03 (9.1400e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [147][360/391]\tTime  0.189 ( 0.224)\tLoss 8.6914e-03 (9.1195e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [147][390/391]\tTime  0.151 ( 0.223)\tLoss 1.1186e-02 (9.1208e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "==> Train Accuracy: Acc@1 99.954 || Acc@5 100.000\n",
            "==> Test Accuracy:  Acc@1 77.790 || Acc@5 93.970\n",
            "==> 91.50 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 148, lr: 0.0008000000000000003 -----\n",
            "Epoch: [148][  0/391]\tTime  0.581 ( 0.581)\tLoss 1.0396e-02 (1.0396e-02)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [148][ 30/391]\tTime  0.294 ( 0.241)\tLoss 8.0204e-03 (8.5216e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [148][ 60/391]\tTime  0.238 ( 0.235)\tLoss 9.4726e-03 (8.6738e-03)\tAcc@1 100.00 ( 99.99)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [148][ 90/391]\tTime  0.326 ( 0.232)\tLoss 5.4105e-03 (8.6935e-03)\tAcc@1 100.00 ( 99.98)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [148][120/391]\tTime  0.267 ( 0.231)\tLoss 6.9797e-03 (8.7454e-03)\tAcc@1 100.00 ( 99.99)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [148][150/391]\tTime  0.266 ( 0.229)\tLoss 6.9042e-03 (8.8856e-03)\tAcc@1 100.00 ( 99.98)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [148][180/391]\tTime  0.323 ( 0.228)\tLoss 5.4507e-03 (8.8560e-03)\tAcc@1 100.00 ( 99.98)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [148][210/391]\tTime  0.281 ( 0.226)\tLoss 1.0732e-02 (8.8319e-03)\tAcc@1 100.00 ( 99.98)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [148][240/391]\tTime  0.211 ( 0.225)\tLoss 6.3853e-03 (8.8263e-03)\tAcc@1 100.00 ( 99.98)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [148][270/391]\tTime  0.167 ( 0.224)\tLoss 1.0790e-02 (8.8420e-03)\tAcc@1 100.00 ( 99.98)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [148][300/391]\tTime  0.156 ( 0.225)\tLoss 7.0331e-03 (8.7957e-03)\tAcc@1 100.00 ( 99.98)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [148][330/391]\tTime  0.193 ( 0.225)\tLoss 1.1495e-02 (8.8014e-03)\tAcc@1 100.00 ( 99.98)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [148][360/391]\tTime  0.160 ( 0.225)\tLoss 6.5729e-03 (8.8186e-03)\tAcc@1 100.00 ( 99.98)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [148][390/391]\tTime  0.151 ( 0.224)\tLoss 8.9271e-03 (8.8256e-03)\tAcc@1 100.00 ( 99.98)\tAcc@5 100.00 (100.00)\n",
            "==> Train Accuracy: Acc@1 99.978 || Acc@5 100.000\n",
            "==> Test Accuracy:  Acc@1 77.840 || Acc@5 93.960\n",
            "==> 91.86 seconds to train this epoch\n",
            "\n",
            "\n",
            "----- epoch: 149, lr: 0.0008000000000000003 -----\n",
            "Epoch: [149][  0/391]\tTime  0.572 ( 0.572)\tLoss 6.0963e-03 (6.0963e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [149][ 30/391]\tTime  0.298 ( 0.245)\tLoss 1.4425e-02 (8.7217e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [149][ 60/391]\tTime  0.323 ( 0.237)\tLoss 6.7018e-03 (8.8049e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [149][ 90/391]\tTime  0.229 ( 0.230)\tLoss 5.1285e-03 (8.7454e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [149][120/391]\tTime  0.173 ( 0.228)\tLoss 1.1193e-02 (9.0689e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [149][150/391]\tTime  0.190 ( 0.227)\tLoss 1.1787e-02 (8.9919e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [149][180/391]\tTime  0.161 ( 0.227)\tLoss 1.1016e-02 (9.1981e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [149][210/391]\tTime  0.177 ( 0.228)\tLoss 5.8209e-03 (9.4063e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [149][240/391]\tTime  0.165 ( 0.228)\tLoss 6.1170e-03 (9.2852e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [149][270/391]\tTime  0.168 ( 0.227)\tLoss 7.1268e-03 (9.2812e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [149][300/391]\tTime  0.247 ( 0.227)\tLoss 7.3063e-03 (9.1972e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [149][330/391]\tTime  0.207 ( 0.226)\tLoss 8.0828e-03 (9.2116e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [149][360/391]\tTime  0.237 ( 0.226)\tLoss 7.4293e-03 (9.1757e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "Epoch: [149][390/391]\tTime  0.152 ( 0.225)\tLoss 1.2572e-02 (9.1779e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n",
            "==> Train Accuracy: Acc@1 99.948 || Acc@5 100.000\n",
            "==> Test Accuracy:  Acc@1 77.590 || Acc@5 93.980\n",
            "==> 92.29 seconds to train this epoch\n",
            "\n",
            "Best Top-1 Accuracy: 77.84\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "3MgZMzK0CFbT",
        "outputId": "58a3b2be-c36e-4597-ce37-cb93bc6a0325"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('model_latest.pt')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_a48391b8-70cd-467c-ab09-4ddf5cc84a3a\", \"model_latest.pt\", 85457625)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}